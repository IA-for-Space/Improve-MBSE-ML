{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rICw0MTsMtam",
    "outputId": "4acf6428-30d0-4699-8dbe-e002169aa480"
   },
   "outputs": [],
   "source": [
    "#!pip install polaris-ml\n",
    "#!pip install fets\n",
    "#!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RlwcETrdgSBZ",
    "outputId": "80d937e4-3ea8-4554-9ccd-bb2dafc31ca8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "lista_archivos_lightsail = [\"./lightsail2_dataset/\" + x for x in os.listdir(\"./lightsail2_dataset\") if x[-3:] == \"txt\"]\n",
    "lista_archivos_marsexpress = [\"./mars_express_dataset/\" + x for x in os.listdir(\"./mars_express_dataset/\") if x[-3:] == \"csv\"]\n",
    "print(lista_archivos_lightsail[:5])\n",
    "print(lista_archivos_marsexpress[:5])\n",
    "\n",
    "print(\"Ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKOK2W_sacnM",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#DATASET PARA LECTURA HUMANA DEL LIGHTSAIL\n",
    "\n",
    "def read_human_from_txt_lightsail(lista_archivos):\n",
    "\n",
    "  etiquetas = [\"Observed_at\", \"Posted_at\"]\n",
    "\n",
    "  dataset = {}\n",
    "\n",
    "  dataset[etiquetas[0]] = []\n",
    "  dataset[etiquetas[1]] = []\n",
    "\n",
    "  archivo1 = open(lista_archivos[0], \"r\")\n",
    "\n",
    "  for linea in archivo1:\n",
    "    if \"Observed at\" in linea:\n",
    "      valor = linea.split(\"at:\")[1].strip()\n",
    "      dataset[\"Observed_at\"].append(valor)\n",
    "    elif \"Posted at\" in linea:\n",
    "      valor = linea.split(\"at:\")[1].strip()\n",
    "      dataset[\"Posted_at\"].append(valor)\n",
    "    elif \":\" in linea and \"=\" in linea:\n",
    "      etiqueta = linea.split(\":\")[1].split(\"=\")[0].strip().replace(\" \", \"_\").replace(\"[\", \"_\").replace(\"]\", \"_\").replace(\"<\", \"_\")\n",
    "      valor = linea.split(\":\")[1].split(\"=\")[1].split(\"[\")[0].strip()\n",
    "      dataset[etiqueta] = []\n",
    "      dataset[etiqueta].append(valor)\n",
    "\n",
    "  for nombre_archivo in lista_archivos[1:]:\n",
    "    archivo = open(nombre_archivo, \"r\")\n",
    "    for linea in archivo:\n",
    "      if \"Observed at\" in linea:\n",
    "        valor = linea.split(\"at:\")[1].strip()\n",
    "        dataset[\"Observed_at\"].append(valor)\n",
    "      elif \"Posted at\" in linea:\n",
    "        valor = linea.split(\"at:\")[1].strip()\n",
    "        dataset[\"Posted_at\"].append(valor)\n",
    "      elif \":\" in linea and \"=\" in linea:\n",
    "        etiqueta = linea.split(\":\")[1].split(\"=\")[0].strip().replace(\" \", \"_\").replace(\"[\", \"_\").replace(\"]\", \"_\").replace(\"<\", \"_\")\n",
    "        valor = linea.split(\":\")[1].split(\"=\")[1].split(\"[\")[0].strip()\n",
    "        dataset[etiqueta].append(valor)\n",
    "\n",
    "  df = pd.DataFrame(dataset.values(), dataset.keys())\n",
    "  df = df.transpose()\n",
    "  return df\n",
    "\n",
    "print(\"Ok\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSL_Yph0WzBZ"
   },
   "outputs": [],
   "source": [
    "#LECTURA DE LOS DATASETS DE LIGHTSAIL2 Y MARS EXPRESS\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def transform_unix_to_utc(series):\n",
    "  if len(series) > 0:\n",
    "    new_series = [str(datetime.utcfromtimestamp(x/1000))[:-3] for x in series]\n",
    "    return new_series\n",
    "  else:\n",
    "    return []\n",
    "\n",
    "def read_from_txt_lightsail(lista_archivos):\n",
    "  dataset = {}\n",
    "  archivo1 = open(lista_archivos[0], \"r\")\n",
    "  kvp_form = False\n",
    "\n",
    "  for linea in archivo1:\n",
    "    if \"KVP form:\" in linea:\n",
    "      kvp_form = True\n",
    "    if kvp_form is True and \"=\" in linea:\n",
    "        etiqueta = linea.split(\"=\")[0].strip()\n",
    "        valor = float(linea.split(\"=\")[1].split(\",\")[0].strip())\n",
    "        dataset[etiqueta] = []\n",
    "        dataset[etiqueta].append(valor)\n",
    "\n",
    "  for nombre_archivo in lista_archivos[1:]:\n",
    "    archivo = open(nombre_archivo, \"r\")\n",
    "    kvp_form = False\n",
    "    for linea in archivo:\n",
    "      if \"KVP form:\" in linea:\n",
    "        kvp_form = True\n",
    "      if kvp_form is True and \"=\" in linea:\n",
    "        etiqueta = linea.split(\"=\")[0].strip()\n",
    "        valor = float(linea.split(\"=\")[1].split(\",\")[0].strip())\n",
    "        dataset[etiqueta].append(valor)\n",
    "\n",
    "  df = pd.DataFrame(dataset.values(), dataset.keys())\n",
    "  df = df.transpose()\n",
    "  return df\n",
    "\n",
    "\n",
    "def read_from_ut_ms(lista_archivos):\n",
    "  columnas = []\n",
    "  for archivo in lista_archivos:\n",
    "    df1 = pd.read_csv(archivo)\n",
    "    columnas += list(df1.columns)\n",
    "    columnas = list(set(columnas))\n",
    "\n",
    "  df = pd.DataFrame(columns=columnas)\n",
    "  df = df.loc[:, ~df.columns.duplicated()]\n",
    "  for archivo in lista_archivos:\n",
    "    df2 = pd.read_csv(archivo)\n",
    "    df2[\"date\"] = transform_unix_to_utc(df2[\"ut_ms\"])\n",
    "    df = df.append(df2)\n",
    "  return df.head(1000)\n",
    "\n",
    "def read_from_utb_ms(lista_archivos):\n",
    "  columnas = []\n",
    "  for archivo in lista_archivos:\n",
    "    df1 = pd.read_csv(archivo)\n",
    "    columnas = list(df1.columns)\n",
    "    columnas.append(\"date\")\n",
    "  df = pd.DataFrame(columns=columnas)\n",
    "  for archivo in lista_archivos:\n",
    "    df2 = pd.read_csv(archivo)\n",
    "    df2[\"date\"] = transform_unix_to_utc(df2[\"utb_ms\"])\n",
    "    df = df.append(df2)\n",
    "  return df.head(1000)\n",
    "\n",
    "def replace_strings_with_int(series):\n",
    "    collection = list(set(series))\n",
    "    new_series = [collection.index(x)+1 for x in series]\n",
    "    return new_series\n",
    "\n",
    "def build_json_mars_express(lista_archivos):\n",
    "    all_jsons = []\n",
    "    for archivo in lista_archivos:\n",
    "        df = pd.read_csv(archivo).head(1000)\n",
    "        for col, row in df.iterrows():\n",
    "            new_json = dict({\"time\":\"\", \"measurament\":\"\", \"tags\":{\"satellite\":\"Mars Express\", \"decoder\":\"\", \"station\":\"\", \"observer\":\"\", \"source\":\"\", \"version\":\"\"}, \"fields\":{}})\n",
    "            new_json[\"time\"] = str(datetime.utcfromtimestamp(row[\"ut_ms\"]/1000))\n",
    "            new_json[\"fields\"] = dict(row)\n",
    "            all_jsons.append(new_json)\n",
    "    fp = open('decoded_frames.json', 'w')\n",
    "    fp.write(json.dumps(all_jsons, indent=3))\n",
    "    fp.close()\n",
    "    \n",
    "        \n",
    "def read_from_csv_mars_express(lista_archivos):\n",
    "\n",
    "  lista_archivos_saaf = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"saaf\"]\n",
    "  lista_archivos_dmop = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"dmop\"]\n",
    "  lista_archivos_ftl = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"ftl\"]\n",
    "  lista_archivos_evtf = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"evtf\"]\n",
    "  lista_archivos_ltdata = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"ltdata\"]\n",
    "\n",
    "  df_saaf = read_from_ut_ms(lista_archivos_saaf).drop(\"ut_ms\", axis=1)\n",
    "  df_dmop = read_from_ut_ms(lista_archivos_dmop).drop(\"ut_ms\", axis=1)\n",
    "  df_ftl = read_from_utb_ms(lista_archivos_ftl)\n",
    "  df_evtf = read_from_ut_ms(lista_archivos_evtf).drop(\"ut_ms\", axis=1)\n",
    "  df_ltdata = read_from_ut_ms(lista_archivos_ltdata).drop(\"ut_ms\", axis=1)\n",
    "    \n",
    "        \n",
    "  df = df_saaf\n",
    "  df = df.join(df_dmop.set_index(\"date\"), on=\"date\", how=\"outer\")\n",
    "  df = df.join(df_ftl.set_index(\"date\"), on=\"date\", how=\"outer\")\n",
    "  df = df.join(df_evtf.set_index(\"date\"), on=\"date\", how=\"outer\")\n",
    "  df = df.join(df_ltdata.set_index(\"date\"), on=\"date\", how=\"outer\").sort_values(\"date\")\n",
    "\n",
    "  df = df.fillna(0)\n",
    "\n",
    "  df[\"subsystem\"] = replace_strings_with_int(df[\"subsystem\"])\n",
    "  df[\"type\"] = replace_strings_with_int(df[\"type\"])\n",
    "  df[\"description\"] = replace_strings_with_int(df[\"description\"])\n",
    "  df[\"flagcomms\"] = replace_strings_with_int(df[\"flagcomms\"])\n",
    "\n",
    "  df = df.drop_duplicates()\n",
    "  return df\n",
    "\n",
    "print(\"Ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SE ALMACENAN LOS DATASETS EN JSONS\n",
    "lista_archivos_ltdata = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"ltdata\"]\n",
    "build_json_mars_express(lista_archivos_ltdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#LECTURA DE LOS DATASETS DE MARS EXPRESS, Y DETECCIÓN DE OUTLIERS\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from limpieza_funciones import grafico_outliers\n",
    "\n",
    "\n",
    "def get_outliers(df):\n",
    "    # Limpieza de datos: detección de outliers.\n",
    "    outlier_method = EllipticEnvelope().fit(df)\n",
    "    scores_pred = outlier_method.decision_function(df)\n",
    "    \n",
    "    Q1 = stats.scoreatpercentile(scores_pred, 25)\n",
    "    Q3 = stats.scoreatpercentile(scores_pred, 75)\n",
    "    RIC = Q3 - Q1\n",
    "    limite_inferior = Q1 - 1.5*RIC\n",
    "    limite_superior = Q3 + 1.5*RIC\n",
    "    \n",
    "    # Estimación de outliers\n",
    "    pos_i = np.where(scores_pred < limite_inferior)\n",
    "    pos_s = np.where(scores_pred > limite_superior)\n",
    "    \n",
    "    # Matriz de outliers\n",
    "    mask_outliers = np.ones(np.shape(scores_pred))\n",
    "    mask_outliers[pos_i] = 0\n",
    "    mask_outliers[pos_s] = 0\n",
    "    return mask_outliers\n",
    "\n",
    "def get_one_dataframe(lista_archivos):\n",
    "  columnas = []\n",
    "  for archivo in lista_archivos:\n",
    "    df1 = pd.read_csv(archivo)\n",
    "    columnas += list(df1.columns)\n",
    "    columnas = list(set(columnas))\n",
    "\n",
    "  df = pd.DataFrame(columns=columnas)\n",
    "  df = df.loc[:, ~df.columns.duplicated()]\n",
    "  for archivo in lista_archivos:\n",
    "    df2 = pd.read_csv(archivo)\n",
    "    df = df.append(df2)\n",
    "  return df.head(1000)\n",
    "\n",
    "\n",
    "def replace_strings_with_int(series):\n",
    "    collection = list(set(series))\n",
    "    new_series = [collection.index(x)+1 for x in series]\n",
    "    return new_series\n",
    "\n",
    "def check_csv_mars_express(lista_archivos):\n",
    "\n",
    "  lista_archivos_saaf = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"saaf\"]\n",
    "  lista_archivos_dmop = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"dmop\"]\n",
    "  lista_archivos_ftl = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"ftl\"]\n",
    "  lista_archivos_evtf = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"evtf\"]\n",
    "  lista_archivos_ltdata = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"ltdata\"]\n",
    "\n",
    "  #df_saaf = get_one_dataframe(lista_archivos_saaf)\n",
    "  #df_dmop = get_one_dataframe(lista_archivos_dmop) \n",
    "  #df_ftl = get_one_dataframe(lista_archivos_ftl)\n",
    "  #df_evtf = get_one_dataframe(lista_archivos_evtf)\n",
    "  df_ltdata = get_one_dataframe(lista_archivos_ltdata)\n",
    "    \n",
    "  #df_saaf_outliers = get_outliers(df_saaf)\n",
    "  #df_dmop_outliers = get_outliers(df_dmop) #no son valores numéricos\n",
    "  #df_ftl_outliers = get_outliers(df_ftl) #no son valores numéricos\n",
    "  #df_evtf_outliers = get_outliers(df_evtf) #no son valores numéricos\n",
    "  df_ltdata_outliers = get_outliers(df_ltdata)\n",
    "  print(df_ltdata_outliers[df_ltdata_outliers==0].shape)\n",
    " \n",
    "  return df_ltdata.iloc[df_ltdata_outliers==0]\n",
    "print(\"Ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPROBAR RESULTADOS ANTES DE UTILIZAR POLARIS\n",
    "outs = check_csv_mars_express(lista_archivos_marsexpress)\n",
    "print(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#COMPROBAR RESULTADOS ANTES DE UTILIZAR POLARIS\n",
    "df = read_from_txt_lightsail(lista_archivos_lightsail)\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "id": "sbj0GWnPysEc",
    "outputId": "d85f37dd-bb2e-4ea3-86e1-7ac722e64309"
   },
   "outputs": [],
   "source": [
    "#COMPROBAR RESULTADOS ANTES DE UTILIZAR POLARIS\n",
    "df = read_from_csv_mars_express(lista_archivos_marsexpress)\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(df.head(25))\n",
    "print(\"subsystem: \", len(set(df[\"subsystem\"])),len(df[\"subsystem\"]))\n",
    "print(\"type: \", len(set(df[\"type\"])),len(df[\"type\"]))\n",
    "print(\"description: \", len(set(df[\"description\"])),len(df[\"description\"]))\n",
    "print(\"flagcomms: \", len(set(df[\"flagcomms\"])),len(df[\"flagcomms\"]), set(df[\"flagcomms\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmpHtEkGWJuH",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#datatest1 = pd.DataFrame({\"Nombre\": [\"Daenerys\", \"Deku\", \"Bakugo\", \"Peter Pan\", \"Akamatsu\", \"Spiderman\", \"Joey Mills\"], \"Serie\": [\"Juego de Tronos\", \"BNHA\", \"BNHA\", \"Peter Pan\", \"Akamatsu y Seven\", \"Spiderman\", \"Helix Studios\"], \"Me gusta\": [\"True\", \"True\", \"True\", \"False\", \"True\", \"True\", \"Supertrue\"]})\n",
    "#datatest2 = pd.DataFrame({\"Nombre\": [\"Davos\", \"Homer\", \"Joey\", \"Jay\", \"Seven\", \"Loki\", \"Blake Mitchell\"], \"Serie\": [\"Juego de Tronos\", \"Simpsons\", \"Friends\", \"Modern Family\", \"Akamatsu y Seven\", \"Loki\", \"Helix Studios\"], \"Me gusta\": [\"False\", \"True\", \"True\", \"False\", \"True\", \"True\", \"True\"]})\n",
    "#datatest3 = pd.DataFrame({\"Nombre\": [\"Arya\", \"Peter\", \"Ted\", \"Gloria\", \"El primo violador\", \"Wanda\", \"Steven Prior\"], \"Serie\": [\"Juego de Tronos\", \"Padre de Familia\", \"HIMYM\", \"Modern Family\", \"Akamatsu y Seven\", \"Wandavision\", \"Tim Tales\"], \"Me gusta\": [\"True\", \"False\", \"False\", \"True\", \"False\", \"True\", \"True\"]})\n",
    "\n",
    "\n",
    "\n",
    "datatest1 = pd.DataFrame({\"Fecha\": [\"07/01/2022\", \"08/01/2022\", \"09/01/2022\", \"10/01/2022\", \"11/01/2022\"],\n",
    "                          \"Clima en Elche\": [\"Soleado\", \"Nublado\", \"Niebla\", \"Nublado\", \"Lluvioso\"]})\n",
    "\n",
    "datatest2 = pd.DataFrame({\"Fecha\": [\"07/01/2022\", \"08/01/2022\", \"09/01/2022\", \"10/01/2022\", \"11/01/2022\"],\n",
    "                          \"Noticia del día\": [\"Secuestran a peter\", \"Vuelve el volcán de la palma\", \"Cancelan a Energuia\", \"Memes de Jordi Wild\", \"Sube la luz\"]})\n",
    "\n",
    "datatest3 = pd.DataFrame({\"Fecha\": [\"07/01/2022\", \"08/01/2022\", \"09/01/2022\", \"10/01/2022\", \"11/01/2022\", \"12/10/2022\"],\n",
    "                          \"Gasolina de mi coche\": [\"Nada\", \"Mucha\", \"Mucha\", \"Media\", \"Poca\", \"Muy Poca\"],\n",
    "                          \"Precio del pan\": [0.5, 0.5, 0.5, 0.6, 0.6, 0.9]})\n",
    "\n",
    "df = datatest1\n",
    "df = df.join(datatest2.set_index(\"Fecha\"), on=\"Fecha\", how=\"outer\")\n",
    "df = df.join(datatest3.set_index(\"Fecha\"), on=\"Fecha\", how=\"outer\")\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzPPwEj0jM29",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#GUARDAR CSV DEL LIGHTSAIL2 PARA PROCESAMIENTO DE POLARIS\n",
    "df_csv = df.to_csv('lightsail_dataset.csv', sep=',', index=False)\n",
    "print(\"Ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benZjPIheBK1"
   },
   "outputs": [],
   "source": [
    "#GUARDAR CSV DEL MARS EXPRESS PARA PROCESAMIENTO DE POLARIS\n",
    "df_csv = df.to_csv('marsexpress_dataset.csv', sep=',', index=False)\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CLEANER.PY DE POLARIS, PARA PODER PERSONALIZARLO SEGÚN EL DATASET CONVENGA\n",
    "\n",
    "\"\"\"Module for cleaning data\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "# pylint: disable=R0903\n",
    "class Cleaner:\n",
    "    \"\"\"Class for cleaning features.\n",
    "    \"\"\"\n",
    "    def __init__(self, metadata, cleaning_params):\n",
    "        # in percent, maximum na rows in a column\n",
    "        self._col_threshold = cleaning_params.col_max_na_percentage\n",
    "        # in percent, maximum na columns in a row\n",
    "        self._row_threshold = cleaning_params.row_max_na_percentage\n",
    "        self._metadata = metadata\n",
    "\n",
    "    def handle_missing_values(self, dataframe):\n",
    "        \"\"\"Preprocess data to remove unnecessary rows and columns (filled with\n",
    "        nan)\n",
    "\n",
    "        :param dataframe: Dataframe that needs to be preprocessed\n",
    "        :type dataframe: pd.DataFrame\n",
    "        :return: Preprocessed Dataframe\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        initial_shape = dataframe.shape\n",
    "\n",
    "        # Dropping columns first so that frames without necessary columns\n",
    "        # of data are removed.\n",
    "        # Remove columns not satisfying criteria\n",
    "        count_na_col = dataframe.isna().sum()\n",
    "        count_na_col = count_na_col * (100 / dataframe.shape[0])\n",
    "        dataframe = dataframe.loc[:, count_na_col < self._col_threshold]\n",
    "\n",
    "        # Remove rows not satisfying criteria\n",
    "        count_na_row = dataframe.isna().sum(axis=1)\n",
    "        if dataframe.shape[1] != 0:\n",
    "            count_na_row = count_na_row * (100 / dataframe.shape[1])\n",
    "        else:\n",
    "            count_na_row = 0\n",
    "\n",
    "        dataframe = dataframe.loc[count_na_row < self._row_threshold, :]\n",
    "\n",
    "        # ffill will fill all but the first set of nans\n",
    "        # bfill will fill the first set of nans\n",
    "        # (if nans are present in the first few rows)\n",
    "        # Fill nans with nearest value\n",
    "        dataframe = dataframe.fillna(method=\"ffill\")\n",
    "        dataframe = dataframe.fillna(method=\"bfill\")\n",
    "\n",
    "        final_shape = dataframe.shape\n",
    "\n",
    "        LOGGER.debug(\"Initial Shape: %s, Final Shape: %s\", initial_shape,\n",
    "                     final_shape)\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    def drop_constant_values(self, dataframe):\n",
    "        \"\"\"Preprocess data to remove columns with\n",
    "        constant values\n",
    "\n",
    "        :param dataframe: Dataframe that needs to be preprocessed\n",
    "        :type dataframe: pd.DataFrame\n",
    "        :return: Preprocessed Dataframe\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "        if 'analysis' in self._metadata:\n",
    "            constants = [\n",
    "                column for column, tag in self._metadata['analysis']\n",
    "                ['column_tags'].items() if tag == \"constant\"\n",
    "            ]\n",
    "\n",
    "            LOGGER.info('Dropping constant column(s) : %s',\n",
    "                        ','.join(constants))\n",
    "\n",
    "            return dataframe.drop(constants, axis=1)\n",
    "\n",
    "        return dataframe\n",
    "\n",
    "    @staticmethod\n",
    "    def drop_non_numeric_values(dataframe):\n",
    "        \"\"\"Preprocess data to remove non numeric columns\n",
    "\n",
    "        :param dataframe: Dataframe that needs to be preprocessed\n",
    "        :type dataframe: pd.DataFrame\n",
    "        :return: Preprocessed Dataframe\n",
    "        :rtype: pd.DataFrame\n",
    "        \"\"\"\n",
    "\n",
    "        return dataframe.select_dtypes(include=['number', 'datetime'])\n",
    "\n",
    "print(\"Ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#CROSS_CORRELATION.PY DE POLARIS, PARA PODER PERSONALIZARLO SEGÚN EL DATASET CONVENGA\n",
    "\"\"\"\n",
    "Cross Correlation module\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import enlighten\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# Used for tracking ML process results\n",
    "from mlflow import log_metric, log_param, log_params, start_run\n",
    "# Used for the pipeline interface of scikit learn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "# eXtreme Gradient Boost algorithm\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Remove this line when feature engineering is in place\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "\n",
    "class XCorr(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Cross Correlation predictor class\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_metadata, cross_correlation_params):\n",
    "        \"\"\" Initialize an XCorr object\n",
    "\n",
    "            :param dataset_metadata: The metadata of the dataset\n",
    "            :type dataset_metadata: PolarisMetadata\n",
    "            :param cross_correlation_params: XCorr parameters\n",
    "            :type cross_correlation_params: CrossCorrelationParameters\n",
    "        \"\"\"\n",
    "        self.models = None\n",
    "        self._importances_map = None\n",
    "        self._feature_cleaner = Cleaner(\n",
    "            dataset_metadata, cross_correlation_params.dataset_cleaning_params)\n",
    "        self.xcorr_params = {\n",
    "            \"random_state\": cross_correlation_params.random_state,\n",
    "            \"test_size\": cross_correlation_params.test_size,\n",
    "            \"gridsearch_scoring\": cross_correlation_params.gridsearch_scoring,\n",
    "            \"gridsearch_n_splits\":\n",
    "            cross_correlation_params.gridsearch_n_splits,\n",
    "        }\n",
    "        # If we're importing from CSV, the dataset_metadata may not\n",
    "        # have the feature_columns key.\n",
    "        try:\n",
    "            self.xcorr_params['feature_columns'] = dataset_metadata[\n",
    "                'analysis']['feature_columns']\n",
    "        except KeyError:\n",
    "            LOGGER.info(\n",
    "                \"No feature_columns entry in metatdata, setting to empty array\"\n",
    "            )\n",
    "            self.xcorr_params['feature_columns'] = []\n",
    "\n",
    "        if cross_correlation_params.use_gridsearch:\n",
    "            self.method = self.gridsearch\n",
    "            self.mlf_logging = self.gridsearch_mlf_logging\n",
    "        else:\n",
    "            self.method = self.regression\n",
    "            self.mlf_logging = self.regression_mlf_logging\n",
    "\n",
    "        self.model_params = {\n",
    "            \"current\": cross_correlation_params.model_params,\n",
    "            \"cpu\": cross_correlation_params.model_cpu_params\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def importances_map(self):\n",
    "        \"\"\"\n",
    "        Return the importances_map value as Pandas Dataframe.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return self._importances_map\n",
    "\n",
    "    @importances_map.setter\n",
    "    def importances_map(self, importances_map):\n",
    "        self._importances_map = importances_map\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\" Train on a dataframe\n",
    "\n",
    "            The input dataframe will be split column by column\n",
    "            considering each one as a prediction target.\n",
    "\n",
    "            :param X: Input dataframe\n",
    "            :type X: pd.DataFrame\n",
    "            :raises Exception: If encountered any unhandled error\n",
    "                during model fitting\n",
    "        \"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input data should be a DataFrame\")\n",
    "\n",
    "        if self.models is None:\n",
    "            self.models = []\n",
    "\n",
    "        manager = enlighten.get_manager()\n",
    "\n",
    "        LOGGER.info(\"Clearing Data. Removing unnecessary columns\")\n",
    "        X = self._feature_cleaner.drop_constant_values(X)\n",
    "        X = self._feature_cleaner.drop_non_numeric_values(X)\n",
    "        X = self._feature_cleaner.handle_missing_values(X)\n",
    "\n",
    "        self.reset_importance_map(X.columns)\n",
    "\n",
    "        parameters = self.__build_parameters(X)\n",
    "\n",
    "        pbar = manager.counter(total=len(parameters),\n",
    "                               desc=\"Columns\",\n",
    "                               unit=\"columns\")\n",
    "\n",
    "        with start_run(run_name='cross_correlate', nested=True):\n",
    "            self.mlf_logging()\n",
    "            for column in parameters:\n",
    "                LOGGER.info(column)\n",
    "                try:\n",
    "                    self.models.append(\n",
    "                        self.method(X.drop([column], axis=1), X[column],\n",
    "                                    self.model_params['current']))\n",
    "                except Exception as err:  # pylint: disable-msg=broad-except\n",
    "                    if self.model_params['current'].get(\n",
    "                            \"predictor\") == \"gpu_predictor\":\n",
    "                        LOGGER.info(\" \".join([\n",
    "                            \"Encountered error using GPU.\",\n",
    "                            \"Trying with CPU parameters now!\"\n",
    "                        ]))\n",
    "                        self.model_params['current'] = self.model_params['cpu']\n",
    "                    else:\n",
    "                        raise err\n",
    "                pbar.update()\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\" Unused method in this predictor \"\"\"\n",
    "        return self\n",
    "\n",
    "    def regression(self, df_in, target_series, model_params):\n",
    "        \"\"\" Fit a model to predict target_series with df_in features/columns\n",
    "            and retain the features importances in the dependency matrix.\n",
    "\n",
    "            :param df_in: Input dataframe representing the context, predictors\n",
    "            :type df_in: pd.DataFrame\n",
    "            :param target_series: pandas series of the target variable. Share\n",
    "                the same indexes as the df_in dataframe\n",
    "            :type target_series: pd.Series\n",
    "            :param model_params: Parameters for the XGB model\n",
    "            :type model_params: dict\n",
    "            :return: A fitted XGBRegressor\n",
    "            :rtype: XGBRegressor\n",
    "        \"\"\"\n",
    "        # Split df_in and target to train and test dataset\n",
    "        df_in_train, df_in_test, target_train, target_test = train_test_split(\n",
    "            df_in,\n",
    "            target_series,\n",
    "            test_size=0.2,\n",
    "            random_state=self.xcorr_params['random_state'])\n",
    "\n",
    "        # Create and train a XGBoost regressor\n",
    "        regr_m = XGBRegressor(**model_params)\n",
    "        regr_m.fit(df_in_train, target_train)\n",
    "\n",
    "        # Make predictions\n",
    "        target_series_predict = regr_m.predict(df_in_test)\n",
    "\n",
    "        try:\n",
    "            rmse = np.sqrt(\n",
    "                mean_squared_error(target_test, target_series_predict))\n",
    "            log_metric(target_series.name, rmse)\n",
    "            LOGGER.info('Making predictions for : %s', target_series.name)\n",
    "            LOGGER.info('Root Mean Square Error : %s', str(rmse))\n",
    "        except Exception:  # pylint: disable-msg=broad-except\n",
    "            # Because of large (close to infinite values) or nans\n",
    "            LOGGER.error('Cannot find RMS Error for %s', target_series.name)\n",
    "            LOGGER.debug('Expected %s, Predicted %s', str(target_test),\n",
    "                         str(target_series_predict))\n",
    "\n",
    "        # indices = np.argsort(regr_m.feature_importances_)[::-1]\n",
    "        # After the model is trained\n",
    "        new_row = {}\n",
    "        for column, feat_imp in zip(df_in.columns,\n",
    "                                    regr_m.feature_importances_):\n",
    "            new_row[column] = [feat_imp]\n",
    "\n",
    "        # Current target is not in df_in, so manually adding it\n",
    "        new_row[target_series.name] = [0.0]\n",
    "\n",
    "        # Sorting new_row to avoid concatenation warnings\n",
    "        new_row = dict(sorted(new_row.items()))\n",
    "\n",
    "        # Concatenating new information about feature importances\n",
    "        if self._importances_map is not None:\n",
    "            self._importances_map = pd.concat([\n",
    "                self._importances_map,\n",
    "                pd.DataFrame(index=[target_series.name], data=new_row)\n",
    "            ])\n",
    "        return regr_m\n",
    "\n",
    "    def gridsearch(self, df_in, target_series, params):\n",
    "        \"\"\" Apply grid search to fine-tune XGBoost hyperparameters\n",
    "            and then call the regression method with the best grid\n",
    "            search parameters.\n",
    "\n",
    "            :param df_in: Input dataframe representing the context, predictors\n",
    "            :type df_in: pd.DataFrame\n",
    "            :param target_series: Pandas series of the target variable. Share\n",
    "                the same indexes as the df_in dataframe\n",
    "            :type target_series: pd.Series\n",
    "            :param params: The hyperparameters to use on the gridsearch\n",
    "                method\n",
    "            :type params: dict\n",
    "            :raises TypeError: If df_in is not Pandas DataFrame\n",
    "            :return: A fitted XGBRegressor\n",
    "            :rtype: XGBRegressor\n",
    "        \"\"\"\n",
    "        if not isinstance(df_in, pd.DataFrame):\n",
    "            LOGGER.error(\"Expected %s got %s for df_in in gridsearch\",\n",
    "                         pd.DataFrame, type(df_in))\n",
    "            raise TypeError\n",
    "\n",
    "        random_state = self.xcorr_params['random_state']\n",
    "        kfolds = KFold(n_splits=self.xcorr_params['gridsearch_n_splits'],\n",
    "                       shuffle=True,\n",
    "                       random_state=random_state)\n",
    "        regr_m = XGBRegressor(random_state=random_state,\n",
    "                              predictor=\"cpu_predictor\",\n",
    "                              tree_method=\"auto\",\n",
    "                              n_jobs=-1)\n",
    "\n",
    "        gs_regr = GridSearchCV(regr_m,\n",
    "                               param_grid=params,\n",
    "                               cv=kfolds,\n",
    "                               scoring=self.xcorr_params['gridsearch_scoring'],\n",
    "                               n_jobs=-1,\n",
    "                               verbose=1)\n",
    "        gs_regr.fit(df_in, target_series)\n",
    "\n",
    "        log_param(target_series.name + ' best estimator', gs_regr.best_params_)\n",
    "        LOGGER.info(\"%s best estimator : %s\", target_series.name,\n",
    "                    str(gs_regr.best_estimator_))\n",
    "        return self.regression(df_in, target_series, gs_regr.best_params_)\n",
    "\n",
    "    def reset_importance_map(self, columns):\n",
    "        \"\"\"\n",
    "        Creating an empty importance map\n",
    "\n",
    "        :param columns: List of column names for the importance map\n",
    "        :rtype columns: pd.Index or array-like\n",
    "        \"\"\"\n",
    "        if self._importances_map is None:\n",
    "            self._importances_map = pd.DataFrame(data={}, columns=columns)\n",
    "\n",
    "    def common_mlf_logging(self):\n",
    "        \"\"\" Log the parameters used for gridsearch and regression\n",
    "            in mlflow\n",
    "        \"\"\"\n",
    "        log_param('Test size', self.xcorr_params['test_size'])\n",
    "        log_param('Model', 'XGBRegressor')\n",
    "\n",
    "    def gridsearch_mlf_logging(self):\n",
    "        \"\"\" Log the parameters used for gridsearch\n",
    "            in mlflow\n",
    "        \"\"\"\n",
    "        log_param('Gridsearch scoring',\n",
    "                  self.xcorr_params['gridsearch_scoring'])\n",
    "        log_param('Gridsearch parameters', self.model_params)\n",
    "        self.common_mlf_logging()\n",
    "\n",
    "    def regression_mlf_logging(self):\n",
    "        \"\"\" Log the parameters used for regression\n",
    "            in mlflow.\n",
    "        \"\"\"\n",
    "        self.common_mlf_logging()\n",
    "        log_params(self.model_params)\n",
    "\n",
    "    def __build_parameters(self, X):\n",
    "        \"\"\" Remove features only from\n",
    "            being predicted.\n",
    "\n",
    "            :param X: The dataset\n",
    "            :type X: pd.DataFrame\n",
    "            :return: List of remaining features that are not removed\n",
    "            :rtype: list\n",
    "        \"\"\"\n",
    "        if self.xcorr_params['feature_columns'] is None:\n",
    "            return list(X.columns)\n",
    "\n",
    "        LOGGER.info('Removing features from the parameters : %s',\n",
    "                    self.xcorr_params['feature_columns'])\n",
    "        feature_to_remove = set(self.xcorr_params['feature_columns'])\n",
    "\n",
    "        return [x for x in list(X.columns) if x not in feature_to_remove]\n",
    "\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoDmNLQib2tg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ANALISYS.PY DE POLARIS, PARA PODER PERSONALIZARLO SEGÚN EL DATASET CONVENGA\n",
    "\n",
    "\"\"\"\n",
    "Module to launch different data analysis.\n",
    "\"\"\"\n",
    "import logging\n",
    "from fets.math import TSIntegrale\n",
    "from mlflow import set_experiment\n",
    "\n",
    "from polaris.data.graph import PolarisGraph\n",
    "from polaris.data.readers import read_polaris_data\n",
    "from polaris.dataset.metadata import PolarisMetadata\n",
    "from polaris.learn.feature.extraction import create_list_of_transformers, \\\n",
    "    extract_best_features\n",
    "from polaris.learn.predictor.cross_correlation_configurator import \\\n",
    "    CrossCorrelationConfigurator\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class NoFramesInInputFile(Exception):\n",
    "    \"\"\"Raised when frames dataframe is empty\"\"\"\n",
    "\n",
    "\n",
    "def feature_extraction(input_file, param_col):\n",
    "    \"\"\"\n",
    "    Start feature extraction using the given settings.\n",
    "\n",
    "        :param input_file: Path of a CSV file that will be\n",
    "            converted to a dataframe\n",
    "        :type input_file: str\n",
    "        :param param_col: Target column name\n",
    "        :type param_col: str\n",
    "    \"\"\"\n",
    "    # Create a small list of two transformers which will generate two\n",
    "    # different pipelines\n",
    "    transformers = create_list_of_transformers([\"5min\", \"15min\"], TSIntegrale)\n",
    "\n",
    "    # Extract the best features of the two pipelines\n",
    "    out = extract_best_features(input_file,\n",
    "                                transformers,\n",
    "                                target_column=param_col,\n",
    "                                time_unit=\"ms\")\n",
    "\n",
    "    # out[0] is the FeatureImportanceOptimization object\n",
    "    # from polaris.learn.feature.selection\n",
    "    # pylint: disable=E1101\n",
    "    print(out[0].best_features)\n",
    "\n",
    "\n",
    "# pylint: disable-msg=too-many-arguments\n",
    "def cross_correlate(input_file,\n",
    "                    output_graph_file=None,\n",
    "                    xcorr_configuration_file=None,\n",
    "                    graph_link_threshold=0.1,\n",
    "                    use_gridsearch=False,\n",
    "                    csv_sep=',',\n",
    "                    force_cpu=False):\n",
    "    \"\"\"\n",
    "    Catch linear and non-linear correlations between all columns of the\n",
    "    input data.\n",
    "\n",
    "        :param input_file: CSV or JSON file path that will be\n",
    "            converted to a dataframe\n",
    "        :type input_file: str\n",
    "        :param output_graph_file: Output file path for the generated graph.\n",
    "            It will overwrite if the file already exists. Defaults to None,\n",
    "            which is'/tmp/polaris_graph.json'\n",
    "        :type output_graph_file: str, optional\n",
    "        :param xcorr_configuration_file: XCorr configuration file path,\n",
    "            defaults to None. Refer to CrossCorrelationConfigurator for\n",
    "            the default parameters\n",
    "        :type xcorr_configuration_file: str, optional\n",
    "        :param graph_link_threshold: Minimum link value to be considered\n",
    "            as a link between two nodes\n",
    "        :type graph_link_threshold: float, optional\n",
    "        :param use_gridsearch: Use grid search for the cross correlation.\n",
    "            If this is set to False, then it will just use regression.\n",
    "            Defaults to False\n",
    "        :type use_gridsearch: bool, optional\n",
    "        :param csv_sep: The character that separates the columns inside of\n",
    "            the CSV file, defaults to ','\n",
    "        :type csv_sep: str, optional\n",
    "        :param force_cpu: Force CPU for cross corelation, defaults to False\n",
    "        :type force_cpu: bool, optional\n",
    "        :raises NoFramesInInputFile: If there are no frames in the converted\n",
    "            dataframe\n",
    "    \"\"\"\n",
    "    # Reading input file - index is considered on first column\n",
    "    metadata, dataframe = read_polaris_data(input_file, csv_sep)\n",
    "\n",
    "    if dataframe.empty:\n",
    "        LOGGER.error(\"Empty list of frames -- nothing to learn from!\")\n",
    "        raise NoFramesInInputFile\n",
    "\n",
    "    input_data = normalize_dataframe(dataframe)\n",
    "    source = metadata['satellite_name']\n",
    "\n",
    "    set_experiment(experiment_name=source)\n",
    "\n",
    "    xcorr_configurator = CrossCorrelationConfigurator(\n",
    "        xcorr_configuration_file=xcorr_configuration_file,\n",
    "        use_gridsearch=use_gridsearch,\n",
    "        force_cpu=force_cpu)\n",
    "\n",
    "    # Creating and fitting cross-correlator\n",
    "    xcorr = XCorr(metadata, xcorr_configurator.get_configuration())\n",
    "    xcorr.fit(input_data)\n",
    "\n",
    "    if output_graph_file is None:\n",
    "        output_graph_file = \"/tmp/polaris_graph.json\"\n",
    "\n",
    "    metadata = PolarisMetadata({\"satellite_name\": source})\n",
    "    graph = PolarisGraph(metadata=metadata)\n",
    "    graph.from_heatmap(xcorr.importances_map, graph_link_threshold)\n",
    "    with open(output_graph_file, 'w') as graph_file:\n",
    "        graph_file.write(graph.to_json())\n",
    "\n",
    "\n",
    "def normalize_dataframe(dataframe):\n",
    "    \"\"\"\n",
    "        Apply dataframe modification so it's compatible\n",
    "        with the learn module. The time column is first\n",
    "        set as the index of the dataframe. Then, we drop\n",
    "        the time column.\n",
    "\n",
    "        :param dataframe: The pandas dataframe to normalize\n",
    "        :type dataframe: pd.DataFrame\n",
    "        :return: Pandas dataframe normalized\n",
    "        :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    #PARA MARS EXPRESS\n",
    "    dataframe.index = dataframe.date\n",
    "    dataframe.drop(['date'], axis=1, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "    #PARA LIGHTSAIL2\n",
    "    #return dataframe\n",
    "\n",
    "    \n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SLFiMJXiTeY",
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#GENERA EL GRAFO DE LIGHTSAIL2\n",
    "cross_correlate(\"lightsail_dataset.csv\")\n",
    "print(\"Ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "Z0vAEqPbePhd",
    "outputId": "e727f3c9-9867-4397-e0fb-df8332c47a33"
   },
   "outputs": [],
   "source": [
    "#GENERA EL GRAFO DE MARS EXPRESS\n",
    "cross_correlate(\"marsexpress_dataset.csv\")\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[\"date\"][2:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPUTACIÓN DE VALORES AUSENTES\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "#df = df.drop(\"date\", axis=1)\n",
    "# Limpieza de datos: imputación valores ausentes (modelo).\n",
    "imp = SimpleImputer(missing_values=0, strategy='mean')\n",
    "imp.fit(df)\n",
    "\n",
    "aa = imp.transform(df)\n",
    "print(aa)\n",
    "display(df.head(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ENCUENTRA OUTLIERS PARA IDENTIFICAR ANOMALÍAS\n",
    "\n",
    "from scipy import stats\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from limpieza_funciones import grafico_outliers\n",
    "\n",
    "# Carga de datos.\n",
    "# Limpieza de datos: detección de outliers.\n",
    "outlier_method = EllipticEnvelope().fit(df)\n",
    "scores_pred = outlier_method.decision_function(df)\n",
    "threshold = stats.scoreatpercentile(scores_pred, 25)\n",
    "# Dibujar gráfica de outliers.\n",
    "grafico_outliers(df, outlier_method, 150, threshold, -7, 7)\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SE COMPRUEBAN LOS OUTLIERS\n",
    "outliers = get_all_outliers(df)\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TFM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
