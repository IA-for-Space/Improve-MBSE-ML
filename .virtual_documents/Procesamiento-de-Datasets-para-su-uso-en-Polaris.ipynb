#!pip install polaris-ml


import os
import pandas as pd
import numpy as np

lista_archivos_lightsail = ["./lightsail2_dataset/" + x for x in os.listdir("./lightsail2_dataset") if x[-3:] == "txt"]
lista_archivos_marsexpress = ["./mars_express_dataset/" + x for x in os.listdir("./mars_express_dataset/") if x[-3:] == "csv"]
print(lista_archivos_lightsail[:5])
print(lista_archivos_marsexpress[:5])




#DATASET PARA LECTURA HUMANA DEL LIGHTSAIL

def read_human_from_txt_lightsail(lista_archivos):

  etiquetas = ["Observed_at", "Posted_at"]

  dataset = {}

  dataset[etiquetas[0]] = []
  dataset[etiquetas[1]] = []

  archivo1 = open(lista_archivos[0], "r")

  for linea in archivo1:
    if "Observed at" in linea:
      valor = linea.split("at:")[1].strip()
      dataset["Observed_at"].append(valor)
    elif "Posted at" in linea:
      valor = linea.split("at:")[1].strip()
      dataset["Posted_at"].append(valor)
    elif ":" in linea and "=" in linea:
      etiqueta = linea.split(":")[1].split("=")[0].strip().replace(" ", "_").replace("[", "_").replace("]", "_").replace("<", "_")
      valor = linea.split(":")[1].split("=")[1].split("[")[0].strip()
      dataset[etiqueta] = []
      dataset[etiqueta].append(valor)

  for nombre_archivo in lista_archivos[1:]:
    archivo = open(nombre_archivo, "r")
    for linea in archivo:
      if "Observed at" in linea:
        valor = linea.split("at:")[1].strip()
        dataset["Observed_at"].append(valor)
      elif "Posted at" in linea:
        valor = linea.split("at:")[1].strip()
        dataset["Posted_at"].append(valor)
      elif ":" in linea and "=" in linea:
        etiqueta = linea.split(":")[1].split("=")[0].strip().replace(" ", "_").replace("[", "_").replace("]", "_").replace("<", "_")
        valor = linea.split(":")[1].split("=")[1].split("[")[0].strip()
        dataset[etiqueta].append(valor)

  df = pd.DataFrame(dataset.values(), dataset.keys())
  df = df.transpose()
  return df





#LECTURA DE LOS DATASETS DE LIGHTSAIL Y MARS EXPRESS
from datetime import datetime

def transform_unix_to_utc(series):
  if len(series) > 0:
    new_series = [str(datetime.utcfromtimestamp(x/1000)) for x in series]
    return new_series
  else:
    return []

def read_from_txt_lightsail():
  dataset = {}
  archivo1 = open(lista_archivos[0], "r")
  kvp_form = False

  for linea in archivo1:
    if "KVP form:" in linea:
      kvp_form = True
    if kvp_form is True and "=" in linea:
        etiqueta = linea.split("=")[0].strip()
        valor = float(linea.split("=")[1].split(",")[0].strip())
        dataset[etiqueta] = []
        dataset[etiqueta].append(valor)

  for nombre_archivo in lista_archivos[1:]:
    archivo = open(nombre_archivo, "r")
    kvp_form = False
    for linea in archivo:
      if "KVP form:" in linea:
        kvp_form = True
      if kvp_form is True and "=" in linea:
        etiqueta = linea.split("=")[0].strip()
        valor = float(linea.split("=")[1].split(",")[0].strip())
        dataset[etiqueta].append(valor)

  df = pd.DataFrame(dataset.values(), dataset.keys())
  df = df.transpose()
  return df


def read_from_ut_ms(lista_archivos):
  columnas = []
  for archivo in lista_archivos:
    df1 = pd.read_csv(archivo)
    columnas += list(df1.columns)
    columnas = list(set(columnas))

  df = pd.DataFrame(columns=columnas)
  df = df.loc[:, ~df.columns.duplicated()]
  for archivo in lista_archivos:
    df2 = pd.read_csv(archivo)
    df2["date"] = transform_unix_to_utc(df2["ut_ms"])
    df = df.append(df2)
  return df

def read_from_utb_ms(lista_archivos):
  columnas = []
  for archivo in lista_archivos:
    df1 = pd.read_csv(archivo)
    columnas = list(df1.columns)
    columnas.append("date")
  df = pd.DataFrame(columns=columnas)
  for archivo in lista_archivos:
    df2 = pd.read_csv(archivo)
    df2["date"] = transform_unix_to_utc(df2["utb_ms"])
    df = df.append(df2)
  return df

def read_from_csv_mars_express(lista_archivos):

  lista_archivos_saaf = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "saaf"]
  lista_archivos_dmop = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "dmop"]
  lista_archivos_ftl = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "ftl"]
  lista_archivos_evtf = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "evtf"]
  lista_archivos_ltdata = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "ltdata"]

  df_saaf = read_from_ut_ms(lista_archivos_saaf)
  df_dmop = read_from_ut_ms(lista_archivos_dmop)
  df_ftl = read_from_utb_ms(lista_archivos_ftl)
  df_evtf = read_from_ut_ms(lista_archivos_evtf)
  df_ltdata = read_from_ut_ms(lista_archivos_ltdata)

  all_columns = list(df_saaf.columns) + list(df_dmop.columns) + list(df_ftl.columns) + list(df_evtf.columns) + list(df_ltdata.columns)
  all_columns = list(set(all_columns))

  df = pd.DataFrame(columns=all_columns)
  df = df.append(df_saaf, ignore_index=True)
  df = df.append(df_dmop, ignore_index=True)
  df = df.append(df_ftl, ignore_index=True)
  df = df.append(df_evtf, ignore_index=True)
  df = df.append(df_ltdata, ignore_index=True).sort_values("date")


  return df





#COMPROBAR RESULTADOS ANTES DE UTILIZAR POLARIS
df = read_from_csv_mars_express(lista_archivos_marsexpress)
display(df)





#GUARDAR CSV DEL LIGHTSAIL2 PARA PROCESAMIENTO DE POLARIS
df_csv = df.to_csv('lightsail_dataset.csv', sep=',', index=False)



#GUARDAR CSV DEL MARS EXPRESS PARA PROCESAMIENTO DE POLARIS
df_csv = df.to_csv('marsexpress_dataset.csv', sep=',', index=False)


#ANALISYS.PY DE POLARIS, PARA PODER PERSONALIZARLO SEGÚN EL DATASET CONVENGA

"""
Module to launch different data analysis.
"""
import logging

from fets.math import TSIntegrale
from mlflow import set_experiment

from polaris.data.graph import PolarisGraph
from polaris.data.readers import read_polaris_data
from polaris.dataset.metadata import PolarisMetadata
from polaris.learn.feature.extraction import create_list_of_transformers, \
    extract_best_features
from polaris.learn.predictor.cross_correlation import XCorr
from polaris.learn.predictor.cross_correlation_configurator import \
    CrossCorrelationConfigurator

LOGGER = logging.getLogger(__name__)


class NoFramesInInputFile(Exception):
    """Raised when frames dataframe is empty"""


def feature_extraction(input_file, param_col):
    """
    Start feature extraction using the given settings.

        :param input_file: Path of a CSV file that will be
            converted to a dataframe
        :type input_file: str
        :param param_col: Target column name
        :type param_col: str
    """
    # Create a small list of two transformers which will generate two
    # different pipelines
    transformers = create_list_of_transformers(["5min", "15min"], TSIntegrale)

    # Extract the best features of the two pipelines
    out = extract_best_features(input_file,
                                transformers,
                                target_column=param_col,
                                time_unit="ms")

    # out[0] is the FeatureImportanceOptimization object
    # from polaris.learn.feature.selection
    # pylint: disable=E1101
    print(out[0].best_features)


# pylint: disable-msg=too-many-arguments
def cross_correlate(input_file,
                    output_graph_file=None,
                    xcorr_configuration_file=None,
                    graph_link_threshold=0.1,
                    use_gridsearch=False,
                    csv_sep=',',
                    force_cpu=False):
    """
    Catch linear and non-linear correlations between all columns of the
    input data.

        :param input_file: CSV or JSON file path that will be
            converted to a dataframe
        :type input_file: str
        :param output_graph_file: Output file path for the generated graph.
            It will overwrite if the file already exists. Defaults to None,
            which is'/tmp/polaris_graph.json'
        :type output_graph_file: str, optional
        :param xcorr_configuration_file: XCorr configuration file path,
            defaults to None. Refer to CrossCorrelationConfigurator for
            the default parameters
        :type xcorr_configuration_file: str, optional
        :param graph_link_threshold: Minimum link value to be considered
            as a link between two nodes
        :type graph_link_threshold: float, optional
        :param use_gridsearch: Use grid search for the cross correlation.
            If this is set to False, then it will just use regression.
            Defaults to False
        :type use_gridsearch: bool, optional
        :param csv_sep: The character that separates the columns inside of
            the CSV file, defaults to ','
        :type csv_sep: str, optional
        :param force_cpu: Force CPU for cross corelation, defaults to False
        :type force_cpu: bool, optional
        :raises NoFramesInInputFile: If there are no frames in the converted
            dataframe
    """
    # Reading input file - index is considered on first column
    metadata, dataframe = read_polaris_data(input_file, csv_sep)

    if dataframe.empty:
        LOGGER.error("Empty list of frames -- nothing to learn from!")
        raise NoFramesInInputFile

    input_data = normalize_dataframe(dataframe)
    source = metadata['satellite_name']

    set_experiment(experiment_name=source)

    xcorr_configurator = CrossCorrelationConfigurator(
        xcorr_configuration_file=xcorr_configuration_file,
        use_gridsearch=use_gridsearch,
        force_cpu=force_cpu)

    # Creating and fitting cross-correlator
    xcorr = XCorr(metadata, xcorr_configurator.get_configuration())
    xcorr.fit(input_data)

    if output_graph_file is None:
        output_graph_file = "/tmp/polaris_graph.json"

    metadata = PolarisMetadata({"satellite_name": source})
    graph = PolarisGraph(metadata=metadata)
    graph.from_heatmap(xcorr.importances_map, graph_link_threshold)
    with open(output_graph_file, 'w') as graph_file:
        graph_file.write(graph.to_json())


def normalize_dataframe(dataframe):
    """
        Apply dataframe modification so it's compatible
        with the learn module. The time column is first
        set as the index of the dataframe. Then, we drop
        the time column.

        :param dataframe: The pandas dataframe to normalize
        :type dataframe: pd.DataFrame
        :return: Pandas dataframe normalized
        :rtype: pd.DataFrame
    """
    dataframe.index = dataframe.date
    dataframe.drop(['date'], axis=1, inplace=True)

    return dataframe



#GENERA EL GRAFO DE LIGHTSAIL2
cross_correlate("lightsail_dataset.csv")



#GENERA EL GRAFO DE MARS EXPRESS
cross_correlate("marsexpress_dataset.csv")
