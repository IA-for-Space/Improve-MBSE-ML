#!pip install polaris-ml
#!pip install fets
#!pip install mlflow


import os
import pandas as pd
import numpy as np

lista_archivos_lightsail = ["./lightsail2_dataset/" + x for x in os.listdir("./lightsail2_dataset") if x[-3:] == "txt"]
lista_archivos_marsexpress = ["./mars_express_dataset/" + x for x in os.listdir("./mars_express_dataset/") if x[-3:] == "csv"]
print(lista_archivos_lightsail[:5])
print(lista_archivos_marsexpress[:5])

print("Ok")



#DATASET PARA LECTURA HUMANA DEL LIGHTSAIL

def read_human_from_txt_lightsail(lista_archivos):

  etiquetas = ["Observed_at", "Posted_at"]

  dataset = {}

  dataset[etiquetas[0]] = []
  dataset[etiquetas[1]] = []

  archivo1 = open(lista_archivos[0], "r")

  for linea in archivo1:
    if "Observed at" in linea:
      valor = linea.split("at:")[1].strip()
      dataset["Observed_at"].append(valor)
    elif "Posted at" in linea:
      valor = linea.split("at:")[1].strip()
      dataset["Posted_at"].append(valor)
    elif ":" in linea and "=" in linea:
      etiqueta = linea.split(":")[1].split("=")[0].strip().replace(" ", "_").replace("[", "_").replace("]", "_").replace("<", "_")
      valor = linea.split(":")[1].split("=")[1].split("[")[0].strip()
      dataset[etiqueta] = []
      dataset[etiqueta].append(valor)

  for nombre_archivo in lista_archivos[1:]:
    archivo = open(nombre_archivo, "r")
    for linea in archivo:
      if "Observed at" in linea:
        valor = linea.split("at:")[1].strip()
        dataset["Observed_at"].append(valor)
      elif "Posted at" in linea:
        valor = linea.split("at:")[1].strip()
        dataset["Posted_at"].append(valor)
      elif ":" in linea and "=" in linea:
        etiqueta = linea.split(":")[1].split("=")[0].strip().replace(" ", "_").replace("[", "_").replace("]", "_").replace("<", "_")
        valor = linea.split(":")[1].split("=")[1].split("[")[0].strip()
        dataset[etiqueta].append(valor)

  df = pd.DataFrame(dataset.values(), dataset.keys())
  df = df.transpose()
  return df

print("Ok")




#LECTURA DE LOS DATASETS DE LIGHTSAIL2 Y MARS EXPRESS
from datetime import datetime
import json

def transform_unix_to_utc(series):
  if len(series) > 0:
    new_series = [str(datetime.utcfromtimestamp(x/1000))[:-3] for x in series]
    return new_series
  else:
    return []

def read_from_txt_lightsail(lista_archivos):
  dataset = {}
  archivo1 = open(lista_archivos[0], "r")
  kvp_form = False

  for linea in archivo1:
    if "KVP form:" in linea:
      kvp_form = True
    if kvp_form is True and "=" in linea:
        etiqueta = linea.split("=")[0].strip()
        valor = float(linea.split("=")[1].split(",")[0].strip())
        dataset[etiqueta] = []
        dataset[etiqueta].append(valor)

  for nombre_archivo in lista_archivos[1:]:
    archivo = open(nombre_archivo, "r")
    kvp_form = False
    for linea in archivo:
      if "KVP form:" in linea:
        kvp_form = True
      if kvp_form is True and "=" in linea:
        etiqueta = linea.split("=")[0].strip()
        valor = float(linea.split("=")[1].split(",")[0].strip())
        dataset[etiqueta].append(valor)

  df = pd.DataFrame(dataset.values(), dataset.keys())
  df = df.transpose()
  return df


def read_from_ut_ms(lista_archivos):
  columnas = []
  for archivo in lista_archivos:
    df1 = pd.read_csv(archivo)
    columnas += list(df1.columns)
    columnas = list(set(columnas))

  df = pd.DataFrame(columns=columnas)
  df = df.loc[:, ~df.columns.duplicated()]
  for archivo in lista_archivos:
    df2 = pd.read_csv(archivo)
    df2["date"] = transform_unix_to_utc(df2["ut_ms"])
    df = df.append(df2)
  return df.head(1000)

def read_from_utb_ms(lista_archivos):
  columnas = []
  for archivo in lista_archivos:
    df1 = pd.read_csv(archivo)
    columnas = list(df1.columns)
    columnas.append("date")
  df = pd.DataFrame(columns=columnas)
  for archivo in lista_archivos:
    df2 = pd.read_csv(archivo)
    df2["date"] = transform_unix_to_utc(df2["utb_ms"])
    df = df.append(df2)
  return df.head(1000)

def replace_strings_with_int(series):
    collection = list(set(series))
    new_series = [collection.index(x)+1 for x in series]
    return new_series

def build_json_mars_express(lista_archivos):
    all_jsons = []
    for archivo in lista_archivos:
        df = pd.read_csv(archivo).head(1000)
        for col, row in df.iterrows():
            new_json = dict({"time":"", "measurament":"", "tags":{"satellite":"Mars Express", "decoder":"", "station":"", "observer":"", "source":"", "version":""}, "fields":{}})
            new_json["time"] = str(datetime.utcfromtimestamp(row["ut_ms"]/1000))
            new_json["fields"] = dict(row)
            all_jsons.append(new_json)
    fp = open('decoded_frames.json', 'w')
    fp.write(json.dumps(all_jsons, indent=3))
    fp.close()
    
        
def read_from_csv_mars_express(lista_archivos):

  lista_archivos_saaf = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "saaf"]
  lista_archivos_dmop = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "dmop"]
  lista_archivos_ftl = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "ftl"]
  lista_archivos_evtf = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "evtf"]
  lista_archivos_ltdata = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "ltdata"]

  df_saaf = read_from_ut_ms(lista_archivos_saaf).drop("ut_ms", axis=1)
  df_dmop = read_from_ut_ms(lista_archivos_dmop).drop("ut_ms", axis=1)
  df_ftl = read_from_utb_ms(lista_archivos_ftl)
  df_evtf = read_from_ut_ms(lista_archivos_evtf).drop("ut_ms", axis=1)
  df_ltdata = read_from_ut_ms(lista_archivos_ltdata).drop("ut_ms", axis=1)
    
        
  df = df_saaf
  df = df.join(df_dmop.set_index("date"), on="date", how="outer")
  df = df.join(df_ftl.set_index("date"), on="date", how="outer")
  df = df.join(df_evtf.set_index("date"), on="date", how="outer")
  df = df.join(df_ltdata.set_index("date"), on="date", how="outer").sort_values("date")

  df = df.fillna(0)

  df["subsystem"] = replace_strings_with_int(df["subsystem"])
  df["type"] = replace_strings_with_int(df["type"])
  df["description"] = replace_strings_with_int(df["description"])
  df["flagcomms"] = replace_strings_with_int(df["flagcomms"])

  df = df.drop_duplicates()
  return df

print("Ok")



#SE ALMACENAN LOS DATASETS EN JSONS
lista_archivos_ltdata = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "ltdata"]
build_json_mars_express(lista_archivos_ltdata)


#LECTURA DE LOS DATASETS DE MARS EXPRESS, Y DETECCIÓN DE OUTLIERS
from datetime import datetime
from scipy import stats
from sklearn.covariance import EllipticEnvelope
from limpieza_funciones import grafico_outliers


def get_outliers(df):
    # Limpieza de datos: detección de outliers.
    outlier_method = EllipticEnvelope().fit(df)
    scores_pred = outlier_method.decision_function(df)
    
    Q1 = stats.scoreatpercentile(scores_pred, 25)
    Q3 = stats.scoreatpercentile(scores_pred, 75)
    RIC = Q3 - Q1
    limite_inferior = Q1 - 1.5*RIC
    limite_superior = Q3 + 1.5*RIC
    
    # Estimación de outliers
    pos_i = np.where(scores_pred < limite_inferior)
    pos_s = np.where(scores_pred > limite_superior)
    
    # Matriz de outliers
    mask_outliers = np.ones(np.shape(scores_pred))
    mask_outliers[pos_i] = 0
    mask_outliers[pos_s] = 0
    return mask_outliers

def get_one_dataframe(lista_archivos):
  columnas = []
  for archivo in lista_archivos:
    df1 = pd.read_csv(archivo)
    columnas += list(df1.columns)
    columnas = list(set(columnas))

  df = pd.DataFrame(columns=columnas)
  df = df.loc[:, ~df.columns.duplicated()]
  for archivo in lista_archivos:
    df2 = pd.read_csv(archivo)
    df = df.append(df2)
  return df.head(1000)


def replace_strings_with_int(series):
    collection = list(set(series))
    new_series = [collection.index(x)+1 for x in series]
    return new_series

def check_csv_mars_express(lista_archivos):

  lista_archivos_saaf = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "saaf"]
  lista_archivos_dmop = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "dmop"]
  lista_archivos_ftl = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "ftl"]
  lista_archivos_evtf = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "evtf"]
  lista_archivos_ltdata = [x for x in lista_archivos_marsexpress if x.split("--")[-1].split(".")[0] == "ltdata"]

  #df_saaf = get_one_dataframe(lista_archivos_saaf)
  #df_dmop = get_one_dataframe(lista_archivos_dmop) 
  #df_ftl = get_one_dataframe(lista_archivos_ftl)
  #df_evtf = get_one_dataframe(lista_archivos_evtf)
  df_ltdata = get_one_dataframe(lista_archivos_ltdata)
    
  #df_saaf_outliers = get_outliers(df_saaf)
  #df_dmop_outliers = get_outliers(df_dmop) #no son valores numéricos
  #df_ftl_outliers = get_outliers(df_ftl) #no son valores numéricos
  #df_evtf_outliers = get_outliers(df_evtf) #no son valores numéricos
  df_ltdata_outliers = get_outliers(df_ltdata)
  print(df_ltdata_outliers[df_ltdata_outliers==0].shape)
 
  return df_ltdata.iloc[df_ltdata_outliers==0]
print("Ok")



#COMPROBAR RESULTADOS ANTES DE UTILIZAR POLARIS
outs = check_csv_mars_express(lista_archivos_marsexpress)
print(outs)


#COMPROBAR RESULTADOS ANTES DE UTILIZAR POLARIS
df = read_from_txt_lightsail(lista_archivos_lightsail)
display(df.head(25))


#COMPROBAR RESULTADOS ANTES DE UTILIZAR POLARIS
df = read_from_csv_mars_express(lista_archivos_marsexpress)
display(df.head(25))


display(df.head(25))
print("subsystem: ", len(set(df["subsystem"])),len(df["subsystem"]))
print("type: ", len(set(df["type"])),len(df["type"]))
print("description: ", len(set(df["description"])),len(df["description"]))
print("flagcomms: ", len(set(df["flagcomms"])),len(df["flagcomms"]), set(df["flagcomms"]))


#datatest1 = pd.DataFrame({"Nombre": ["Daenerys", "Deku", "Bakugo", "Peter Pan", "Akamatsu", "Spiderman", "Joey Mills"], "Serie": ["Juego de Tronos", "BNHA", "BNHA", "Peter Pan", "Akamatsu y Seven", "Spiderman", "Helix Studios"], "Me gusta": ["True", "True", "True", "False", "True", "True", "Supertrue"]})
#datatest2 = pd.DataFrame({"Nombre": ["Davos", "Homer", "Joey", "Jay", "Seven", "Loki", "Blake Mitchell"], "Serie": ["Juego de Tronos", "Simpsons", "Friends", "Modern Family", "Akamatsu y Seven", "Loki", "Helix Studios"], "Me gusta": ["False", "True", "True", "False", "True", "True", "True"]})
#datatest3 = pd.DataFrame({"Nombre": ["Arya", "Peter", "Ted", "Gloria", "El primo violador", "Wanda", "Steven Prior"], "Serie": ["Juego de Tronos", "Padre de Familia", "HIMYM", "Modern Family", "Akamatsu y Seven", "Wandavision", "Tim Tales"], "Me gusta": ["True", "False", "False", "True", "False", "True", "True"]})



datatest1 = pd.DataFrame({"Fecha": ["07/01/2022", "08/01/2022", "09/01/2022", "10/01/2022", "11/01/2022"],
                          "Clima en Elche": ["Soleado", "Nublado", "Niebla", "Nublado", "Lluvioso"]})

datatest2 = pd.DataFrame({"Fecha": ["07/01/2022", "08/01/2022", "09/01/2022", "10/01/2022", "11/01/2022"],
                          "Noticia del día": ["Secuestran a peter", "Vuelve el volcán de la palma", "Cancelan a Energuia", "Memes de Jordi Wild", "Sube la luz"]})

datatest3 = pd.DataFrame({"Fecha": ["07/01/2022", "08/01/2022", "09/01/2022", "10/01/2022", "11/01/2022", "12/10/2022"],
                          "Gasolina de mi coche": ["Nada", "Mucha", "Mucha", "Media", "Poca", "Muy Poca"],
                          "Precio del pan": [0.5, 0.5, 0.5, 0.6, 0.6, 0.9]})

df = datatest1
df = df.join(datatest2.set_index("Fecha"), on="Fecha", how="outer")
df = df.join(datatest3.set_index("Fecha"), on="Fecha", how="outer")

display(df)


#GUARDAR CSV DEL LIGHTSAIL2 PARA PROCESAMIENTO DE POLARIS
df_csv = df.to_csv('lightsail_dataset.csv', sep=',', index=False)
print("Ok")



#GUARDAR CSV DEL MARS EXPRESS PARA PROCESAMIENTO DE POLARIS
df_csv = df.to_csv('marsexpress_dataset.csv', sep=',', index=False)
print("Ok")


#CLEANER.PY DE POLARIS, PARA PODER PERSONALIZARLO SEGÚN EL DATASET CONVENGA

"""Module for cleaning data
"""

import logging

LOGGER = logging.getLogger(__name__)


# pylint: disable=R0903
class Cleaner:
    """Class for cleaning features.
    """
    def __init__(self, metadata, cleaning_params):
        # in percent, maximum na rows in a column
        self._col_threshold = cleaning_params.col_max_na_percentage
        # in percent, maximum na columns in a row
        self._row_threshold = cleaning_params.row_max_na_percentage
        self._metadata = metadata

    def handle_missing_values(self, dataframe):
        """Preprocess data to remove unnecessary rows and columns (filled with
        nan)

        :param dataframe: Dataframe that needs to be preprocessed
        :type dataframe: pd.DataFrame
        :return: Preprocessed Dataframe
        :rtype: pd.DataFrame
        """
        initial_shape = dataframe.shape

        # Dropping columns first so that frames without necessary columns
        # of data are removed.
        # Remove columns not satisfying criteria
        count_na_col = dataframe.isna().sum()
        count_na_col = count_na_col * (100 / dataframe.shape[0])
        dataframe = dataframe.loc[:, count_na_col < self._col_threshold]

        # Remove rows not satisfying criteria
        count_na_row = dataframe.isna().sum(axis=1)
        if dataframe.shape[1] != 0:
            count_na_row = count_na_row * (100 / dataframe.shape[1])
        else:
            count_na_row = 0

        dataframe = dataframe.loc[count_na_row < self._row_threshold, :]

        # ffill will fill all but the first set of nans
        # bfill will fill the first set of nans
        # (if nans are present in the first few rows)
        # Fill nans with nearest value
        dataframe = dataframe.fillna(method="ffill")
        dataframe = dataframe.fillna(method="bfill")

        final_shape = dataframe.shape

        LOGGER.debug("Initial Shape: %s, Final Shape: %s", initial_shape,
                     final_shape)

        return dataframe

    def drop_constant_values(self, dataframe):
        """Preprocess data to remove columns with
        constant values

        :param dataframe: Dataframe that needs to be preprocessed
        :type dataframe: pd.DataFrame
        :return: Preprocessed Dataframe
        :rtype: pd.DataFrame
        """
        if 'analysis' in self._metadata:
            constants = [
                column for column, tag in self._metadata['analysis']
                ['column_tags'].items() if tag == "constant"
            ]

            LOGGER.info('Dropping constant column(s) : %s',
                        ','.join(constants))

            return dataframe.drop(constants, axis=1)

        return dataframe

    @staticmethod
    def drop_non_numeric_values(dataframe):
        """Preprocess data to remove non numeric columns

        :param dataframe: Dataframe that needs to be preprocessed
        :type dataframe: pd.DataFrame
        :return: Preprocessed Dataframe
        :rtype: pd.DataFrame
        """

        return dataframe.select_dtypes(include=['number', 'datetime'])

print("Ok")



#CROSS_CORRELATION.PY DE POLARIS, PARA PODER PERSONALIZARLO SEGÚN EL DATASET CONVENGA
"""
Cross Correlation module
"""

import logging
import warnings

import enlighten
import numpy as np
import pandas as pd
# Used for tracking ML process results
from mlflow import log_metric, log_param, log_params, start_run
# Used for the pipeline interface of scikit learn
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV, KFold, train_test_split
# eXtreme Gradient Boost algorithm
from xgboost import XGBRegressor


LOGGER = logging.getLogger(__name__)
warnings.simplefilter(action='ignore', category=FutureWarning)
# Remove this line when feature engineering is in place
np.seterr(divide='ignore', invalid='ignore')


class XCorr(BaseEstimator, TransformerMixin):
    """ Cross Correlation predictor class
    """
    def __init__(self, dataset_metadata, cross_correlation_params):
        """ Initialize an XCorr object

            :param dataset_metadata: The metadata of the dataset
            :type dataset_metadata: PolarisMetadata
            :param cross_correlation_params: XCorr parameters
            :type cross_correlation_params: CrossCorrelationParameters
        """
        self.models = None
        self._importances_map = None
        self._feature_cleaner = Cleaner(
            dataset_metadata, cross_correlation_params.dataset_cleaning_params)
        self.xcorr_params = {
            "random_state": cross_correlation_params.random_state,
            "test_size": cross_correlation_params.test_size,
            "gridsearch_scoring": cross_correlation_params.gridsearch_scoring,
            "gridsearch_n_splits":
            cross_correlation_params.gridsearch_n_splits,
        }
        # If we're importing from CSV, the dataset_metadata may not
        # have the feature_columns key.
        try:
            self.xcorr_params['feature_columns'] = dataset_metadata[
                'analysis']['feature_columns']
        except KeyError:
            LOGGER.info(
                "No feature_columns entry in metatdata, setting to empty array"
            )
            self.xcorr_params['feature_columns'] = []

        if cross_correlation_params.use_gridsearch:
            self.method = self.gridsearch
            self.mlf_logging = self.gridsearch_mlf_logging
        else:
            self.method = self.regression
            self.mlf_logging = self.regression_mlf_logging

        self.model_params = {
            "current": cross_correlation_params.model_params,
            "cpu": cross_correlation_params.model_cpu_params
        }

    @property
    def importances_map(self):
        """
        Return the importances_map value as Pandas Dataframe.

        """

        return self._importances_map

    @importances_map.setter
    def importances_map(self, importances_map):
        self._importances_map = importances_map

    def fit(self, X):
        """ Train on a dataframe

            The input dataframe will be split column by column
            considering each one as a prediction target.

            :param X: Input dataframe
            :type X: pd.DataFrame
            :raises Exception: If encountered any unhandled error
                during model fitting
        """
        if not isinstance(X, pd.DataFrame):
            raise TypeError("Input data should be a DataFrame")

        if self.models is None:
            self.models = []

        manager = enlighten.get_manager()

        LOGGER.info("Clearing Data. Removing unnecessary columns")
        X = self._feature_cleaner.drop_constant_values(X)
        X = self._feature_cleaner.drop_non_numeric_values(X)
        X = self._feature_cleaner.handle_missing_values(X)

        self.reset_importance_map(X.columns)

        parameters = self.__build_parameters(X)

        pbar = manager.counter(total=len(parameters),
                               desc="Columns",
                               unit="columns")

        with start_run(run_name='cross_correlate', nested=True):
            self.mlf_logging()
            for column in parameters:
                LOGGER.info(column)
                try:
                    self.models.append(
                        self.method(X.drop([column], axis=1), X[column],
                                    self.model_params['current']))
                except Exception as err:  # pylint: disable-msg=broad-except
                    if self.model_params['current'].get(
                            "predictor") == "gpu_predictor":
                        LOGGER.info(" ".join([
                            "Encountered error using GPU.",
                            "Trying with CPU parameters now!"
                        ]))
                        self.model_params['current'] = self.model_params['cpu']
                    else:
                        raise err
                pbar.update()

    def transform(self):
        """ Unused method in this predictor """
        return self

    def regression(self, df_in, target_series, model_params):
        """ Fit a model to predict target_series with df_in features/columns
            and retain the features importances in the dependency matrix.

            :param df_in: Input dataframe representing the context, predictors
            :type df_in: pd.DataFrame
            :param target_series: pandas series of the target variable. Share
                the same indexes as the df_in dataframe
            :type target_series: pd.Series
            :param model_params: Parameters for the XGB model
            :type model_params: dict
            :return: A fitted XGBRegressor
            :rtype: XGBRegressor
        """
        # Split df_in and target to train and test dataset
        df_in_train, df_in_test, target_train, target_test = train_test_split(
            df_in,
            target_series,
            test_size=0.2,
            random_state=self.xcorr_params['random_state'])

        # Create and train a XGBoost regressor
        regr_m = XGBRegressor(**model_params)
        regr_m.fit(df_in_train, target_train)

        # Make predictions
        target_series_predict = regr_m.predict(df_in_test)

        try:
            rmse = np.sqrt(
                mean_squared_error(target_test, target_series_predict))
            log_metric(target_series.name, rmse)
            LOGGER.info('Making predictions for : %s', target_series.name)
            LOGGER.info('Root Mean Square Error : %s', str(rmse))
        except Exception:  # pylint: disable-msg=broad-except
            # Because of large (close to infinite values) or nans
            LOGGER.error('Cannot find RMS Error for %s', target_series.name)
            LOGGER.debug('Expected %s, Predicted %s', str(target_test),
                         str(target_series_predict))

        # indices = np.argsort(regr_m.feature_importances_)[::-1]
        # After the model is trained
        new_row = {}
        for column, feat_imp in zip(df_in.columns,
                                    regr_m.feature_importances_):
            new_row[column] = [feat_imp]

        # Current target is not in df_in, so manually adding it
        new_row[target_series.name] = [0.0]

        # Sorting new_row to avoid concatenation warnings
        new_row = dict(sorted(new_row.items()))

        # Concatenating new information about feature importances
        if self._importances_map is not None:
            self._importances_map = pd.concat([
                self._importances_map,
                pd.DataFrame(index=[target_series.name], data=new_row)
            ])
        return regr_m

    def gridsearch(self, df_in, target_series, params):
        """ Apply grid search to fine-tune XGBoost hyperparameters
            and then call the regression method with the best grid
            search parameters.

            :param df_in: Input dataframe representing the context, predictors
            :type df_in: pd.DataFrame
            :param target_series: Pandas series of the target variable. Share
                the same indexes as the df_in dataframe
            :type target_series: pd.Series
            :param params: The hyperparameters to use on the gridsearch
                method
            :type params: dict
            :raises TypeError: If df_in is not Pandas DataFrame
            :return: A fitted XGBRegressor
            :rtype: XGBRegressor
        """
        if not isinstance(df_in, pd.DataFrame):
            LOGGER.error("Expected %s got %s for df_in in gridsearch",
                         pd.DataFrame, type(df_in))
            raise TypeError

        random_state = self.xcorr_params['random_state']
        kfolds = KFold(n_splits=self.xcorr_params['gridsearch_n_splits'],
                       shuffle=True,
                       random_state=random_state)
        regr_m = XGBRegressor(random_state=random_state,
                              predictor="cpu_predictor",
                              tree_method="auto",
                              n_jobs=-1)

        gs_regr = GridSearchCV(regr_m,
                               param_grid=params,
                               cv=kfolds,
                               scoring=self.xcorr_params['gridsearch_scoring'],
                               n_jobs=-1,
                               verbose=1)
        gs_regr.fit(df_in, target_series)

        log_param(target_series.name + ' best estimator', gs_regr.best_params_)
        LOGGER.info("%s best estimator : %s", target_series.name,
                    str(gs_regr.best_estimator_))
        return self.regression(df_in, target_series, gs_regr.best_params_)

    def reset_importance_map(self, columns):
        """
        Creating an empty importance map

        :param columns: List of column names for the importance map
        :rtype columns: pd.Index or array-like
        """
        if self._importances_map is None:
            self._importances_map = pd.DataFrame(data={}, columns=columns)

    def common_mlf_logging(self):
        """ Log the parameters used for gridsearch and regression
            in mlflow
        """
        log_param('Test size', self.xcorr_params['test_size'])
        log_param('Model', 'XGBRegressor')

    def gridsearch_mlf_logging(self):
        """ Log the parameters used for gridsearch
            in mlflow
        """
        log_param('Gridsearch scoring',
                  self.xcorr_params['gridsearch_scoring'])
        log_param('Gridsearch parameters', self.model_params)
        self.common_mlf_logging()

    def regression_mlf_logging(self):
        """ Log the parameters used for regression
            in mlflow.
        """
        self.common_mlf_logging()
        log_params(self.model_params)

    def __build_parameters(self, X):
        """ Remove features only from
            being predicted.

            :param X: The dataset
            :type X: pd.DataFrame
            :return: List of remaining features that are not removed
            :rtype: list
        """
        if self.xcorr_params['feature_columns'] is None:
            return list(X.columns)

        LOGGER.info('Removing features from the parameters : %s',
                    self.xcorr_params['feature_columns'])
        feature_to_remove = set(self.xcorr_params['feature_columns'])

        return [x for x in list(X.columns) if x not in feature_to_remove]


print("Ok")


#ANALISYS.PY DE POLARIS, PARA PODER PERSONALIZARLO SEGÚN EL DATASET CONVENGA

"""
Module to launch different data analysis.
"""
import logging
from fets.math import TSIntegrale
from mlflow import set_experiment

from polaris.data.graph import PolarisGraph
from polaris.data.readers import read_polaris_data
from polaris.dataset.metadata import PolarisMetadata
from polaris.learn.feature.extraction import create_list_of_transformers, \
    extract_best_features
from polaris.learn.predictor.cross_correlation_configurator import \
    CrossCorrelationConfigurator

LOGGER = logging.getLogger(__name__)


class NoFramesInInputFile(Exception):
    """Raised when frames dataframe is empty"""


def feature_extraction(input_file, param_col):
    """
    Start feature extraction using the given settings.

        :param input_file: Path of a CSV file that will be
            converted to a dataframe
        :type input_file: str
        :param param_col: Target column name
        :type param_col: str
    """
    # Create a small list of two transformers which will generate two
    # different pipelines
    transformers = create_list_of_transformers(["5min", "15min"], TSIntegrale)

    # Extract the best features of the two pipelines
    out = extract_best_features(input_file,
                                transformers,
                                target_column=param_col,
                                time_unit="ms")

    # out[0] is the FeatureImportanceOptimization object
    # from polaris.learn.feature.selection
    # pylint: disable=E1101
    print(out[0].best_features)


# pylint: disable-msg=too-many-arguments
def cross_correlate(input_file,
                    output_graph_file=None,
                    xcorr_configuration_file=None,
                    graph_link_threshold=0.1,
                    use_gridsearch=False,
                    csv_sep=',',
                    force_cpu=False):
    """
    Catch linear and non-linear correlations between all columns of the
    input data.

        :param input_file: CSV or JSON file path that will be
            converted to a dataframe
        :type input_file: str
        :param output_graph_file: Output file path for the generated graph.
            It will overwrite if the file already exists. Defaults to None,
            which is'/tmp/polaris_graph.json'
        :type output_graph_file: str, optional
        :param xcorr_configuration_file: XCorr configuration file path,
            defaults to None. Refer to CrossCorrelationConfigurator for
            the default parameters
        :type xcorr_configuration_file: str, optional
        :param graph_link_threshold: Minimum link value to be considered
            as a link between two nodes
        :type graph_link_threshold: float, optional
        :param use_gridsearch: Use grid search for the cross correlation.
            If this is set to False, then it will just use regression.
            Defaults to False
        :type use_gridsearch: bool, optional
        :param csv_sep: The character that separates the columns inside of
            the CSV file, defaults to ','
        :type csv_sep: str, optional
        :param force_cpu: Force CPU for cross corelation, defaults to False
        :type force_cpu: bool, optional
        :raises NoFramesInInputFile: If there are no frames in the converted
            dataframe
    """
    # Reading input file - index is considered on first column
    metadata, dataframe = read_polaris_data(input_file, csv_sep)

    if dataframe.empty:
        LOGGER.error("Empty list of frames -- nothing to learn from!")
        raise NoFramesInInputFile

    input_data = normalize_dataframe(dataframe)
    source = metadata['satellite_name']

    set_experiment(experiment_name=source)

    xcorr_configurator = CrossCorrelationConfigurator(
        xcorr_configuration_file=xcorr_configuration_file,
        use_gridsearch=use_gridsearch,
        force_cpu=force_cpu)

    # Creating and fitting cross-correlator
    xcorr = XCorr(metadata, xcorr_configurator.get_configuration())
    xcorr.fit(input_data)

    if output_graph_file is None:
        output_graph_file = "/tmp/polaris_graph.json"

    metadata = PolarisMetadata({"satellite_name": source})
    graph = PolarisGraph(metadata=metadata)
    graph.from_heatmap(xcorr.importances_map, graph_link_threshold)
    with open(output_graph_file, 'w') as graph_file:
        graph_file.write(graph.to_json())


def normalize_dataframe(dataframe):
    """
        Apply dataframe modification so it's compatible
        with the learn module. The time column is first
        set as the index of the dataframe. Then, we drop
        the time column.

        :param dataframe: The pandas dataframe to normalize
        :type dataframe: pd.DataFrame
        :return: Pandas dataframe normalized
        :rtype: pd.DataFrame
    """
    #PARA MARS EXPRESS
    dataframe.index = dataframe.date
    dataframe.drop(['date'], axis=1, inplace=True)
    return dataframe

    #PARA LIGHTSAIL2
    #return dataframe

    

print("Ok")


#GENERA EL GRAFO DE LIGHTSAIL2
cross_correlate("lightsail_dataset.csv")
print("Ok")



#GENERA EL GRAFO DE MARS EXPRESS
cross_correlate("marsexpress_dataset.csv")
print("Ok")


display(df["date"][2:5])


#IMPUTACIÓN DE VALORES AUSENTES
import numpy as np
from sklearn.impute import SimpleImputer
#df = df.drop("date", axis=1)
# Limpieza de datos: imputación valores ausentes (modelo).
imp = SimpleImputer(missing_values=0, strategy='mean')
imp.fit(df)

aa = imp.transform(df)
print(aa)
display(df.head(25))


#ENCUENTRA OUTLIERS PARA IDENTIFICAR ANOMALÍAS

from scipy import stats
from sklearn.covariance import EllipticEnvelope
from limpieza_funciones import grafico_outliers

# Carga de datos.
# Limpieza de datos: detección de outliers.
outlier_method = EllipticEnvelope().fit(df)
scores_pred = outlier_method.decision_function(df)
threshold = stats.scoreatpercentile(scores_pred, 25)
# Dibujar gráfica de outliers.
grafico_outliers(df, outlier_method, 150, threshold, -7, 7)

print("Ok")


#SE COMPRUEBAN LOS OUTLIERS
outliers = get_all_outliers(df)
print(outliers)



