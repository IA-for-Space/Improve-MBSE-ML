{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rICw0MTsMtam",
    "outputId": "4acf6428-30d0-4699-8dbe-e002169aa480"
   },
   "outputs": [],
   "source": [
    "#!pip install polaris-ml\n",
    "#!pip install fets\n",
    "#!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RlwcETrdgSBZ",
    "outputId": "80d937e4-3ea8-4554-9ccd-bb2dafc31ca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"Ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cross Correlation module\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import enlighten\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from polaris.feature.cleaner import Cleaner\n",
    "# Used for tracking ML process results\n",
    "from mlflow import log_metric, log_param, log_params, start_run\n",
    "# Used for the pipeline interface of scikit learn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "\n",
    "# eXtreme Gradient Boost algorithm\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#RandomForest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Extratrees regressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "#AdaBoostRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "#GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#HistGradientBoostingRegressor\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "#VotingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Remove this line when feature engineering is in place\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "\n",
    "class XCorr(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Cross Correlation predictor class\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_metadata, cross_correlation_params, regressor):\n",
    "        \"\"\" Initialize an XCorr object\n",
    "\n",
    "            :param dataset_metadata: The metadata of the dataset\n",
    "            :type dataset_metadata: PolarisMetadata\n",
    "            :param cross_correlation_params: XCorr parameters\n",
    "            :type cross_correlation_params: CrossCorrelationParameters\n",
    "        \"\"\"\n",
    "        self._regressor = regressor\n",
    "        self.models = None\n",
    "        self._importances_map = None\n",
    "        self._feature_cleaner = Cleaner(\n",
    "            dataset_metadata, cross_correlation_params.dataset_cleaning_params)\n",
    "        self.xcorr_params = {\n",
    "            \"random_state\": cross_correlation_params.random_state,\n",
    "            \"test_size\": cross_correlation_params.test_size,\n",
    "            \"gridsearch_scoring\": cross_correlation_params.gridsearch_scoring,\n",
    "            \"gridsearch_n_splits\":\n",
    "            cross_correlation_params.gridsearch_n_splits,\n",
    "        }\n",
    "        # If we're importing from CSV, the dataset_metadata may not\n",
    "        # have the feature_columns key.\n",
    "        try:\n",
    "            self.xcorr_params['feature_columns'] = dataset_metadata[\n",
    "                'analysis']['feature_columns']\n",
    "        except KeyError:\n",
    "            LOGGER.info(\n",
    "                \"No feature_columns entry in metatdata, setting to empty array\"\n",
    "            )\n",
    "            self.xcorr_params['feature_columns'] = []\n",
    "\n",
    "        if cross_correlation_params.use_gridsearch:\n",
    "            self.method = self.gridsearch\n",
    "            self.mlf_logging = self.gridsearch_mlf_logging\n",
    "        else:\n",
    "            self.method = self.regression\n",
    "            self.mlf_logging = self.regression_mlf_logging\n",
    "\n",
    "        self.model_params = {\n",
    "            \"current\": cross_correlation_params.model_params,\n",
    "            \"cpu\": cross_correlation_params.model_cpu_params\n",
    "        }\n",
    "        \n",
    "    @property\n",
    "    def regressor(self):\n",
    "        return self._regressor\n",
    "    \n",
    "    @regressor.setter\n",
    "    def regressor(self, regressor):\n",
    "        self._regressor = regressor\n",
    "\n",
    "    @property\n",
    "    def importances_map(self):\n",
    "        \"\"\"\n",
    "        Return the importances_map value as Pandas Dataframe.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return self._importances_map\n",
    "\n",
    "    @importances_map.setter\n",
    "    def importances_map(self, importances_map):\n",
    "        self._importances_map = importances_map\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\" Train on a dataframe\n",
    "\n",
    "            The input dataframe will be split column by column\n",
    "            considering each one as a prediction target.\n",
    "\n",
    "            :param X: Input dataframe\n",
    "            :type X: pd.DataFrame\n",
    "            :raises Exception: If encountered any unhandled error\n",
    "                during model fitting\n",
    "        \"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input data should be a DataFrame\")\n",
    "\n",
    "        if self.models is None:\n",
    "            self.models = []\n",
    "\n",
    "        manager = enlighten.get_manager()\n",
    "\n",
    "        LOGGER.info(\"Clearing Data. Removing unnecessary columns\")\n",
    "        X = self._feature_cleaner.drop_constant_values(X)\n",
    "        X = self._feature_cleaner.drop_non_numeric_values(X)\n",
    "        X = self._feature_cleaner.handle_missing_values(X)\n",
    "\n",
    "        self.reset_importance_map(X.columns)\n",
    "\n",
    "        parameters = self.__build_parameters(X)\n",
    "\n",
    "        pbar = manager.counter(total=len(parameters),\n",
    "                               desc=\"Columns\",\n",
    "                               unit=\"columns\")\n",
    "\n",
    "        with start_run(run_name='cross_correlate', nested=True):\n",
    "            self.mlf_logging()\n",
    "            for column in parameters:\n",
    "                LOGGER.info(column)\n",
    "                try:\n",
    "                    self.models.append(\n",
    "                        self.method(X.drop([column], axis=1), X[column],\n",
    "                                    self.model_params['current']))\n",
    "                except Exception as err:  # pylint: disable-msg=broad-except\n",
    "                    if self.model_params['current'].get(\n",
    "                            \"predictor\") == \"gpu_predictor\":\n",
    "                        LOGGER.info(\" \".join([\n",
    "                            \"Encountered error using GPU.\",\n",
    "                            \"Trying with CPU parameters now!\"\n",
    "                        ]))\n",
    "                        self.model_params['current'] = self.model_params['cpu']\n",
    "                    else:\n",
    "                        raise err\n",
    "                pbar.update()\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\" Unused method in this predictor \"\"\"\n",
    "        return self\n",
    "    \n",
    "        \n",
    "    def regression(self, df_in, target_series, model_params):\n",
    "        \"\"\" Fit a model to predict target_series with df_in features/columns\n",
    "            and retain the features importances in the dependency matrix.\n",
    "\n",
    "            :param df_in: Input dataframe representing the context, predictors\n",
    "            :type df_in: pd.DataFrame\n",
    "            :param target_series: pandas series of the target variable. Share\n",
    "                the same indexes as the df_in dataframe\n",
    "            :type target_series: pd.Series\n",
    "            :param model_params: Parameters for the XGB model\n",
    "            :type model_params: dict\n",
    "            :return: A fitted XGBRegressor\n",
    "            :rtype: XGBRegressor\n",
    "        \"\"\"\n",
    "        # Split df_in and target to train and test dataset\n",
    "        df_in_train, df_in_test, target_train, target_test = train_test_split(\n",
    "            df_in,\n",
    "            target_series,\n",
    "            test_size=0.2,\n",
    "            random_state=self.xcorr_params['random_state'])\n",
    "\n",
    "\n",
    "        regressors_dict = {\"XGB\": XGBRegressor(**model_params),\n",
    "                           \"RandomForest\": RandomForestRegressor(),\n",
    "                           \"AdaBoost\": AdaBoostRegressor(),\n",
    "                           \"ExtraTrees\": ExtraTreesRegressor(),\n",
    "                           \"GradientBoosting\": GradientBoostingRegressor(),\n",
    "                           \"Voting\": \"VotingRegressor()\"}\n",
    "\n",
    "        \"\"\" if self._regressor == \"XGboosting\":\n",
    "            # Create and train a XGBoost regressor\n",
    "            regr_m = XGBRegressor(**model_params)\n",
    "            \n",
    "        elif self._regressor == \"RandomForest\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            regr_m = RandomForestRegressor()\n",
    "        \n",
    "        elif self._regressor == \"AdaBoost\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            regr_m = AdaBoostRegressor()\n",
    "        \n",
    "        elif self._regressor == \"ExtraTrees\":\n",
    "             # Create and train a Sci-kit regressor\n",
    "            regr_m = ExtraTreesRegressor()\n",
    "        \n",
    "        elif self._regressor == \"GradientBoosting\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            regr_m = GradientBoostingRegressor()\n",
    "        \n",
    "        elif self._regressor == \"HistGradientBoosting\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            #regr_m = HistGradientBoostingRegressor()\n",
    "            pass\n",
    "            \n",
    "        elif self._regressor == \"Voting\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            regr_m = VotingRegressor()\n",
    "        \n",
    "        \"\"\"\n",
    "        regr_m = regressors_dict[self._regressor]\n",
    "        regr_m.fit(df_in_train, target_train)\n",
    "\n",
    "        # Make predictions\n",
    "        target_series_predict = regr_m.predict(df_in_test)\n",
    "\n",
    "        try:\n",
    "            rmse = np.sqrt(\n",
    "                mean_squared_error(target_test, target_series_predict))\n",
    "            log_metric(target_series.name, rmse)\n",
    "            LOGGER.info('Making predictions for : %s', target_series.name)\n",
    "            LOGGER.info('Root Mean Square Error : %s', str(rmse))\n",
    "        except Exception:  # pylint: disable-msg=broad-except\n",
    "            # Because of large (close to infinite values) or nans\n",
    "            LOGGER.error('Cannot find RMS Error for %s', target_series.name)\n",
    "            LOGGER.debug('Expected %s, Predicted %s', str(target_test),\n",
    "                         str(target_series_predict))\n",
    "\n",
    "        # indices = np.argsort(regr_m.feature_importances_)[::-1]\n",
    "        # After the model is trained\n",
    "        new_row = {}\n",
    "        for column, feat_imp in zip(df_in.columns,\n",
    "                                    regr_m.feature_importances_):\n",
    "            new_row[column] = [feat_imp]\n",
    "\n",
    "        # Current target is not in df_in, so manually adding it\n",
    "        new_row[target_series.name] = [0.0]\n",
    "\n",
    "        # Sorting new_row to avoid concatenation warnings\n",
    "        new_row = dict(sorted(new_row.items()))\n",
    "\n",
    "        # Concatenating new information about feature importances\n",
    "        if self._importances_map is not None:\n",
    "            self._importances_map = pd.concat([\n",
    "                self._importances_map,\n",
    "                pd.DataFrame(index=[target_series.name], data=new_row)\n",
    "            ])\n",
    "        return regr_m\n",
    "\n",
    "    def gridsearch(self, df_in, target_series, params):\n",
    "        \"\"\" Apply grid search to fine-tune XGBoost hyperparameters\n",
    "            and then call the regression method with the best grid\n",
    "            search parameters.\n",
    "\n",
    "            :param df_in: Input dataframe representing the context, predictors\n",
    "            :type df_in: pd.DataFrame\n",
    "            :param target_series: Pandas series of the target variable. Share\n",
    "                the same indexes as the df_in dataframe\n",
    "            :type target_series: pd.Series\n",
    "            :param params: The hyperparameters to use on the gridsearch\n",
    "                method\n",
    "            :type params: dict\n",
    "            :raises TypeError: If df_in is not Pandas DataFrame\n",
    "            :return: A fitted XGBRegressor\n",
    "            :rtype: XGBRegressor\n",
    "        \"\"\"\n",
    "        if not isinstance(df_in, pd.DataFrame):\n",
    "            LOGGER.error(\"Expected %s got %s for df_in in gridsearch\",\n",
    "                         pd.DataFrame, type(df_in))\n",
    "            raise TypeError\n",
    "\n",
    "        random_state = self.xcorr_params['random_state']\n",
    "        kfolds = KFold(n_splits=self.xcorr_params['gridsearch_n_splits'],\n",
    "                       shuffle=True,\n",
    "                       random_state=random_state)\n",
    "        regr_m = XGBRegressor(random_state=random_state,\n",
    "                              predictor=\"cpu_predictor\",\n",
    "                              tree_method=\"auto\",\n",
    "                              n_jobs=-1)\n",
    "\n",
    "        gs_regr = GridSearchCV(regr_m,\n",
    "                               param_grid=params,\n",
    "                               cv=kfolds,\n",
    "                               scoring=self.xcorr_params['gridsearch_scoring'],\n",
    "                               n_jobs=-1,\n",
    "                               verbose=1)\n",
    "        gs_regr.fit(df_in, target_series)\n",
    "\n",
    "        log_param(target_series.name + ' best estimator', gs_regr.best_params_)\n",
    "        LOGGER.info(\"%s best estimator : %s\", target_series.name,\n",
    "                    str(gs_regr.best_estimator_))\n",
    "        return self.regression(df_in, target_series, gs_regr.best_params_)\n",
    "\n",
    "    def reset_importance_map(self, columns):\n",
    "        \"\"\"\n",
    "        Creating an empty importance map\n",
    "\n",
    "        :param columns: List of column names for the importance map\n",
    "        :rtype columns: pd.Index or array-like\n",
    "        \"\"\"\n",
    "        if self._importances_map is None:\n",
    "            self._importances_map = pd.DataFrame(data={}, columns=columns)\n",
    "\n",
    "    def common_mlf_logging(self):\n",
    "        \"\"\" Log the parameters used for gridsearch and regression\n",
    "            in mlflow\n",
    "        \"\"\"\n",
    "        log_param('Test size', self.xcorr_params['test_size'])\n",
    "        log_param('Model', 'XGBRegressor')\n",
    "\n",
    "    def gridsearch_mlf_logging(self):\n",
    "        \"\"\" Log the parameters used for gridsearch\n",
    "            in mlflow\n",
    "        \"\"\"\n",
    "        log_param('Gridsearch scoring',\n",
    "                  self.xcorr_params['gridsearch_scoring'])\n",
    "        log_param('Gridsearch parameters', self.model_params)\n",
    "        self.common_mlf_logging()\n",
    "\n",
    "    def regression_mlf_logging(self):\n",
    "        \"\"\" Log the parameters used for regression\n",
    "            in mlflow.\n",
    "        \"\"\"\n",
    "        self.common_mlf_logging()\n",
    "        log_params(self.model_params)\n",
    "\n",
    "    def __build_parameters(self, X):\n",
    "        \"\"\" Remove features only from\n",
    "            being predicted.\n",
    "\n",
    "            :param X: The dataset\n",
    "            :type X: pd.DataFrame\n",
    "            :return: List of remaining features that are not removed\n",
    "            :rtype: list\n",
    "        \"\"\"\n",
    "        if self.xcorr_params['feature_columns'] is None:\n",
    "            return list(X.columns)\n",
    "\n",
    "        LOGGER.info('Removing features from the parameters : %s',\n",
    "                    self.xcorr_params['feature_columns'])\n",
    "        feature_to_remove = set(self.xcorr_params['feature_columns'])\n",
    "\n",
    "        return [x for x in list(X.columns) if x not in feature_to_remove]\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Module to launch different data analysis.\n",
    "\"\"\"\n",
    "import logging\n",
    "\n",
    "from fets.math import TSIntegrale\n",
    "from mlflow import set_experiment\n",
    "import polaris\n",
    "from polaris.data.graph import PolarisGraph\n",
    "from polaris.data.readers import read_polaris_data\n",
    "from polaris.dataset.metadata import PolarisMetadata\n",
    "from polaris.learn.feature.extraction import create_list_of_transformers, \\\n",
    "    extract_best_features\n",
    "from polaris.learn.predictor.cross_correlation_configurator import \\\n",
    "    CrossCorrelationConfigurator\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class NoFramesInInputFile(Exception):\n",
    "    \"\"\"Raised when frames dataframe is empty\"\"\"\n",
    "\n",
    "\n",
    "def feature_extraction(input_file, param_col):\n",
    "    \"\"\"\n",
    "    Start feature extraction using the given settings.\n",
    "\n",
    "        :param input_file: Path of a CSV file that will be\n",
    "            converted to a dataframe\n",
    "        :type input_file: str\n",
    "        :param param_col: Target column name\n",
    "        :type param_col: str\n",
    "    \"\"\"\n",
    "    # Create a small list of two transformers which will generate two\n",
    "    # different pipelines\n",
    "    transformers = create_list_of_transformers([\"5min\", \"15min\"], TSIntegrale)\n",
    "\n",
    "    # Extract the best features of the two pipelines\n",
    "    out = extract_best_features(input_file,\n",
    "                                transformers,\n",
    "                                target_column=param_col,\n",
    "                                time_unit=\"ms\")\n",
    "\n",
    "    # out[0] is the FeatureImportanceOptimization object\n",
    "    # from polaris.learn.feature.selection\n",
    "    # pylint: disable=E1101\n",
    "    print(out[0].best_features)\n",
    "\n",
    "\n",
    "# pylint: disable-msg=too-many-arguments\n",
    "def cross_correlate(input_file,\n",
    "                    regressor=\"XGB\",\n",
    "                    output_graph_file=None,\n",
    "                    xcorr_configuration_file=None,\n",
    "                    graph_link_threshold=0.1,\n",
    "                    use_gridsearch=False,\n",
    "                    csv_sep=',',\n",
    "                    force_cpu=False):\n",
    "    \"\"\"\n",
    "    Catch linear and non-linear correlations between all columns of the\n",
    "    input data.\n",
    "\n",
    "        :param input_file: CSV or JSON file path that will be\n",
    "            converted to a dataframe\n",
    "        :type input_file: str\n",
    "        :param output_graph_file: Output file path for the generated graph.\n",
    "            It will overwrite if the file already exists. Defaults to None,\n",
    "            which is'/tmp/polaris_graph.json'\n",
    "        :type output_graph_file: str, optional\n",
    "        :param xcorr_configuration_file: XCorr configuration file path,\n",
    "            defaults to None. Refer to CrossCorrelationConfigurator for\n",
    "            the default parameters\n",
    "        :type xcorr_configuration_file: str, optional\n",
    "        :param graph_link_threshold: Minimum link value to be considered\n",
    "            as a link between two nodes\n",
    "        :type graph_link_threshold: float, optional\n",
    "        :param use_gridsearch: Use grid search for the cross correlation.\n",
    "            If this is set to False, then it will just use regression.\n",
    "            Defaults to False\n",
    "        :type use_gridsearch: bool, optional\n",
    "        :param csv_sep: The character that separates the columns inside of\n",
    "            the CSV file, defaults to ','\n",
    "        :type csv_sep: str, optional\n",
    "        :param force_cpu: Force CPU for cross corelation, defaults to False\n",
    "        :type force_cpu: bool, optional\n",
    "        :raises NoFramesInInputFile: If there are no frames in the converted\n",
    "            dataframe\n",
    "    \"\"\"\n",
    "    # Reading input file - index is considered on first column\n",
    "    metadata, dataframe = read_polaris_data(input_file, csv_sep)\n",
    "\n",
    "    if dataframe.empty:\n",
    "        LOGGER.error(\"Empty list of frames -- nothing to learn from!\")\n",
    "        raise NoFramesInInputFile\n",
    "\n",
    "    input_data = normalize_dataframe(dataframe)\n",
    "    source = metadata['satellite_name']\n",
    "\n",
    "    set_experiment(experiment_name=source)\n",
    "\n",
    "    xcorr_configurator = CrossCorrelationConfigurator(\n",
    "        xcorr_configuration_file=xcorr_configuration_file,\n",
    "        use_gridsearch=use_gridsearch,\n",
    "        force_cpu=force_cpu)\n",
    "\n",
    "    # Creating and fitting cross-correlator\n",
    "    xcorr = XCorr(metadata, xcorr_configurator.get_configuration(), regressor)\n",
    "    xcorr.fit(input_data)\n",
    "\n",
    "    if output_graph_file is None:\n",
    "        output_graph_file = \"/tmp/polaris_graph_\"+ regressor +\".json\"\n",
    "\n",
    "    metadata = PolarisMetadata({\"satellite_name\": source})\n",
    "    graph = PolarisGraph(metadata=metadata)\n",
    "    graph.from_heatmap(xcorr.importances_map, graph_link_threshold)\n",
    "    with open(output_graph_file, 'w') as graph_file:\n",
    "        graph_file.write(graph.to_json())\n",
    "\n",
    "\n",
    "def normalize_dataframe(dataframe):\n",
    "    \"\"\"\n",
    "        Apply dataframe modification so it's compatible\n",
    "        with the learn module. The time column is first\n",
    "        set as the index of the dataframe. Then, we drop\n",
    "        the time column.\n",
    "\n",
    "        :param dataframe: The pandas dataframe to normalize\n",
    "        :type dataframe: pd.DataFrame\n",
    "        :return: Pandas dataframe normalized\n",
    "        :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    dataframe.index = dataframe.ut_ms\n",
    "    dataframe.drop(['ut_ms'], axis=1, inplace=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoosting\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Columns 100%|██████████████████████████████████████████████████| 59/59 [02:38&lt;00:00, 0.37 columns/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#GENERA LOS GRAFOS\n",
    "\n",
    "print(\"XGBoosting\")\n",
    "cross_correlate(\"marsexpress_dataset.csv\", \"XGB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Columns 100%|██████████████████████████████████████████████████| 70/70 [08:43&lt;00:00, 0.13 columns/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"AdaBoost\")\n",
    "cross_correlate(\"marsexpress_dataset_medium.csv\", \"AdaBoost\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient boosting\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Columns 100%|██████████████████████████████████████████████████| 70/70 [24:43&lt;00:00, 0.05 columns/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"Gradient boosting\")\n",
    "cross_correlate(\"marsexpress_dataset_medium.csv\", \"GradientBoosting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Columns 100%|███████████████████████████████████████████████| 70/70 [1h 28:19&lt;00:00, 0.01 columns/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Random Forest\")\n",
    "cross_correlate(\"marsexpress_dataset_medium.csv\", \"RandomForest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra trees\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Columns 100%|██████████████████████████████████████████████████| 70/70 [33:10&lt;00:00, 0.04 columns/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"Extra trees\")\n",
    "cross_correlate(\"marsexpress_dataset_medium.csv\", \"ExtraTrees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "from graph_to_object import getObjectFromGraph\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def count_graph_nodes_and_links(file):\n",
    "    with open(file + \".json\") as f:\n",
    "        graph_1 = json.loads(f.read())\n",
    "    list_graph_nodes = [x[\"id\"] for x in graph_1[\"graph\"][\"nodes\"]]\n",
    "    df_graph_links = pd.DataFrame([x for x in graph_1[\"graph\"][\"links\"]])\n",
    "    return df_graph_links, list_graph_nodes\n",
    "\n",
    "\n",
    "def get_free_nodes(df_links, list_nodes):\n",
    "    free_nodes = []\n",
    "    for node in list_nodes:\n",
    "        if(node not in list(df_links[\"target\"]) and node not in list(df_links[\"source\"]) ):\n",
    "            free_nodes.append(node)\n",
    "    return free_nodes\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Método</th>\n",
       "      <th>NumNodos</th>\n",
       "      <th>Num Links</th>\n",
       "      <th>Num Nodos Libres</th>\n",
       "      <th>Intensidad De Relación Media</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB</td>\n",
       "      <td>170</td>\n",
       "      <td>279</td>\n",
       "      <td>37</td>\n",
       "      <td>0.417207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>70</td>\n",
       "      <td>129</td>\n",
       "      <td>3</td>\n",
       "      <td>0.286504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>70</td>\n",
       "      <td>158</td>\n",
       "      <td>2</td>\n",
       "      <td>0.215193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>70</td>\n",
       "      <td>122</td>\n",
       "      <td>7</td>\n",
       "      <td>0.220408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>70</td>\n",
       "      <td>158</td>\n",
       "      <td>0</td>\n",
       "      <td>0.296550</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Método  NumNodos  Num Links  Num Nodos Libres  \\\n",
       "0               XGB       170        279                37   \n",
       "1      RandomForest        70        129                 3   \n",
       "2          AdaBoost        70        158                 2   \n",
       "3        ExtraTrees        70        122                 7   \n",
       "4  GradientBoosting        70        158                 0   \n",
       "\n",
       "   Intensidad De Relación Media  \n",
       "0                      0.417207  \n",
       "1                      0.286504  \n",
       "2                      0.215193  \n",
       "3                      0.220408  \n",
       "4                      0.296550  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['NPWD2372',\n",
       " 'NPWD2401',\n",
       " 'NPWD2402',\n",
       " 'NPWD2451',\n",
       " 'NPWD2471',\n",
       " 'NPWD2472',\n",
       " 'NPWD2481',\n",
       " 'NPWD2482',\n",
       " 'NPWD2491',\n",
       " 'NPWD2501']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NPWD2372</td>\n",
       "      <td>NPWD2491</td>\n",
       "      <td>0.214113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NPWD2372</td>\n",
       "      <td>NPWD2791</td>\n",
       "      <td>0.170794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NPWD2372</td>\n",
       "      <td>PSF</td>\n",
       "      <td>0.263657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NPWD2401</td>\n",
       "      <td>NPWD2482</td>\n",
       "      <td>0.827711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NPWD2401</td>\n",
       "      <td>NPWD2722</td>\n",
       "      <td>0.111661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NPWD2471</td>\n",
       "      <td>NPWD2871</td>\n",
       "      <td>0.100953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NPWD2472</td>\n",
       "      <td>NPWD2792</td>\n",
       "      <td>0.746384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>NPWD2482</td>\n",
       "      <td>NPWD2401</td>\n",
       "      <td>0.846373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>NPWD2482</td>\n",
       "      <td>NPWD2722</td>\n",
       "      <td>0.132483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>NPWD2491</td>\n",
       "      <td>NPWD2372</td>\n",
       "      <td>0.220936</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source    target     value\n",
       "0  NPWD2372  NPWD2491  0.214113\n",
       "1  NPWD2372  NPWD2791  0.170794\n",
       "2  NPWD2372       PSF  0.263657\n",
       "3  NPWD2401  NPWD2482  0.827711\n",
       "4  NPWD2401  NPWD2722  0.111661\n",
       "5  NPWD2471  NPWD2871  0.100953\n",
       "6  NPWD2472  NPWD2792  0.746384\n",
       "7  NPWD2482  NPWD2401  0.846373\n",
       "8  NPWD2482  NPWD2722  0.132483\n",
       "9  NPWD2491  NPWD2372  0.220936"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_XGB_links, list_XGB_nodes  = count_graph_nodes_and_links(\"lightsail_graphs/polaris_graph_XGB\")\n",
    "df_randomforest_links, list_randomforest_nodes = count_graph_nodes_and_links(\"marsexpress_graphs/polaris_graph_randomforest\")\n",
    "df_adaboost_links, list_adaboost_nodes = count_graph_nodes_and_links(\"marsexpress_graphs/polaris_graph_adaboost\")\n",
    "df_extratrees_links, list_extratrees_nodes = count_graph_nodes_and_links(\"marsexpress_graphs/polaris_graph_extratrees\")\n",
    "df_gradientboosting_links, list_gradientboosting_nodes = count_graph_nodes_and_links(\"marsexpress_graphs/polaris_graph_GradientBoosting\")\n",
    "\n",
    "todos ={\"Método\":[\"XGB\", \"RandomForest\", \"AdaBoost\", \"ExtraTrees\", \"GradientBoosting\"],\n",
    "        \"NumNodos\": [len(list_XGB_nodes), len(list_randomforest_nodes), len(list_adaboost_nodes), len(list_extratrees_nodes), len(list_gradientboosting_nodes)],\n",
    "       \"Num Links\": [df_XGB_links.shape[0], df_randomforest_links.shape[0], df_adaboost_links.shape[0], df_extratrees_links.shape[0], df_gradientboosting_links.shape[0] ],\n",
    "       \"Num Nodos Libres\": [len(get_free_nodes(df_XGB_links, list_XGB_nodes)), len(get_free_nodes(df_randomforest_links, list_randomforest_nodes)), len(get_free_nodes(df_adaboost_links, list_adaboost_nodes)), len(get_free_nodes(df_extratrees_links, list_extratrees_nodes )), len(get_free_nodes(df_gradientboosting_links, list_gradientboosting_nodes)) ],\n",
    "       \"Intensidad De Relación Media\":[df_XGB_links[\"value\"].mean(), df_randomforest_links[\"value\"].mean(), df_adaboost_links[\"value\"].mean(), df_extratrees_links[\"value\"].mean(), df_gradientboosting_links[\"value\"].mean() ]}\n",
    "\n",
    "display(pd.DataFrame(todos))\n",
    "display(list_extratrees_nodes[:10])\n",
    "display(df_extratrees_links.head(10))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TFM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "de072921dc87486613898b1ef56959cc98c50a630fb49de1898fb32d92a683cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
