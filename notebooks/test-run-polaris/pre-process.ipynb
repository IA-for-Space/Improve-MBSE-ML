{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rICw0MTsMtam",
    "outputId": "4acf6428-30d0-4699-8dbe-e002169aa480"
   },
   "outputs": [],
   "source": [
    "#!pip install polaris-ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RlwcETrdgSBZ",
    "outputId": "80d937e4-3ea8-4554-9ccd-bb2dafc31ca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./lightsail2_dataset/2127458.txt', './lightsail2_dataset/2127459.txt', './lightsail2_dataset/2127460.txt', './lightsail2_dataset/2127461.txt', './lightsail2_dataset/2127462.txt']\n",
      "['./mars_express_dataset/context--2008-08-22_2010-07-10--dmop.csv', './mars_express_dataset/context--2008-08-22_2010-07-10--evtf.csv', './mars_express_dataset/context--2008-08-22_2010-07-10--ftl.csv', './mars_express_dataset/context--2008-08-22_2010-07-10--ltdata.csv', './mars_express_dataset/context--2010-07-10_2012-05-27--dmop.csv']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "lista_archivos_lightsail = [\"./lightsail2/\" + x for x in os.listdir(\"./lightsail2\") if x[-3:] == \"txt\"]\n",
    "lista_archivos_marsexpress = [\"./mars_express/\" + x for x in os.listdir(\"./mars_express/\") if x[-3:] == \"csv\"]\n",
    "print(lista_archivos_lightsail[:5])\n",
    "print(lista_archivos_marsexpress[:5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mKOK2W_sacnM"
   },
   "outputs": [],
   "source": [
    "#DATASET PARA LECTURA HUMANA DEL LIGHTSAIL\n",
    "\n",
    "def read_human_from_txt_lightsail(lista_archivos):\n",
    "\n",
    "  etiquetas = [\"Observed_at\", \"Posted_at\"]\n",
    "\n",
    "  dataset = {}\n",
    "\n",
    "  dataset[etiquetas[0]] = []\n",
    "  dataset[etiquetas[1]] = []\n",
    "\n",
    "  archivo1 = open(lista_archivos[0], \"r\")\n",
    "\n",
    "  for linea in archivo1:\n",
    "    if \"Observed at\" in linea:\n",
    "      valor = linea.split(\"at:\")[1].strip()\n",
    "      dataset[\"Observed_at\"].append(valor)\n",
    "    elif \"Posted at\" in linea:\n",
    "      valor = linea.split(\"at:\")[1].strip()\n",
    "      dataset[\"Posted_at\"].append(valor)\n",
    "    elif \":\" in linea and \"=\" in linea:\n",
    "      etiqueta = linea.split(\":\")[1].split(\"=\")[0].strip().replace(\" \", \"_\").replace(\"[\", \"_\").replace(\"]\", \"_\").replace(\"<\", \"_\")\n",
    "      valor = linea.split(\":\")[1].split(\"=\")[1].split(\"[\")[0].strip()\n",
    "      dataset[etiqueta] = []\n",
    "      dataset[etiqueta].append(valor)\n",
    "\n",
    "  for nombre_archivo in lista_archivos[1:]:\n",
    "    archivo = open(nombre_archivo, \"r\")\n",
    "    for linea in archivo:\n",
    "      if \"Observed at\" in linea:\n",
    "        valor = linea.split(\"at:\")[1].strip()\n",
    "        dataset[\"Observed_at\"].append(valor)\n",
    "      elif \"Posted at\" in linea:\n",
    "        valor = linea.split(\"at:\")[1].strip()\n",
    "        dataset[\"Posted_at\"].append(valor)\n",
    "      elif \":\" in linea and \"=\" in linea:\n",
    "        etiqueta = linea.split(\":\")[1].split(\"=\")[0].strip().replace(\" \", \"_\").replace(\"[\", \"_\").replace(\"]\", \"_\").replace(\"<\", \"_\")\n",
    "        valor = linea.split(\":\")[1].split(\"=\")[1].split(\"[\")[0].strip()\n",
    "        dataset[etiqueta].append(valor)\n",
    "\n",
    "  df = pd.DataFrame(dataset.values(), dataset.keys())\n",
    "  df = df.transpose()\n",
    "  return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "OSL_Yph0WzBZ"
   },
   "outputs": [],
   "source": [
    "#LECTURA DE LOS DATASETS DE LIGHTSAIL Y MARS EXPRESS\n",
    "from datetime import datetime\n",
    "\n",
    "def transform_unix_to_utc(series):\n",
    "  if len(series) > 0:\n",
    "    new_series = [str(datetime.utcfromtimestamp(x/1000)) for x in series]\n",
    "    return new_series\n",
    "  else:\n",
    "    return []\n",
    "\n",
    "def read_from_txt_lightsail():\n",
    "  dataset = {}\n",
    "  archivo1 = open(lista_archivos[0], \"r\")\n",
    "  kvp_form = False\n",
    "\n",
    "  for linea in archivo1:\n",
    "    if \"KVP form:\" in linea:\n",
    "      kvp_form = True\n",
    "    if kvp_form is True and \"=\" in linea:\n",
    "        etiqueta = linea.split(\"=\")[0].strip()\n",
    "        valor = float(linea.split(\"=\")[1].split(\",\")[0].strip())\n",
    "        dataset[etiqueta] = []\n",
    "        dataset[etiqueta].append(valor)\n",
    "\n",
    "  for nombre_archivo in lista_archivos[1:]:\n",
    "    archivo = open(nombre_archivo, \"r\")\n",
    "    kvp_form = False\n",
    "    for linea in archivo:\n",
    "      if \"KVP form:\" in linea:\n",
    "        kvp_form = True\n",
    "      if kvp_form is True and \"=\" in linea:\n",
    "        etiqueta = linea.split(\"=\")[0].strip()\n",
    "        valor = float(linea.split(\"=\")[1].split(\",\")[0].strip())\n",
    "        dataset[etiqueta].append(valor)\n",
    "\n",
    "  df = pd.DataFrame(dataset.values(), dataset.keys())\n",
    "  df = df.transpose()\n",
    "  return df\n",
    "\n",
    "\n",
    "def read_from_ut_ms(lista_archivos):\n",
    "  columnas = []\n",
    "  for archivo in lista_archivos:\n",
    "    df1 = pd.read_csv(archivo)\n",
    "    columnas += list(df1.columns)\n",
    "    columnas = list(set(columnas))\n",
    "\n",
    "  df = pd.DataFrame(columns=columnas)\n",
    "  df = df.loc[:, ~df.columns.duplicated()]\n",
    "  for archivo in lista_archivos:\n",
    "    df2 = pd.read_csv(archivo)\n",
    "    df2[\"date\"] = transform_unix_to_utc(df2[\"ut_ms\"])\n",
    "    df = df.append(df2)\n",
    "  return df\n",
    "\n",
    "def read_from_utb_ms(lista_archivos):\n",
    "  columnas = []\n",
    "  for archivo in lista_archivos:\n",
    "    df1 = pd.read_csv(archivo)\n",
    "    columnas = list(df1.columns)\n",
    "    columnas.append(\"date\")\n",
    "  df = pd.DataFrame(columns=columnas)\n",
    "  for archivo in lista_archivos:\n",
    "    df2 = pd.read_csv(archivo)\n",
    "    df2[\"date\"] = transform_unix_to_utc(df2[\"utb_ms\"])\n",
    "    df = df.append(df2)\n",
    "  return df\n",
    "\n",
    "def read_from_csv_mars_express(lista_archivos):\n",
    "\n",
    "  lista_archivos_saaf = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"saaf\"]\n",
    "  lista_archivos_dmop = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"dmop\"]\n",
    "  lista_archivos_ftl = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"ftl\"]\n",
    "  lista_archivos_evtf = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"evtf\"]\n",
    "  lista_archivos_ltdata = [x for x in lista_archivos_marsexpress if x.split(\"--\")[-1].split(\".\")[0] == \"ltdata\"]\n",
    "\n",
    "  df_saaf = read_from_ut_ms(lista_archivos_saaf)\n",
    "  df_dmop = read_from_ut_ms(lista_archivos_dmop)\n",
    "  df_ftl = read_from_utb_ms(lista_archivos_ftl)\n",
    "  df_evtf = read_from_ut_ms(lista_archivos_evtf)\n",
    "  df_ltdata = read_from_ut_ms(lista_archivos_ltdata)\n",
    "\n",
    "  all_columns = list(df_saaf.columns) + list(df_dmop.columns) + list(df_ftl.columns) + list(df_evtf.columns) + list(df_ltdata.columns)\n",
    "  all_columns = list(set(all_columns))\n",
    "\n",
    "  df = pd.DataFrame(columns=all_columns)\n",
    "  df = df.append(df_saaf, ignore_index=True)\n",
    "  df = df.append(df_dmop, ignore_index=True)\n",
    "  df = df.append(df_ftl, ignore_index=True)\n",
    "  df = df.append(df_evtf, ignore_index=True)\n",
    "  df = df.append(df_ltdata, ignore_index=True).sort_values(\"date\")\n",
    "\n",
    "\n",
    "  return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 835
    },
    "id": "sbj0GWnPysEc",
    "outputId": "d85f37dd-bb2e-4ea3-86e1-7ac722e64309"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_machine_from_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-731070ac3fbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#COMPROBAR RESULTADOS ANTES DE UTILIZAR POLARIS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_machine_from_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlista_archivos_marsexpress\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'read_machine_from_csv' is not defined"
     ]
    }
   ],
   "source": [
    "#COMPROBAR RESULTADOS ANTES DE UTILIZAR POLARIS\n",
    "df = read_from_csv_mars_express(lista_archivos_marsexpress)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WmpHtEkGWJuH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzPPwEj0jM29"
   },
   "outputs": [],
   "source": [
    "#GUARDAR CSV DEL LIGHTSAIL2 PARA PROCESAMIENTO DE POLARIS\n",
    "df_csv = df.to_csv('lightsail_preprossed.csv', sep=',', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "benZjPIheBK1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-5096f6b45068>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#GUARDAR CSV DEL MARS EXPRESS PARA PROCESAMIENTO DE POLARIS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf_csv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'marsexpress_dataset.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#GUARDAR CSV DEL MARS EXPRESS PARA PROCESAMIENTO DE POLARIS\n",
    "df_csv = df.to_csv('marsexpress_preprossed.csv', sep=',', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoDmNLQib2tg"
   },
   "outputs": [],
   "source": [
    "#ANALISYS.PY DE POLARIS, PARA PODER PERSONALIZARLO SEGÚN EL DATASET CONVENGA\n",
    "\n",
    "\"\"\"\n",
    "Module to launch different data analysis.\n",
    "\"\"\"\n",
    "import logging\n",
    "\n",
    "from fets.math import TSIntegrale\n",
    "from mlflow import set_experiment\n",
    "\n",
    "from polaris.data.graph import PolarisGraph\n",
    "from polaris.data.readers import read_polaris_data\n",
    "from polaris.dataset.metadata import PolarisMetadata\n",
    "from polaris.learn.feature.extraction import create_list_of_transformers, \\\n",
    "    extract_best_features\n",
    "from polaris.learn.predictor.cross_correlation import XCorr\n",
    "from polaris.learn.predictor.cross_correlation_configurator import \\\n",
    "    CrossCorrelationConfigurator\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class NoFramesInInputFile(Exception):\n",
    "    \"\"\"Raised when frames dataframe is empty\"\"\"\n",
    "\n",
    "\n",
    "def feature_extraction(input_file, param_col):\n",
    "    \"\"\"\n",
    "    Start feature extraction using the given settings.\n",
    "\n",
    "        :param input_file: Path of a CSV file that will be\n",
    "            converted to a dataframe\n",
    "        :type input_file: str\n",
    "        :param param_col: Target column name\n",
    "        :type param_col: str\n",
    "    \"\"\"\n",
    "    # Create a small list of two transformers which will generate two\n",
    "    # different pipelines\n",
    "    transformers = create_list_of_transformers([\"5min\", \"15min\"], TSIntegrale)\n",
    "\n",
    "    # Extract the best features of the two pipelines\n",
    "    out = extract_best_features(input_file,\n",
    "                                transformers,\n",
    "                                target_column=param_col,\n",
    "                                time_unit=\"ms\")\n",
    "\n",
    "    # out[0] is the FeatureImportanceOptimization object\n",
    "    # from polaris.learn.feature.selection\n",
    "    # pylint: disable=E1101\n",
    "    print(out[0].best_features)\n",
    "\n",
    "\n",
    "# pylint: disable-msg=too-many-arguments\n",
    "def cross_correlate(input_file,\n",
    "                    output_graph_file=None,\n",
    "                    xcorr_configuration_file=None,\n",
    "                    graph_link_threshold=0.1,\n",
    "                    use_gridsearch=False,\n",
    "                    csv_sep=',',\n",
    "                    force_cpu=False):\n",
    "    \"\"\"\n",
    "    Catch linear and non-linear correlations between all columns of the\n",
    "    input data.\n",
    "\n",
    "        :param input_file: CSV or JSON file path that will be\n",
    "            converted to a dataframe\n",
    "        :type input_file: str\n",
    "        :param output_graph_file: Output file path for the generated graph.\n",
    "            It will overwrite if the file already exists. Defaults to None,\n",
    "            which is'/tmp/polaris_graph.json'\n",
    "        :type output_graph_file: str, optional\n",
    "        :param xcorr_configuration_file: XCorr configuration file path,\n",
    "            defaults to None. Refer to CrossCorrelationConfigurator for\n",
    "            the default parameters\n",
    "        :type xcorr_configuration_file: str, optional\n",
    "        :param graph_link_threshold: Minimum link value to be considered\n",
    "            as a link between two nodes\n",
    "        :type graph_link_threshold: float, optional\n",
    "        :param use_gridsearch: Use grid search for the cross correlation.\n",
    "            If this is set to False, then it will just use regression.\n",
    "            Defaults to False\n",
    "        :type use_gridsearch: bool, optional\n",
    "        :param csv_sep: The character that separates the columns inside of\n",
    "            the CSV file, defaults to ','\n",
    "        :type csv_sep: str, optional\n",
    "        :param force_cpu: Force CPU for cross corelation, defaults to False\n",
    "        :type force_cpu: bool, optional\n",
    "        :raises NoFramesInInputFile: If there are no frames in the converted\n",
    "            dataframe\n",
    "    \"\"\"\n",
    "    # Reading input file - index is considered on first column\n",
    "    metadata, dataframe = read_polaris_data(input_file, csv_sep)\n",
    "\n",
    "    if dataframe.empty:\n",
    "        LOGGER.error(\"Empty list of frames -- nothing to learn from!\")\n",
    "        raise NoFramesInInputFile\n",
    "\n",
    "    input_data = normalize_dataframe(dataframe)\n",
    "    source = metadata['satellite_name']\n",
    "\n",
    "    set_experiment(experiment_name=source)\n",
    "\n",
    "    xcorr_configurator = CrossCorrelationConfigurator(\n",
    "        xcorr_configuration_file=xcorr_configuration_file,\n",
    "        use_gridsearch=use_gridsearch,\n",
    "        force_cpu=force_cpu)\n",
    "\n",
    "    # Creating and fitting cross-correlator\n",
    "    xcorr = XCorr(metadata, xcorr_configurator.get_configuration())\n",
    "    xcorr.fit(input_data)\n",
    "\n",
    "    if output_graph_file is None:\n",
    "        output_graph_file = \"/tmp/polaris_graph.json\"\n",
    "\n",
    "    metadata = PolarisMetadata({\"satellite_name\": source})\n",
    "    graph = PolarisGraph(metadata=metadata)\n",
    "    graph.from_heatmap(xcorr.importances_map, graph_link_threshold)\n",
    "    with open(output_graph_file, 'w') as graph_file:\n",
    "        graph_file.write(graph.to_json())\n",
    "\n",
    "\n",
    "def normalize_dataframe(dataframe):\n",
    "    \"\"\"\n",
    "        Apply dataframe modification so it's compatible\n",
    "        with the learn module. The time column is first\n",
    "        set as the index of the dataframe. Then, we drop\n",
    "        the time column.\n",
    "\n",
    "        :param dataframe: The pandas dataframe to normalize\n",
    "        :type dataframe: pd.DataFrame\n",
    "        :return: Pandas dataframe normalized\n",
    "        :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    dataframe.index = dataframe.date\n",
    "    dataframe.drop(['date'], axis=1, inplace=True)\n",
    "\n",
    "    return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-SLFiMJXiTeY"
   },
   "outputs": [],
   "source": [
    "#GENERA EL GRAFO DE LIGHTSAIL2\n",
    "cross_correlate(\"lightsail_preprossed.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "Z0vAEqPbePhd",
    "outputId": "e727f3c9-9867-4397-e0fb-df8332c47a33"
   },
   "outputs": [],
   "source": [
    "#GENERA EL GRAFO DE MARS EXPRESS\n",
    "cross_correlate(\"marsexpress_preprossed.csv\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TFM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
