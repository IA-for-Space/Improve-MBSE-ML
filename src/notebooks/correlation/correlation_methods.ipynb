{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rICw0MTsMtam",
    "outputId": "4acf6428-30d0-4699-8dbe-e002169aa480"
   },
   "outputs": [],
   "source": [
    "#!pip install polaris-ml\n",
    "#!pip install fets\n",
    "#!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RlwcETrdgSBZ",
    "outputId": "80d937e4-3ea8-4554-9ccd-bb2dafc31ca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"Ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cross Correlation module\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import enlighten\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from polaris.feature.cleaner import Cleaner\n",
    "# Used for tracking ML process results\n",
    "from mlflow import log_metric, log_param, log_params, start_run\n",
    "# Used for the pipeline interface of scikit learn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "\n",
    "# eXtreme Gradient Boost algorithm\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#RandomForest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Extratrees regressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "#AdaBoostRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "#GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#HistGradientBoostingRegressor\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "#VotingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Remove this line when feature engineering is in place\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "\n",
    "class XCorr(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Cross Correlation predictor class\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_metadata, cross_correlation_params, regressor):\n",
    "        \"\"\" Initialize an XCorr object\n",
    "\n",
    "            :param dataset_metadata: The metadata of the dataset\n",
    "            :type dataset_metadata: PolarisMetadata\n",
    "            :param cross_correlation_params: XCorr parameters\n",
    "            :type cross_correlation_params: CrossCorrelationParameters\n",
    "        \"\"\"\n",
    "        self._regressor = regressor\n",
    "        self.models = None\n",
    "        self._importances_map = None\n",
    "        self._feature_cleaner = Cleaner(\n",
    "            dataset_metadata, cross_correlation_params.dataset_cleaning_params)\n",
    "        self.xcorr_params = {\n",
    "            \"random_state\": cross_correlation_params.random_state,\n",
    "            \"test_size\": cross_correlation_params.test_size,\n",
    "            \"gridsearch_scoring\": cross_correlation_params.gridsearch_scoring,\n",
    "            \"gridsearch_n_splits\":\n",
    "            cross_correlation_params.gridsearch_n_splits,\n",
    "        }\n",
    "        # If we're importing from CSV, the dataset_metadata may not\n",
    "        # have the feature_columns key.\n",
    "        try:\n",
    "            self.xcorr_params['feature_columns'] = dataset_metadata[\n",
    "                'analysis']['feature_columns']\n",
    "        except KeyError:\n",
    "            LOGGER.info(\n",
    "                \"No feature_columns entry in metatdata, setting to empty array\"\n",
    "            )\n",
    "            self.xcorr_params['feature_columns'] = []\n",
    "\n",
    "        if cross_correlation_params.use_gridsearch:\n",
    "            self.method = self.gridsearch\n",
    "            self.mlf_logging = self.gridsearch_mlf_logging\n",
    "        else:\n",
    "            self.method = self.regression\n",
    "            self.mlf_logging = self.regression_mlf_logging\n",
    "\n",
    "        self.model_params = {\n",
    "            \"current\": cross_correlation_params.model_params,\n",
    "            \"cpu\": cross_correlation_params.model_cpu_params\n",
    "        }\n",
    "        \n",
    "    @property\n",
    "    def regressor(self):\n",
    "        return self._regressor\n",
    "    \n",
    "    @regressor.setter\n",
    "    def regressor(self, regressor):\n",
    "        self._regressor = regressor\n",
    "\n",
    "    @property\n",
    "    def importances_map(self):\n",
    "        \"\"\"\n",
    "        Return the importances_map value as Pandas Dataframe.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return self._importances_map\n",
    "\n",
    "    @importances_map.setter\n",
    "    def importances_map(self, importances_map):\n",
    "        self._importances_map = importances_map\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\" Train on a dataframe\n",
    "\n",
    "            The input dataframe will be split column by column\n",
    "            considering each one as a prediction target.\n",
    "\n",
    "            :param X: Input dataframe\n",
    "            :type X: pd.DataFrame\n",
    "            :raises Exception: If encountered any unhandled error\n",
    "                during model fitting\n",
    "        \"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input data should be a DataFrame\")\n",
    "\n",
    "        if self.models is None:\n",
    "            self.models = []\n",
    "\n",
    "        manager = enlighten.get_manager()\n",
    "\n",
    "        LOGGER.info(\"Clearing Data. Removing unnecessary columns\")\n",
    "        X = self._feature_cleaner.drop_constant_values(X)\n",
    "        X = self._feature_cleaner.drop_non_numeric_values(X)\n",
    "        X = self._feature_cleaner.handle_missing_values(X)\n",
    "\n",
    "        self.reset_importance_map(X.columns)\n",
    "\n",
    "        parameters = self.__build_parameters(X)\n",
    "\n",
    "        pbar = manager.counter(total=len(parameters),\n",
    "                               desc=\"Columns\",\n",
    "                               unit=\"columns\")\n",
    "\n",
    "        with start_run(run_name='cross_correlate', nested=True):\n",
    "            self.mlf_logging()\n",
    "            for column in parameters:\n",
    "                LOGGER.info(column)\n",
    "                try:\n",
    "                    self.models.append(\n",
    "                        self.method(X.drop([column], axis=1), X[column],\n",
    "                                    self.model_params['current']))\n",
    "                except Exception as err:  # pylint: disable-msg=broad-except\n",
    "                    if self.model_params['current'].get(\n",
    "                            \"predictor\") == \"gpu_predictor\":\n",
    "                        LOGGER.info(\" \".join([\n",
    "                            \"Encountered error using GPU.\",\n",
    "                            \"Trying with CPU parameters now!\"\n",
    "                        ]))\n",
    "                        self.model_params['current'] = self.model_params['cpu']\n",
    "                    else:\n",
    "                        raise err\n",
    "                pbar.update()\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\" Unused method in this predictor \"\"\"\n",
    "        return self\n",
    "    \n",
    "        \n",
    "    def regression(self, df_in, target_series, model_params):\n",
    "        \"\"\" Fit a model to predict target_series with df_in features/columns\n",
    "            and retain the features importances in the dependency matrix.\n",
    "\n",
    "            :param df_in: Input dataframe representing the context, predictors\n",
    "            :type df_in: pd.DataFrame\n",
    "            :param target_series: pandas series of the target variable. Share\n",
    "                the same indexes as the df_in dataframe\n",
    "            :type target_series: pd.Series\n",
    "            :param model_params: Parameters for the XGB model\n",
    "            :type model_params: dict\n",
    "            :return: A fitted XGBRegressor\n",
    "            :rtype: XGBRegressor\n",
    "        \"\"\"\n",
    "        # Split df_in and target to train and test dataset\n",
    "        df_in_train, df_in_test, target_train, target_test = train_test_split(\n",
    "            df_in,\n",
    "            target_series,\n",
    "            test_size=0.2,\n",
    "            random_state=self.xcorr_params['random_state'])\n",
    "\n",
    "\n",
    "        regressors_dict = {\"XGB\": XGBRegressor(**model_params),\n",
    "                           \"RandomForest\": RandomForestRegressor(),\n",
    "                           \"AdaBoost\": AdaBoostRegressor(),\n",
    "                           \"ExtraTrees\": ExtraTreesRegressor(),\n",
    "                           \"GradientBoosting\": GradientBoostingRegressor(),\n",
    "                           \"Voting\": \"VotingRegressor()\"}\n",
    "\n",
    "        \"\"\" if self._regressor == \"XGboosting\":\n",
    "            # Create and train a XGBoost regressor\n",
    "            regr_m = XGBRegressor(**model_params)\n",
    "            \n",
    "        elif self._regressor == \"RandomForest\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            regr_m = RandomForestRegressor()\n",
    "        \n",
    "        elif self._regressor == \"AdaBoost\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            regr_m = AdaBoostRegressor()\n",
    "        \n",
    "        elif self._regressor == \"ExtraTrees\":\n",
    "             # Create and train a Sci-kit regressor\n",
    "            regr_m = ExtraTreesRegressor()\n",
    "        \n",
    "        elif self._regressor == \"GradientBoosting\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            regr_m = GradientBoostingRegressor()\n",
    "        \n",
    "        elif self._regressor == \"HistGradientBoosting\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            #regr_m = HistGradientBoostingRegressor()\n",
    "            pass\n",
    "            \n",
    "        elif self._regressor == \"Voting\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            regr_m = VotingRegressor()\n",
    "        \n",
    "        \"\"\"\n",
    "        regr_m = regressors_dict[self._regressor]\n",
    "        regr_m.fit(df_in_train, target_train)\n",
    "\n",
    "        # Make predictions\n",
    "        target_series_predict = regr_m.predict(df_in_test)\n",
    "\n",
    "        try:\n",
    "            rmse = np.sqrt(\n",
    "                mean_squared_error(target_test, target_series_predict))\n",
    "            log_metric(target_series.name, rmse)\n",
    "            LOGGER.info('Making predictions for : %s', target_series.name)\n",
    "            LOGGER.info('Root Mean Square Error : %s', str(rmse))\n",
    "        except Exception:  # pylint: disable-msg=broad-except\n",
    "            # Because of large (close to infinite values) or nans\n",
    "            LOGGER.error('Cannot find RMS Error for %s', target_series.name)\n",
    "            LOGGER.debug('Expected %s, Predicted %s', str(target_test),\n",
    "                         str(target_series_predict))\n",
    "\n",
    "        # indices = np.argsort(regr_m.feature_importances_)[::-1]\n",
    "        # After the model is trained\n",
    "        new_row = {}\n",
    "        for column, feat_imp in zip(df_in.columns,\n",
    "                                    regr_m.feature_importances_):\n",
    "            new_row[column] = [feat_imp]\n",
    "\n",
    "        # Current target is not in df_in, so manually adding it\n",
    "        new_row[target_series.name] = [0.0]\n",
    "\n",
    "        # Sorting new_row to avoid concatenation warnings\n",
    "        new_row = dict(sorted(new_row.items()))\n",
    "\n",
    "        # Concatenating new information about feature importances\n",
    "        if self._importances_map is not None:\n",
    "            self._importances_map = pd.concat([\n",
    "                self._importances_map,\n",
    "                pd.DataFrame(index=[target_series.name], data=new_row)\n",
    "            ])\n",
    "        return regr_m\n",
    "\n",
    "    def gridsearch(self, df_in, target_series, params):\n",
    "        \"\"\" Apply grid search to fine-tune XGBoost hyperparameters\n",
    "            and then call the regression method with the best grid\n",
    "            search parameters.\n",
    "\n",
    "            :param df_in: Input dataframe representing the context, predictors\n",
    "            :type df_in: pd.DataFrame\n",
    "            :param target_series: Pandas series of the target variable. Share\n",
    "                the same indexes as the df_in dataframe\n",
    "            :type target_series: pd.Series\n",
    "            :param params: The hyperparameters to use on the gridsearch\n",
    "                method\n",
    "            :type params: dict\n",
    "            :raises TypeError: If df_in is not Pandas DataFrame\n",
    "            :return: A fitted XGBRegressor\n",
    "            :rtype: XGBRegressor\n",
    "        \"\"\"\n",
    "        if not isinstance(df_in, pd.DataFrame):\n",
    "            LOGGER.error(\"Expected %s got %s for df_in in gridsearch\",\n",
    "                         pd.DataFrame, type(df_in))\n",
    "            raise TypeError\n",
    "\n",
    "        random_state = self.xcorr_params['random_state']\n",
    "        kfolds = KFold(n_splits=self.xcorr_params['gridsearch_n_splits'],\n",
    "                       shuffle=True,\n",
    "                       random_state=random_state)\n",
    "        regr_m = XGBRegressor(random_state=random_state,\n",
    "                              predictor=\"cpu_predictor\",\n",
    "                              tree_method=\"auto\",\n",
    "                              n_jobs=-1)\n",
    "\n",
    "        gs_regr = GridSearchCV(regr_m,\n",
    "                               param_grid=params,\n",
    "                               cv=kfolds,\n",
    "                               scoring=self.xcorr_params['gridsearch_scoring'],\n",
    "                               n_jobs=-1,\n",
    "                               verbose=1)\n",
    "        gs_regr.fit(df_in, target_series)\n",
    "\n",
    "        log_param(target_series.name + ' best estimator', gs_regr.best_params_)\n",
    "        LOGGER.info(\"%s best estimator : %s\", target_series.name,\n",
    "                    str(gs_regr.best_estimator_))\n",
    "        return self.regression(df_in, target_series, gs_regr.best_params_)\n",
    "\n",
    "    def reset_importance_map(self, columns):\n",
    "        \"\"\"\n",
    "        Creating an empty importance map\n",
    "\n",
    "        :param columns: List of column names for the importance map\n",
    "        :rtype columns: pd.Index or array-like\n",
    "        \"\"\"\n",
    "        if self._importances_map is None:\n",
    "            self._importances_map = pd.DataFrame(data={}, columns=columns)\n",
    "\n",
    "    def common_mlf_logging(self):\n",
    "        \"\"\" Log the parameters used for gridsearch and regression\n",
    "            in mlflow\n",
    "        \"\"\"\n",
    "        log_param('Test size', self.xcorr_params['test_size'])\n",
    "        log_param('Model', 'XGBRegressor')\n",
    "\n",
    "    def gridsearch_mlf_logging(self):\n",
    "        \"\"\" Log the parameters used for gridsearch\n",
    "            in mlflow\n",
    "        \"\"\"\n",
    "        log_param('Gridsearch scoring',\n",
    "                  self.xcorr_params['gridsearch_scoring'])\n",
    "        log_param('Gridsearch parameters', self.model_params)\n",
    "        self.common_mlf_logging()\n",
    "\n",
    "    def regression_mlf_logging(self):\n",
    "        \"\"\" Log the parameters used for regression\n",
    "            in mlflow.\n",
    "        \"\"\"\n",
    "        self.common_mlf_logging()\n",
    "        log_params(self.model_params)\n",
    "\n",
    "    def __build_parameters(self, X):\n",
    "        \"\"\" Remove features only from\n",
    "            being predicted.\n",
    "\n",
    "            :param X: The dataset\n",
    "            :type X: pd.DataFrame\n",
    "            :return: List of remaining features that are not removed\n",
    "            :rtype: list\n",
    "        \"\"\"\n",
    "        if self.xcorr_params['feature_columns'] is None:\n",
    "            return list(X.columns)\n",
    "\n",
    "        LOGGER.info('Removing features from the parameters : %s',\n",
    "                    self.xcorr_params['feature_columns'])\n",
    "        feature_to_remove = set(self.xcorr_params['feature_columns'])\n",
    "\n",
    "        return [x for x in list(X.columns) if x not in feature_to_remove]\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Module to launch different data analysis.\n",
    "\"\"\"\n",
    "import logging\n",
    "\n",
    "from fets.math import TSIntegrale\n",
    "from mlflow import set_experiment\n",
    "import polaris\n",
    "from polaris.data.graph import PolarisGraph\n",
    "from polaris.data.readers import read_polaris_data\n",
    "from polaris.dataset.metadata import PolarisMetadata\n",
    "from polaris.learn.feature.extraction import create_list_of_transformers, \\\n",
    "    extract_best_features\n",
    "from polaris.learn.predictor.cross_correlation_configurator import \\\n",
    "    CrossCorrelationConfigurator\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class NoFramesInInputFile(Exception):\n",
    "    \"\"\"Raised when frames dataframe is empty\"\"\"\n",
    "\n",
    "\n",
    "def feature_extraction(input_file, param_col):\n",
    "    \"\"\"\n",
    "    Start feature extraction using the given settings.\n",
    "\n",
    "        :param input_file: Path of a CSV file that will be\n",
    "            converted to a dataframe\n",
    "        :type input_file: str\n",
    "        :param param_col: Target column name\n",
    "        :type param_col: str\n",
    "    \"\"\"\n",
    "    # Create a small list of two transformers which will generate two\n",
    "    # different pipelines\n",
    "    transformers = create_list_of_transformers([\"5min\", \"15min\"], TSIntegrale)\n",
    "\n",
    "    # Extract the best features of the two pipelines\n",
    "    out = extract_best_features(input_file,\n",
    "                                transformers,\n",
    "                                target_column=param_col,\n",
    "                                time_unit=\"ms\")\n",
    "\n",
    "    # out[0] is the FeatureImportanceOptimization object\n",
    "    # from polaris.learn.feature.selection\n",
    "    # pylint: disable=E1101\n",
    "    print(out[0].best_features)\n",
    "\n",
    "\n",
    "# pylint: disable-msg=too-many-arguments\n",
    "def cross_correlate(input_file,\n",
    "                    regressor=\"XGB\",\n",
    "                    output_graph_file=None,\n",
    "                    xcorr_configuration_file=None,\n",
    "                    graph_link_threshold=0.1,\n",
    "                    use_gridsearch=False,\n",
    "                    csv_sep=',',\n",
    "                    force_cpu=False):\n",
    "    \"\"\"\n",
    "    Catch linear and non-linear correlations between all columns of the\n",
    "    input data.\n",
    "\n",
    "        :param input_file: CSV or JSON file path that will be\n",
    "            converted to a dataframe\n",
    "        :type input_file: str\n",
    "        :param output_graph_file: Output file path for the generated graph.\n",
    "            It will overwrite if the file already exists. Defaults to None,\n",
    "            which is'/tmp/polaris_graph.json'\n",
    "        :type output_graph_file: str, optional\n",
    "        :param xcorr_configuration_file: XCorr configuration file path,\n",
    "            defaults to None. Refer to CrossCorrelationConfigurator for\n",
    "            the default parameters\n",
    "        :type xcorr_configuration_file: str, optional\n",
    "        :param graph_link_threshold: Minimum link value to be considered\n",
    "            as a link between two nodes\n",
    "        :type graph_link_threshold: float, optional\n",
    "        :param use_gridsearch: Use grid search for the cross correlation.\n",
    "            If this is set to False, then it will just use regression.\n",
    "            Defaults to False\n",
    "        :type use_gridsearch: bool, optional\n",
    "        :param csv_sep: The character that separates the columns inside of\n",
    "            the CSV file, defaults to ','\n",
    "        :type csv_sep: str, optional\n",
    "        :param force_cpu: Force CPU for cross corelation, defaults to False\n",
    "        :type force_cpu: bool, optional\n",
    "        :raises NoFramesInInputFile: If there are no frames in the converted\n",
    "            dataframe\n",
    "    \"\"\"\n",
    "    # Reading input file - index is considered on first column\n",
    "    metadata, dataframe = read_polaris_data(input_file, csv_sep)\n",
    "\n",
    "    if dataframe.empty:\n",
    "        LOGGER.error(\"Empty list of frames -- nothing to learn from!\")\n",
    "        raise NoFramesInInputFile\n",
    "\n",
    "    input_data = normalize_dataframe(dataframe)\n",
    "    source = metadata['satellite_name']\n",
    "\n",
    "    set_experiment(experiment_name=source)\n",
    "\n",
    "    xcorr_configurator = CrossCorrelationConfigurator(\n",
    "        xcorr_configuration_file=xcorr_configuration_file,\n",
    "        use_gridsearch=use_gridsearch,\n",
    "        force_cpu=force_cpu)\n",
    "\n",
    "    # Creating and fitting cross-correlator\n",
    "    xcorr = XCorr(metadata, xcorr_configurator.get_configuration(), regressor)\n",
    "    xcorr.fit(input_data)\n",
    "\n",
    "    if output_graph_file is None:\n",
    "        output_graph_file = \"/tmp/polaris_graph.json\"\n",
    "\n",
    "    metadata = PolarisMetadata({\"satellite_name\": source})\n",
    "    graph = PolarisGraph(metadata=metadata)\n",
    "    graph.from_heatmap(xcorr.importances_map, graph_link_threshold)\n",
    "    with open(output_graph_file, 'w') as graph_file:\n",
    "        graph_file.write(graph.to_json())\n",
    "\n",
    "\n",
    "def normalize_dataframe(dataframe):\n",
    "    \"\"\"\n",
    "        Apply dataframe modification so it's compatible\n",
    "        with the learn module. The time column is first\n",
    "        set as the index of the dataframe. Then, we drop\n",
    "        the time column.\n",
    "\n",
    "        :param dataframe: The pandas dataframe to normalize\n",
    "        :type dataframe: pd.DataFrame\n",
    "        :return: Pandas dataframe normalized\n",
    "        :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    dataframe.index = dataframe.time\n",
    "    dataframe.drop(['time'], axis=1, inplace=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-6-9e4c8a80f6fa>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-9e4c8a80f6fa>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    cross_correlate(\"lightsail_dataset.csv\", RandomForestB\")\u001b[0m\n\u001b[1;37m                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "#GENERA EL GRAFO DE LIGHTSAIL2\n",
    "cross_correlate(\"lightsail_dataset.csv\", RandomForestB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest\")\n",
    "cross_correlate(\"lightsail_dataset.csv\", \"RandomForest\")\n",
    "print(\"AdaBoost\")\n",
    "cross_correlate(\"lightsail_dataset.csv\", \"AdaBoost\")\n",
    "print(\"Extra trees\")\n",
    "cross_correlate(\"lightsail_dataset.csv\", \"ExtraTrees\")\n",
    "print(\"Gradient boosting\")\n",
    "cross_correlate(\"lightsail_dataset.csv\", \"GradientBoosting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(239, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from graph_to_object import getObjectFromGraph\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "archivo1 = open(\"polaris_graph_adaboost.json\", \"r\")\n",
    "graph_1 = json.loads(all_file)\n",
    "list_graph_nodes = [x[\"id\"] for x in graph_1[\"graph\"][\"nodes\"]]\n",
    "df_graph_links = pd.DataFrame([x for x in graph_1[\"graph\"][\"links\"]])\n",
    "display(len(list_graph_nodes))\n",
    "display(df_graph_links.shape)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TFM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "de072921dc87486613898b1ef56959cc98c50a630fb49de1898fb32d92a683cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
