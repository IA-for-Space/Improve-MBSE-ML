{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rICw0MTsMtam",
    "outputId": "4acf6428-30d0-4699-8dbe-e002169aa480"
   },
   "outputs": [],
   "source": [
    "#!pip install polaris-ml\n",
    "#!pip install fets\n",
    "#!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RlwcETrdgSBZ",
    "outputId": "80d937e4-3ea8-4554-9ccd-bb2dafc31ca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"Ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Cross Correlation module\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import enlighten\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from polaris.feature.cleaner import Cleaner\n",
    "# Used for tracking ML process results\n",
    "from mlflow import log_metric, log_param, log_params, start_run\n",
    "# Used for the pipeline interface of scikit learn\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "\n",
    "# eXtreme Gradient Boost algorithm\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "#RandomForest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#Extratrees regressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "#AdaBoostRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "#GradientBoostingRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "#HistGradientBoostingRegressor\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "#VotingRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Remove this line when feature engineering is in place\n",
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "\n",
    "\n",
    "class XCorr(BaseEstimator, TransformerMixin):\n",
    "    \"\"\" Cross Correlation predictor class\n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_metadata, cross_correlation_params, regressor):\n",
    "        \"\"\" Initialize an XCorr object\n",
    "\n",
    "            :param dataset_metadata: The metadata of the dataset\n",
    "            :type dataset_metadata: PolarisMetadata\n",
    "            :param cross_correlation_params: XCorr parameters\n",
    "            :type cross_correlation_params: CrossCorrelationParameters\n",
    "        \"\"\"\n",
    "        self._regressor = regressor\n",
    "        self.models = None\n",
    "        self._importances_map = None\n",
    "        self._feature_cleaner = Cleaner(\n",
    "            dataset_metadata, cross_correlation_params.dataset_cleaning_params)\n",
    "        self.xcorr_params = {\n",
    "            \"random_state\": cross_correlation_params.random_state,\n",
    "            \"test_size\": cross_correlation_params.test_size,\n",
    "            \"gridsearch_scoring\": cross_correlation_params.gridsearch_scoring,\n",
    "            \"gridsearch_n_splits\":\n",
    "            cross_correlation_params.gridsearch_n_splits,\n",
    "        }\n",
    "        # If we're importing from CSV, the dataset_metadata may not\n",
    "        # have the feature_columns key.\n",
    "        try:\n",
    "            self.xcorr_params['feature_columns'] = dataset_metadata[\n",
    "                'analysis']['feature_columns']\n",
    "        except KeyError:\n",
    "            LOGGER.info(\n",
    "                \"No feature_columns entry in metatdata, setting to empty array\"\n",
    "            )\n",
    "            self.xcorr_params['feature_columns'] = []\n",
    "\n",
    "        if cross_correlation_params.use_gridsearch:\n",
    "            self.method = self.gridsearch\n",
    "            self.mlf_logging = self.gridsearch_mlf_logging\n",
    "        else:\n",
    "            self.method = self.regression\n",
    "            self.mlf_logging = self.regression_mlf_logging\n",
    "\n",
    "        self.model_params = {\n",
    "            \"current\": cross_correlation_params.model_params,\n",
    "            \"cpu\": cross_correlation_params.model_cpu_params\n",
    "        }\n",
    "        \n",
    "    @property\n",
    "    def regressor(self):\n",
    "        return self._regressor\n",
    "    \n",
    "    @regressor.setter\n",
    "    def regressor(self, regressor):\n",
    "        self._regressor = regressor\n",
    "\n",
    "    @property\n",
    "    def importances_map(self):\n",
    "        \"\"\"\n",
    "        Return the importances_map value as Pandas Dataframe.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        return self._importances_map\n",
    "\n",
    "    @importances_map.setter\n",
    "    def importances_map(self, importances_map):\n",
    "        self._importances_map = importances_map\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\" Train on a dataframe\n",
    "\n",
    "            The input dataframe will be split column by column\n",
    "            considering each one as a prediction target.\n",
    "\n",
    "            :param X: Input dataframe\n",
    "            :type X: pd.DataFrame\n",
    "            :raises Exception: If encountered any unhandled error\n",
    "                during model fitting\n",
    "        \"\"\"\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"Input data should be a DataFrame\")\n",
    "\n",
    "        if self.models is None:\n",
    "            self.models = []\n",
    "\n",
    "        manager = enlighten.get_manager()\n",
    "\n",
    "        LOGGER.info(\"Clearing Data. Removing unnecessary columns\")\n",
    "        X = self._feature_cleaner.drop_constant_values(X)\n",
    "        X = self._feature_cleaner.drop_non_numeric_values(X)\n",
    "        X = self._feature_cleaner.handle_missing_values(X)\n",
    "\n",
    "        self.reset_importance_map(X.columns)\n",
    "\n",
    "        parameters = self.__build_parameters(X)\n",
    "\n",
    "        pbar = manager.counter(total=len(parameters),\n",
    "                               desc=\"Columns\",\n",
    "                               unit=\"columns\")\n",
    "\n",
    "        with start_run(run_name='cross_correlate', nested=True):\n",
    "            self.mlf_logging()\n",
    "            for column in parameters:\n",
    "                LOGGER.info(column)\n",
    "                try:\n",
    "                    self.models.append(\n",
    "                        self.method(X.drop([column], axis=1), X[column],\n",
    "                                    self.model_params['current']))\n",
    "                except Exception as err:  # pylint: disable-msg=broad-except\n",
    "                    if self.model_params['current'].get(\n",
    "                            \"predictor\") == \"gpu_predictor\":\n",
    "                        LOGGER.info(\" \".join([\n",
    "                            \"Encountered error using GPU.\",\n",
    "                            \"Trying with CPU parameters now!\"\n",
    "                        ]))\n",
    "                        self.model_params['current'] = self.model_params['cpu']\n",
    "                    else:\n",
    "                        raise err\n",
    "                pbar.update()\n",
    "\n",
    "    def transform(self):\n",
    "        \"\"\" Unused method in this predictor \"\"\"\n",
    "        return self\n",
    "    \n",
    "        \n",
    "    def regression(self, df_in, target_series, model_params):\n",
    "        \"\"\" Fit a model to predict target_series with df_in features/columns\n",
    "            and retain the features importances in the dependency matrix.\n",
    "\n",
    "            :param df_in: Input dataframe representing the context, predictors\n",
    "            :type df_in: pd.DataFrame\n",
    "            :param target_series: pandas series of the target variable. Share\n",
    "                the same indexes as the df_in dataframe\n",
    "            :type target_series: pd.Series\n",
    "            :param model_params: Parameters for the XGB model\n",
    "            :type model_params: dict\n",
    "            :return: A fitted XGBRegressor\n",
    "            :rtype: XGBRegressor\n",
    "        \"\"\"\n",
    "        # Split df_in and target to train and test dataset\n",
    "        df_in_train, df_in_test, target_train, target_test = train_test_split(\n",
    "            df_in,\n",
    "            target_series,\n",
    "            test_size=0.2,\n",
    "            random_state=self.xcorr_params['random_state'])\n",
    "\n",
    "\n",
    "        regressors_dict = {\"XGB\": XGBRegressor(**model_params),\n",
    "                           \"RandomForest\": RandomForestRegressor(),\n",
    "                           \"AdaBoost\": AdaBoostRegressor(),\n",
    "                           \"ExtraTrees\": ExtraTreesRegressor(),\n",
    "                           \"GradientBoosting\": GradientBoostingRegressor(),\n",
    "                           \"Voting\": \"VotingRegressor()\"}\n",
    "\n",
    "        \"\"\" if self._regressor == \"XGboosting\":\n",
    "            # Create and train a XGBoost regressor\n",
    "            regr_m = XGBRegressor(**model_params)\n",
    "            \n",
    "        elif self._regressor == \"RandomForest\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            regr_m = RandomForestRegressor()\n",
    "        \n",
    "        elif self._regressor == \"AdaBoost\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            regr_m = AdaBoostRegressor()\n",
    "        \n",
    "        elif self._regressor == \"ExtraTrees\":\n",
    "             # Create and train a Sci-kit regressor\n",
    "            regr_m = ExtraTreesRegressor()\n",
    "        \n",
    "        elif self._regressor == \"GradientBoosting\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            regr_m = GradientBoostingRegressor()\n",
    "        \n",
    "        elif self._regressor == \"HistGradientBoosting\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            #regr_m = HistGradientBoostingRegressor()\n",
    "            pass\n",
    "            \n",
    "        elif self._regressor == \"Voting\":\n",
    "            # Create and train a Sci-kit regressor\n",
    "            regr_m = VotingRegressor()\n",
    "        \n",
    "        \"\"\"\n",
    "        regr_m = regressors_dict[self._regressor]\n",
    "        regr_m.fit(df_in_train, target_train)\n",
    "\n",
    "        # Make predictions\n",
    "        target_series_predict = regr_m.predict(df_in_test)\n",
    "\n",
    "        try:\n",
    "            rmse = np.sqrt(\n",
    "                mean_squared_error(target_test, target_series_predict))\n",
    "            log_metric(target_series.name, rmse)\n",
    "            LOGGER.info('Making predictions for : %s', target_series.name)\n",
    "            LOGGER.info('Root Mean Square Error : %s', str(rmse))\n",
    "        except Exception:  # pylint: disable-msg=broad-except\n",
    "            # Because of large (close to infinite values) or nans\n",
    "            LOGGER.error('Cannot find RMS Error for %s', target_series.name)\n",
    "            LOGGER.debug('Expected %s, Predicted %s', str(target_test),\n",
    "                         str(target_series_predict))\n",
    "\n",
    "        # indices = np.argsort(regr_m.feature_importances_)[::-1]\n",
    "        # After the model is trained\n",
    "        new_row = {}\n",
    "        for column, feat_imp in zip(df_in.columns,\n",
    "                                    regr_m.feature_importances_):\n",
    "            new_row[column] = [feat_imp]\n",
    "\n",
    "        # Current target is not in df_in, so manually adding it\n",
    "        new_row[target_series.name] = [0.0]\n",
    "\n",
    "        # Sorting new_row to avoid concatenation warnings\n",
    "        new_row = dict(sorted(new_row.items()))\n",
    "\n",
    "        # Concatenating new information about feature importances\n",
    "        if self._importances_map is not None:\n",
    "            self._importances_map = pd.concat([\n",
    "                self._importances_map,\n",
    "                pd.DataFrame(index=[target_series.name], data=new_row)\n",
    "            ])\n",
    "        return regr_m\n",
    "\n",
    "    def gridsearch(self, df_in, target_series, params):\n",
    "        \"\"\" Apply grid search to fine-tune XGBoost hyperparameters\n",
    "            and then call the regression method with the best grid\n",
    "            search parameters.\n",
    "\n",
    "            :param df_in: Input dataframe representing the context, predictors\n",
    "            :type df_in: pd.DataFrame\n",
    "            :param target_series: Pandas series of the target variable. Share\n",
    "                the same indexes as the df_in dataframe\n",
    "            :type target_series: pd.Series\n",
    "            :param params: The hyperparameters to use on the gridsearch\n",
    "                method\n",
    "            :type params: dict\n",
    "            :raises TypeError: If df_in is not Pandas DataFrame\n",
    "            :return: A fitted XGBRegressor\n",
    "            :rtype: XGBRegressor\n",
    "        \"\"\"\n",
    "        if not isinstance(df_in, pd.DataFrame):\n",
    "            LOGGER.error(\"Expected %s got %s for df_in in gridsearch\",\n",
    "                         pd.DataFrame, type(df_in))\n",
    "            raise TypeError\n",
    "\n",
    "        random_state = self.xcorr_params['random_state']\n",
    "        kfolds = KFold(n_splits=self.xcorr_params['gridsearch_n_splits'],\n",
    "                       shuffle=True,\n",
    "                       random_state=random_state)\n",
    "        regr_m = XGBRegressor(random_state=random_state,\n",
    "                              predictor=\"cpu_predictor\",\n",
    "                              tree_method=\"auto\",\n",
    "                              n_jobs=-1)\n",
    "\n",
    "        gs_regr = GridSearchCV(regr_m,\n",
    "                               param_grid=params,\n",
    "                               cv=kfolds,\n",
    "                               scoring=self.xcorr_params['gridsearch_scoring'],\n",
    "                               n_jobs=-1,\n",
    "                               verbose=1)\n",
    "        gs_regr.fit(df_in, target_series)\n",
    "\n",
    "        log_param(target_series.name + ' best estimator', gs_regr.best_params_)\n",
    "        LOGGER.info(\"%s best estimator : %s\", target_series.name,\n",
    "                    str(gs_regr.best_estimator_))\n",
    "        return self.regression(df_in, target_series, gs_regr.best_params_)\n",
    "\n",
    "    def reset_importance_map(self, columns):\n",
    "        \"\"\"\n",
    "        Creating an empty importance map\n",
    "\n",
    "        :param columns: List of column names for the importance map\n",
    "        :rtype columns: pd.Index or array-like\n",
    "        \"\"\"\n",
    "        if self._importances_map is None:\n",
    "            self._importances_map = pd.DataFrame(data={}, columns=columns)\n",
    "\n",
    "    def common_mlf_logging(self):\n",
    "        \"\"\" Log the parameters used for gridsearch and regression\n",
    "            in mlflow\n",
    "        \"\"\"\n",
    "        log_param('Test size', self.xcorr_params['test_size'])\n",
    "        log_param('Model', 'XGBRegressor')\n",
    "\n",
    "    def gridsearch_mlf_logging(self):\n",
    "        \"\"\" Log the parameters used for gridsearch\n",
    "            in mlflow\n",
    "        \"\"\"\n",
    "        log_param('Gridsearch scoring',\n",
    "                  self.xcorr_params['gridsearch_scoring'])\n",
    "        log_param('Gridsearch parameters', self.model_params)\n",
    "        self.common_mlf_logging()\n",
    "\n",
    "    def regression_mlf_logging(self):\n",
    "        \"\"\" Log the parameters used for regression\n",
    "            in mlflow.\n",
    "        \"\"\"\n",
    "        self.common_mlf_logging()\n",
    "        log_params(self.model_params)\n",
    "\n",
    "    def __build_parameters(self, X):\n",
    "        \"\"\" Remove features only from\n",
    "            being predicted.\n",
    "\n",
    "            :param X: The dataset\n",
    "            :type X: pd.DataFrame\n",
    "            :return: List of remaining features that are not removed\n",
    "            :rtype: list\n",
    "        \"\"\"\n",
    "        if self.xcorr_params['feature_columns'] is None:\n",
    "            return list(X.columns)\n",
    "\n",
    "        LOGGER.info('Removing features from the parameters : %s',\n",
    "                    self.xcorr_params['feature_columns'])\n",
    "        feature_to_remove = set(self.xcorr_params['feature_columns'])\n",
    "\n",
    "        return [x for x in list(X.columns) if x not in feature_to_remove]\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Module to launch different data analysis.\n",
    "\"\"\"\n",
    "import logging\n",
    "\n",
    "from fets.math import TSIntegrale\n",
    "from mlflow import set_experiment\n",
    "import polaris\n",
    "from polaris.data.graph import PolarisGraph\n",
    "from polaris.data.readers import read_polaris_data\n",
    "from polaris.dataset.metadata import PolarisMetadata\n",
    "from polaris.learn.feature.extraction import create_list_of_transformers, \\\n",
    "    extract_best_features\n",
    "from polaris.learn.predictor.cross_correlation_configurator import \\\n",
    "    CrossCorrelationConfigurator\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class NoFramesInInputFile(Exception):\n",
    "    \"\"\"Raised when frames dataframe is empty\"\"\"\n",
    "\n",
    "\n",
    "def feature_extraction(input_file, param_col):\n",
    "    \"\"\"\n",
    "    Start feature extraction using the given settings.\n",
    "\n",
    "        :param input_file: Path of a CSV file that will be\n",
    "            converted to a dataframe\n",
    "        :type input_file: str\n",
    "        :param param_col: Target column name\n",
    "        :type param_col: str\n",
    "    \"\"\"\n",
    "    # Create a small list of two transformers which will generate two\n",
    "    # different pipelines\n",
    "    transformers = create_list_of_transformers([\"5min\", \"15min\"], TSIntegrale)\n",
    "\n",
    "    # Extract the best features of the two pipelines\n",
    "    out = extract_best_features(input_file,\n",
    "                                transformers,\n",
    "                                target_column=param_col,\n",
    "                                time_unit=\"ms\")\n",
    "\n",
    "    # out[0] is the FeatureImportanceOptimization object\n",
    "    # from polaris.learn.feature.selection\n",
    "    # pylint: disable=E1101\n",
    "    print(out[0].best_features)\n",
    "\n",
    "\n",
    "# pylint: disable-msg=too-many-arguments\n",
    "def cross_correlate(input_file,\n",
    "                    regressor=\"XGB\",\n",
    "                    output_graph_file=None,\n",
    "                    xcorr_configuration_file=None,\n",
    "                    graph_link_threshold=0.1,\n",
    "                    use_gridsearch=False,\n",
    "                    csv_sep=',',\n",
    "                    force_cpu=False):\n",
    "    \"\"\"\n",
    "    Catch linear and non-linear correlations between all columns of the\n",
    "    input data.\n",
    "\n",
    "        :param input_file: CSV or JSON file path that will be\n",
    "            converted to a dataframe\n",
    "        :type input_file: str\n",
    "        :param output_graph_file: Output file path for the generated graph.\n",
    "            It will overwrite if the file already exists. Defaults to None,\n",
    "            which is'/tmp/polaris_graph.json'\n",
    "        :type output_graph_file: str, optional\n",
    "        :param xcorr_configuration_file: XCorr configuration file path,\n",
    "            defaults to None. Refer to CrossCorrelationConfigurator for\n",
    "            the default parameters\n",
    "        :type xcorr_configuration_file: str, optional\n",
    "        :param graph_link_threshold: Minimum link value to be considered\n",
    "            as a link between two nodes\n",
    "        :type graph_link_threshold: float, optional\n",
    "        :param use_gridsearch: Use grid search for the cross correlation.\n",
    "            If this is set to False, then it will just use regression.\n",
    "            Defaults to False\n",
    "        :type use_gridsearch: bool, optional\n",
    "        :param csv_sep: The character that separates the columns inside of\n",
    "            the CSV file, defaults to ','\n",
    "        :type csv_sep: str, optional\n",
    "        :param force_cpu: Force CPU for cross corelation, defaults to False\n",
    "        :type force_cpu: bool, optional\n",
    "        :raises NoFramesInInputFile: If there are no frames in the converted\n",
    "            dataframe\n",
    "    \"\"\"\n",
    "    # Reading input file - index is considered on first column\n",
    "    metadata, dataframe = read_polaris_data(input_file, csv_sep)\n",
    "\n",
    "    if dataframe.empty:\n",
    "        LOGGER.error(\"Empty list of frames -- nothing to learn from!\")\n",
    "        raise NoFramesInInputFile\n",
    "\n",
    "    input_data = normalize_dataframe(dataframe)\n",
    "    source = metadata['satellite_name']\n",
    "\n",
    "    set_experiment(experiment_name=source)\n",
    "\n",
    "    xcorr_configurator = CrossCorrelationConfigurator(\n",
    "        xcorr_configuration_file=xcorr_configuration_file,\n",
    "        use_gridsearch=use_gridsearch,\n",
    "        force_cpu=force_cpu)\n",
    "\n",
    "    # Creating and fitting cross-correlator\n",
    "    xcorr = XCorr(metadata, xcorr_configurator.get_configuration(), regressor)\n",
    "    xcorr.fit(input_data)\n",
    "\n",
    "    if output_graph_file is None:\n",
    "        output_graph_file = \"/tmp/polaris_graph.json\"\n",
    "\n",
    "    metadata = PolarisMetadata({\"satellite_name\": source})\n",
    "    graph = PolarisGraph(metadata=metadata)\n",
    "    graph.from_heatmap(xcorr.importances_map, graph_link_threshold)\n",
    "    with open(output_graph_file, 'w') as graph_file:\n",
    "        graph_file.write(graph.to_json())\n",
    "\n",
    "\n",
    "def normalize_dataframe(dataframe):\n",
    "    \"\"\"\n",
    "        Apply dataframe modification so it's compatible\n",
    "        with the learn module. The time column is first\n",
    "        set as the index of the dataframe. Then, we drop\n",
    "        the time column.\n",
    "\n",
    "        :param dataframe: The pandas dataframe to normalize\n",
    "        :type dataframe: pd.DataFrame\n",
    "        :return: Pandas dataframe normalized\n",
    "        :rtype: pd.DataFrame\n",
    "    \"\"\"\n",
    "    dataframe.index = dataframe.time\n",
    "    dataframe.drop(['time'], axis=1, inplace=True)\n",
    "\n",
    "    return dataframe\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '2'. Detailed error Yaml file 'C:\\Users\\manni\\Desktop\\VIU\\TFM\\repo\\Improve-MBSE-ML\\mlruns\\2\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 261, in list_experiments\n",
      "    experiment = self._get_experiment(exp_id, view_type)\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 344, in _get_experiment\n",
      "    meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\utils\\file_utils.py\", line 175, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Users\\manni\\Desktop\\VIU\\TFM\\repo\\Improve-MBSE-ML\\mlruns\\2\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Columns 100%|████████████████████████████████████████████████| 170/170 [01:16&lt;00:00, 2.24 columns/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022/01/26 20:51:14 WARNING mlflow.tracking.context.git_context: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
      "The git executable must be specified in one of the following ways:\n",
      "    - be included in your $PATH\n",
      "    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
      "    - explicitly set via git.refresh()\n",
      "\n",
      "All git commands will error until this is rectified.\n",
      "\n",
      "This initial warning can be silenced or aggravated in the future by setting the\n",
      "$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
      "    - quiet|q|silence|s|none|n|0: for no warning or exception\n",
      "    - warn|w|warning|1: for a printed warning\n",
      "    - error|e|raise|r|2: for a raised exception\n",
      "\n",
      "Example:\n",
      "    export GIT_PYTHON_REFRESH=quiet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#GENERA EL GRAFO DE LIGHTSAIL2\n",
    "cross_correlate(\"lightsail_dataset.csv\", \"XGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '2'. Detailed error Yaml file 'C:\\Users\\manni\\Desktop\\VIU\\TFM\\repo\\Improve-MBSE-ML\\mlruns\\2\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 261, in list_experiments\n",
      "    experiment = self._get_experiment(exp_id, view_type)\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 344, in _get_experiment\n",
      "    meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\utils\\file_utils.py\", line 175, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Users\\manni\\Desktop\\VIU\\TFM\\repo\\Improve-MBSE-ML\\mlruns\\2\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Columns 100%|████████████████████████████████████████████████| 170/170 [00:26&lt;00:00, 6.62 columns/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '2'. Detailed error Yaml file 'C:\\Users\\manni\\Desktop\\VIU\\TFM\\repo\\Improve-MBSE-ML\\mlruns\\2\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 261, in list_experiments\n",
      "    experiment = self._get_experiment(exp_id, view_type)\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 344, in _get_experiment\n",
      "    meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\utils\\file_utils.py\", line 175, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Users\\manni\\Desktop\\VIU\\TFM\\repo\\Improve-MBSE-ML\\mlruns\\2\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Columns 100%|███████████████████████████████████████████████| 170/170 [00:10&lt;00:00, 17.36 columns/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '2'. Detailed error Yaml file 'C:\\Users\\manni\\Desktop\\VIU\\TFM\\repo\\Improve-MBSE-ML\\mlruns\\2\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 261, in list_experiments\n",
      "    experiment = self._get_experiment(exp_id, view_type)\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 344, in _get_experiment\n",
      "    meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\utils\\file_utils.py\", line 175, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Users\\manni\\Desktop\\VIU\\TFM\\repo\\Improve-MBSE-ML\\mlruns\\2\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra trees\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Columns 100%|████████████████████████████████████████████████| 170/170 [00:19&lt;00:00, 9.09 columns/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Malformed experiment '2'. Detailed error Yaml file 'C:\\Users\\manni\\Desktop\\VIU\\TFM\\repo\\Improve-MBSE-ML\\mlruns\\2\\meta.yaml' does not exist.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 261, in list_experiments\n",
      "    experiment = self._get_experiment(exp_id, view_type)\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\store\\tracking\\file_store.py\", line 344, in _get_experiment\n",
      "    meta = read_yaml(experiment_dir, FileStore.META_DATA_FILE_NAME)\n",
      "  File \"C:\\WPy64-3920\\python-3.9.2.amd64\\lib\\site-packages\\mlflow\\utils\\file_utils.py\", line 175, in read_yaml\n",
      "    raise MissingConfigException(\"Yaml file '%s' does not exist.\" % file_path)\n",
      "mlflow.exceptions.MissingConfigException: Yaml file 'C:\\Users\\manni\\Desktop\\VIU\\TFM\\repo\\Improve-MBSE-ML\\mlruns\\2\\meta.yaml' does not exist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient boosting\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "</style>\n",
       "<div class=\"enlighten\">\n",
       "  <div class=\"enlighten-bar\">\n",
       "    <pre>Columns 100%|███████████████████████████████████████████████| 170/170 [00:13&lt;00:00, 12.63 columns/s]</pre>\n",
       "  </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Random Forest\")\n",
    "cross_correlate(\"lightsail_dataset.csv\", \"RandomForest\")\n",
    "print(\"AdaBoost\")\n",
    "cross_correlate(\"lightsail_dataset.csv\", \"AdaBoost\")\n",
    "print(\"Extra trees\")\n",
    "cross_correlate(\"lightsail_dataset.csv\", \"ExtraTrees\")\n",
    "print(\"Gradient boosting\")\n",
    "cross_correlate(\"lightsail_dataset.csv\", \"GradientBoosting\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "from graph_to_object import getObjectFromGraph\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def count_graph_nodes_and_links(file):\n",
    "    with open(file + \".json\") as f:\n",
    "        graph_1 = json.loads(f.read())\n",
    "    list_graph_nodes = [x[\"id\"] for x in graph_1[\"graph\"][\"nodes\"]]\n",
    "    df_graph_links = pd.DataFrame([x for x in graph_1[\"graph\"][\"links\"]])\n",
    "    return df_graph_links, list_graph_nodes\n",
    "\n",
    "\n",
    "def get_free_nodes(df_links, list_nodes):\n",
    "    free_nodes = []\n",
    "    for node in list_nodes:\n",
    "        if(node not in list(df_links[\"target\"]) and node not in list(df_links[\"source\"]) ):\n",
    "            free_nodes.append(node)\n",
    "    return free_nodes\n",
    "\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Método</th>\n",
       "      <th>NumNodos</th>\n",
       "      <th>Num Links</th>\n",
       "      <th>Num Nodos Libres</th>\n",
       "      <th>Intensidad De Relación Media</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGB</td>\n",
       "      <td>170</td>\n",
       "      <td>279</td>\n",
       "      <td>37</td>\n",
       "      <td>0.417207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RandomForest</td>\n",
       "      <td>170</td>\n",
       "      <td>184</td>\n",
       "      <td>69</td>\n",
       "      <td>0.243238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>170</td>\n",
       "      <td>239</td>\n",
       "      <td>41</td>\n",
       "      <td>0.286610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ExtraTrees</td>\n",
       "      <td>170</td>\n",
       "      <td>299</td>\n",
       "      <td>44</td>\n",
       "      <td>0.199616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>170</td>\n",
       "      <td>389</td>\n",
       "      <td>37</td>\n",
       "      <td>0.241108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Método  NumNodos  Num Links  Num Nodos Libres  \\\n",
       "0               XGB       170        279                37   \n",
       "1      RandomForest       170        184                69   \n",
       "2          AdaBoost       170        239                41   \n",
       "3        ExtraTrees       170        299                44   \n",
       "4  GradientBoosting       170        389                37   \n",
       "\n",
       "   Intensidad De Relación Media  \n",
       "0                      0.417207  \n",
       "1                      0.243238  \n",
       "2                      0.286610  \n",
       "3                      0.199616  \n",
       "4                      0.241108  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['atmelPwr_curr',\n",
       " 'atmelPwr_volt',\n",
       " 'threeV_plTmp',\n",
       " 'threeV_plPwr_volt',\n",
       " 'threeV_plPwr_curr',\n",
       " 'fiveV_plPwr_volt',\n",
       " 'fiveV_plPwr_curr',\n",
       " 'daughter_aTmp',\n",
       " 'daughter_aPwr_volt',\n",
       " 'daughter_aPwr_curr']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>threeV_plTmp</td>\n",
       "      <td>daughter_aTmp</td>\n",
       "      <td>0.193964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>threeV_plTmp</td>\n",
       "      <td>daughter_bTmp</td>\n",
       "      <td>0.213247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>threeV_plTmp</td>\n",
       "      <td>boardTempNz</td>\n",
       "      <td>0.141566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>threeV_plPwr_curr</td>\n",
       "      <td>py_mag_x</td>\n",
       "      <td>0.129101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fiveV_plPwr_curr</td>\n",
       "      <td>mode</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fiveV_plPwr_curr</td>\n",
       "      <td>torq_z_reg_volt</td>\n",
       "      <td>0.229882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fiveV_plPwr_curr</td>\n",
       "      <td>torq_z_curr</td>\n",
       "      <td>0.280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>daughter_aTmp</td>\n",
       "      <td>threeV_plTmp</td>\n",
       "      <td>0.170108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>daughter_aTmp</td>\n",
       "      <td>daughter_bTmp</td>\n",
       "      <td>0.212743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>daughter_aTmp</td>\n",
       "      <td>cam1_heater_running</td>\n",
       "      <td>0.101245</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              source               target     value\n",
       "0       threeV_plTmp        daughter_aTmp  0.193964\n",
       "1       threeV_plTmp        daughter_bTmp  0.213247\n",
       "2       threeV_plTmp          boardTempNz  0.141566\n",
       "3  threeV_plPwr_curr             py_mag_x  0.129101\n",
       "4   fiveV_plPwr_curr                 mode  0.220000\n",
       "5   fiveV_plPwr_curr      torq_z_reg_volt  0.229882\n",
       "6   fiveV_plPwr_curr          torq_z_curr  0.280000\n",
       "7      daughter_aTmp         threeV_plTmp  0.170108\n",
       "8      daughter_aTmp        daughter_bTmp  0.212743\n",
       "9      daughter_aTmp  cam1_heater_running  0.101245"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_XGB_links, list_XGB_nodes  = count_graph_nodes_and_links(\"polaris_graph_XGB\")\n",
    "df_randomforest_links, list_randomforest_nodes = count_graph_nodes_and_links(\"polaris_graph_randomforest\")\n",
    "df_adaboost_links, list_adaboost_nodes = count_graph_nodes_and_links(\"polaris_graph_adaboost\")\n",
    "df_extratrees_links, list_extratrees_nodes = count_graph_nodes_and_links(\"polaris_graph_extratrees\")\n",
    "df_gradientboosting_links, list_gradientboosting_nodes = count_graph_nodes_and_links(\"polaris_graph_gradientboosting\")\n",
    "\n",
    "todos ={\"Método\":[\"XGB\", \"RandomForest\", \"AdaBoost\", \"ExtraTrees\", \"GradientBoosting\"],\n",
    "        \"NumNodos\": [len(list_XGB_nodes), len(list_randomforest_nodes), len(list_adaboost_nodes), len(list_extratrees_nodes), len(list_gradientboosting_nodes)],\n",
    "       \"Num Links\": [df_XGB_links.shape[0], df_randomforest_links.shape[0], df_adaboost_links.shape[0], df_extratrees_links.shape[0], df_gradientboosting_links.shape[0] ],\n",
    "       \"Num Nodos Libres\": [len(get_free_nodes(df_XGB_links, list_XGB_nodes)), len(get_free_nodes(df_randomforest_links, list_randomforest_nodes)), len(get_free_nodes(df_adaboost_links, list_adaboost_nodes)), len(get_free_nodes(df_extratrees_links, list_extratrees_nodes )), len(get_free_nodes(df_gradientboosting_links, list_gradientboosting_nodes)) ],\n",
    "       \"Intensidad De Relación Media\":[df_XGB_links[\"value\"].mean(), df_randomforest_links[\"value\"].mean(), df_adaboost_links[\"value\"].mean(), df_extratrees_links[\"value\"].mean(), df_gradientboosting_links[\"value\"].mean() ]}\n",
    "\n",
    "display(pd.DataFrame(todos))\n",
    "display(list_extratrees_nodes[:10])\n",
    "display(df_extratrees_links.head(10))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TFM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "de072921dc87486613898b1ef56959cc98c50a630fb49de1898fb32d92a683cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
