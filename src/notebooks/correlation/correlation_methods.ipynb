{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rICw0MTsMtam",
    "outputId": "4acf6428-30d0-4699-8dbe-e002169aa480"
   },
   "outputs": [],
   "source": [
    "#!pip install polaris-ml\r\n",
    "#!pip install fets\r\n",
    "#!pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RlwcETrdgSBZ",
    "outputId": "80d937e4-3ea8-4554-9ccd-bb2dafc31ca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression as LR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "print(\"Ok\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\r\n",
    "Cross Correlation module\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "import logging\r\n",
    "import warnings\r\n",
    "\r\n",
    "import enlighten\r\n",
    "import numpy as np\r\n",
    "import pandas as pd\r\n",
    "from polaris.feature.cleaner import Cleaner\r\n",
    "# Used for tracking ML process results\r\n",
    "from mlflow import log_metric, log_param, log_params, start_run\r\n",
    "# Used for the pipeline interface of scikit learn\r\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\r\n",
    "from sklearn.metrics import mean_squared_error\r\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\r\n",
    "\r\n",
    "# eXtreme Gradient Boost algorithm\r\n",
    "from xgboost import XGBRegressor\r\n",
    "\r\n",
    "#RandomForest\r\n",
    "from sklearn.ensemble import RandomForestRegressor\r\n",
    "\r\n",
    "#Extratrees regressor\r\n",
    "from sklearn.ensemble import ExtraTreesRegressor\r\n",
    "\r\n",
    "#AdaBoostRegressor\r\n",
    "from sklearn.ensemble import AdaBoostRegressor\r\n",
    "\r\n",
    "#GradientBoostingRegressor\r\n",
    "from sklearn.ensemble import GradientBoostingRegressor\r\n",
    "\r\n",
    "#HistGradientBoostingRegressor\r\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\r\n",
    "\r\n",
    "#VotingRegressor\r\n",
    "from sklearn.ensemble import VotingRegressor\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "\r\n",
    "\r\n",
    "LOGGER = logging.getLogger(__name__)\r\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\r\n",
    "# Remove this line when feature engineering is in place\r\n",
    "np.seterr(divide='ignore', invalid='ignore')\r\n",
    "\r\n",
    "\r\n",
    "class XCorr(BaseEstimator, TransformerMixin):\r\n",
    "    \"\"\" Cross Correlation predictor class\r\n",
    "    \"\"\"\r\n",
    "    def __init__(self, dataset_metadata, cross_correlation_params, regressor):\r\n",
    "        \"\"\" Initialize an XCorr object\r\n",
    "\r\n",
    "            :param dataset_metadata: The metadata of the dataset\r\n",
    "            :type dataset_metadata: PolarisMetadata\r\n",
    "            :param cross_correlation_params: XCorr parameters\r\n",
    "            :type cross_correlation_params: CrossCorrelationParameters\r\n",
    "        \"\"\"\r\n",
    "        self._regressor = regressor\r\n",
    "        self.models = None\r\n",
    "        self._importances_map = None\r\n",
    "        self._feature_cleaner = Cleaner(\r\n",
    "            dataset_metadata, cross_correlation_params.dataset_cleaning_params)\r\n",
    "        self.xcorr_params = {\r\n",
    "            \"random_state\": cross_correlation_params.random_state,\r\n",
    "            \"test_size\": cross_correlation_params.test_size,\r\n",
    "            \"gridsearch_scoring\": cross_correlation_params.gridsearch_scoring,\r\n",
    "            \"gridsearch_n_splits\":\r\n",
    "            cross_correlation_params.gridsearch_n_splits,\r\n",
    "        }\r\n",
    "        # If we're importing from CSV, the dataset_metadata may not\r\n",
    "        # have the feature_columns key.\r\n",
    "        try:\r\n",
    "            self.xcorr_params['feature_columns'] = dataset_metadata[\r\n",
    "                'analysis']['feature_columns']\r\n",
    "        except KeyError:\r\n",
    "            LOGGER.info(\r\n",
    "                \"No feature_columns entry in metatdata, setting to empty array\"\r\n",
    "            )\r\n",
    "            self.xcorr_params['feature_columns'] = []\r\n",
    "\r\n",
    "        if cross_correlation_params.use_gridsearch:\r\n",
    "            self.method = self.gridsearch\r\n",
    "            self.mlf_logging = self.gridsearch_mlf_logging\r\n",
    "        else:\r\n",
    "            self.method = self.regression\r\n",
    "            self.mlf_logging = self.regression_mlf_logging\r\n",
    "\r\n",
    "        self.model_params = {\r\n",
    "            \"current\": cross_correlation_params.model_params,\r\n",
    "            \"cpu\": cross_correlation_params.model_cpu_params\r\n",
    "        }\r\n",
    "        \r\n",
    "    @property\r\n",
    "    def regressor(self):\r\n",
    "        return self._regressor\r\n",
    "    \r\n",
    "    @regressor.setter\r\n",
    "    def regressor(self, regressor):\r\n",
    "        self._regressor = regressor\r\n",
    "\r\n",
    "    @property\r\n",
    "    def importances_map(self):\r\n",
    "        \"\"\"\r\n",
    "        Return the importances_map value as Pandas Dataframe.\r\n",
    "\r\n",
    "        \"\"\"\r\n",
    "\r\n",
    "        return self._importances_map\r\n",
    "\r\n",
    "    @importances_map.setter\r\n",
    "    def importances_map(self, importances_map):\r\n",
    "        self._importances_map = importances_map\r\n",
    "\r\n",
    "    def fit(self, X):\r\n",
    "        \"\"\" Train on a dataframe\r\n",
    "\r\n",
    "            The input dataframe will be split column by column\r\n",
    "            considering each one as a prediction target.\r\n",
    "\r\n",
    "            :param X: Input dataframe\r\n",
    "            :type X: pd.DataFrame\r\n",
    "            :raises Exception: If encountered any unhandled error\r\n",
    "                during model fitting\r\n",
    "        \"\"\"\r\n",
    "        if not isinstance(X, pd.DataFrame):\r\n",
    "            raise TypeError(\"Input data should be a DataFrame\")\r\n",
    "\r\n",
    "        if self.models is None:\r\n",
    "            self.models = []\r\n",
    "\r\n",
    "        manager = enlighten.get_manager()\r\n",
    "\r\n",
    "        LOGGER.info(\"Clearing Data. Removing unnecessary columns\")\r\n",
    "        X = self._feature_cleaner.drop_constant_values(X)\r\n",
    "        X = self._feature_cleaner.drop_non_numeric_values(X)\r\n",
    "        X = self._feature_cleaner.handle_missing_values(X)\r\n",
    "\r\n",
    "        self.reset_importance_map(X.columns)\r\n",
    "\r\n",
    "        parameters = self.__build_parameters(X)\r\n",
    "\r\n",
    "        pbar = manager.counter(total=len(parameters),\r\n",
    "                               desc=\"Columns\",\r\n",
    "                               unit=\"columns\")\r\n",
    "\r\n",
    "        with start_run(run_name='cross_correlate', nested=True):\r\n",
    "            self.mlf_logging()\r\n",
    "            for column in parameters:\r\n",
    "                LOGGER.info(column)\r\n",
    "                try:\r\n",
    "                    self.models.append(\r\n",
    "                        self.method(X.drop([column], axis=1), X[column],\r\n",
    "                                    self.model_params['current']))\r\n",
    "                except Exception as err:  # pylint: disable-msg=broad-except\r\n",
    "                    if self.model_params['current'].get(\r\n",
    "                            \"predictor\") == \"gpu_predictor\":\r\n",
    "                        LOGGER.info(\" \".join([\r\n",
    "                            \"Encountered error using GPU.\",\r\n",
    "                            \"Trying with CPU parameters now!\"\r\n",
    "                        ]))\r\n",
    "                        self.model_params['current'] = self.model_params['cpu']\r\n",
    "                    else:\r\n",
    "                        raise err\r\n",
    "                pbar.update()\r\n",
    "\r\n",
    "    def transform(self):\r\n",
    "        \"\"\" Unused method in this predictor \"\"\"\r\n",
    "        return self\r\n",
    "    \r\n",
    "        \r\n",
    "    def regression(self, df_in, target_series, model_params):\r\n",
    "        \"\"\" Fit a model to predict target_series with df_in features/columns\r\n",
    "            and retain the features importances in the dependency matrix.\r\n",
    "\r\n",
    "            :param df_in: Input dataframe representing the context, predictors\r\n",
    "            :type df_in: pd.DataFrame\r\n",
    "            :param target_series: pandas series of the target variable. Share\r\n",
    "                the same indexes as the df_in dataframe\r\n",
    "            :type target_series: pd.Series\r\n",
    "            :param model_params: Parameters for the XGB model\r\n",
    "            :type model_params: dict\r\n",
    "            :return: A fitted XGBRegressor\r\n",
    "            :rtype: XGBRegressor\r\n",
    "        \"\"\"\r\n",
    "        # Split df_in and target to train and test dataset\r\n",
    "        df_in_train, df_in_test, target_train, target_test = train_test_split(\r\n",
    "            df_in,\r\n",
    "            target_series,\r\n",
    "            test_size=0.2,\r\n",
    "            random_state=self.xcorr_params['random_state'])\r\n",
    "\r\n",
    "\r\n",
    "        regressors_dict = {\"XGB\": XGBRegressor(**model_params),\r\n",
    "                           \"RandomForest\": RandomForestRegressor(),\r\n",
    "                           \"AdaBoost\": AdaBoostRegressor(),\r\n",
    "                           \"ExtraTrees\": ExtraTreesRegressor(),\r\n",
    "                           \"GradientBoosting\": GradientBoostingRegressor(),\r\n",
    "                           \"Voting\": \"VotingRegressor()\"}\r\n",
    "\r\n",
    "        \"\"\" if self._regressor == \"XGboosting\":\r\n",
    "            # Create and train a XGBoost regressor\r\n",
    "            regr_m = XGBRegressor(**model_params)\r\n",
    "            \r\n",
    "        elif self._regressor == \"RandomForest\":\r\n",
    "            # Create and train a Sci-kit regressor\r\n",
    "            regr_m = RandomForestRegressor()\r\n",
    "        \r\n",
    "        elif self._regressor == \"AdaBoost\":\r\n",
    "            # Create and train a Sci-kit regressor\r\n",
    "            regr_m = AdaBoostRegressor()\r\n",
    "        \r\n",
    "        elif self._regressor == \"ExtraTrees\":\r\n",
    "             # Create and train a Sci-kit regressor\r\n",
    "            regr_m = ExtraTreesRegressor()\r\n",
    "        \r\n",
    "        elif self._regressor == \"GradientBoosting\":\r\n",
    "            # Create and train a Sci-kit regressor\r\n",
    "            regr_m = GradientBoostingRegressor()\r\n",
    "        \r\n",
    "        elif self._regressor == \"HistGradientBoosting\":\r\n",
    "            # Create and train a Sci-kit regressor\r\n",
    "            #regr_m = HistGradientBoostingRegressor()\r\n",
    "            pass\r\n",
    "            \r\n",
    "        elif self._regressor == \"Voting\":\r\n",
    "            # Create and train a Sci-kit regressor\r\n",
    "            regr_m = VotingRegressor()\r\n",
    "        \r\n",
    "        \"\"\"\r\n",
    "        regr_m = regressors_dict[self._regressor]\r\n",
    "        regr_m.fit(df_in_train, target_train)\r\n",
    "\r\n",
    "        # Make predictions\r\n",
    "        target_series_predict = regr_m.predict(df_in_test)\r\n",
    "\r\n",
    "        try:\r\n",
    "            rmse = np.sqrt(\r\n",
    "                mean_squared_error(target_test, target_series_predict))\r\n",
    "            log_metric(target_series.name, rmse)\r\n",
    "            LOGGER.info('Making predictions for : %s', target_series.name)\r\n",
    "            LOGGER.info('Root Mean Square Error : %s', str(rmse))\r\n",
    "        except Exception:  # pylint: disable-msg=broad-except\r\n",
    "            # Because of large (close to infinite values) or nans\r\n",
    "            LOGGER.error('Cannot find RMS Error for %s', target_series.name)\r\n",
    "            LOGGER.debug('Expected %s, Predicted %s', str(target_test),\r\n",
    "                         str(target_series_predict))\r\n",
    "\r\n",
    "        # indices = np.argsort(regr_m.feature_importances_)[::-1]\r\n",
    "        # After the model is trained\r\n",
    "        new_row = {}\r\n",
    "        for column, feat_imp in zip(df_in.columns,\r\n",
    "                                    regr_m.feature_importances_):\r\n",
    "            new_row[column] = [feat_imp]\r\n",
    "\r\n",
    "        # Current target is not in df_in, so manually adding it\r\n",
    "        new_row[target_series.name] = [0.0]\r\n",
    "\r\n",
    "        # Sorting new_row to avoid concatenation warnings\r\n",
    "        new_row = dict(sorted(new_row.items()))\r\n",
    "\r\n",
    "        # Concatenating new information about feature importances\r\n",
    "        if self._importances_map is not None:\r\n",
    "            self._importances_map = pd.concat([\r\n",
    "                self._importances_map,\r\n",
    "                pd.DataFrame(index=[target_series.name], data=new_row)\r\n",
    "            ])\r\n",
    "        return regr_m\r\n",
    "\r\n",
    "    def gridsearch(self, df_in, target_series, params):\r\n",
    "        \"\"\" Apply grid search to fine-tune XGBoost hyperparameters\r\n",
    "            and then call the regression method with the best grid\r\n",
    "            search parameters.\r\n",
    "\r\n",
    "            :param df_in: Input dataframe representing the context, predictors\r\n",
    "            :type df_in: pd.DataFrame\r\n",
    "            :param target_series: Pandas series of the target variable. Share\r\n",
    "                the same indexes as the df_in dataframe\r\n",
    "            :type target_series: pd.Series\r\n",
    "            :param params: The hyperparameters to use on the gridsearch\r\n",
    "                method\r\n",
    "            :type params: dict\r\n",
    "            :raises TypeError: If df_in is not Pandas DataFrame\r\n",
    "            :return: A fitted XGBRegressor\r\n",
    "            :rtype: XGBRegressor\r\n",
    "        \"\"\"\r\n",
    "        if not isinstance(df_in, pd.DataFrame):\r\n",
    "            LOGGER.error(\"Expected %s got %s for df_in in gridsearch\",\r\n",
    "                         pd.DataFrame, type(df_in))\r\n",
    "            raise TypeError\r\n",
    "\r\n",
    "        random_state = self.xcorr_params['random_state']\r\n",
    "        kfolds = KFold(n_splits=self.xcorr_params['gridsearch_n_splits'],\r\n",
    "                       shuffle=True,\r\n",
    "                       random_state=random_state)\r\n",
    "        regr_m = XGBRegressor(random_state=random_state,\r\n",
    "                              predictor=\"cpu_predictor\",\r\n",
    "                              tree_method=\"auto\",\r\n",
    "                              n_jobs=-1)\r\n",
    "\r\n",
    "        gs_regr = GridSearchCV(regr_m,\r\n",
    "                               param_grid=params,\r\n",
    "                               cv=kfolds,\r\n",
    "                               scoring=self.xcorr_params['gridsearch_scoring'],\r\n",
    "                               n_jobs=-1,\r\n",
    "                               verbose=1)\r\n",
    "        gs_regr.fit(df_in, target_series)\r\n",
    "\r\n",
    "        log_param(target_series.name + ' best estimator', gs_regr.best_params_)\r\n",
    "        LOGGER.info(\"%s best estimator : %s\", target_series.name,\r\n",
    "                    str(gs_regr.best_estimator_))\r\n",
    "        return self.regression(df_in, target_series, gs_regr.best_params_)\r\n",
    "\r\n",
    "    def reset_importance_map(self, columns):\r\n",
    "        \"\"\"\r\n",
    "        Creating an empty importance map\r\n",
    "\r\n",
    "        :param columns: List of column names for the importance map\r\n",
    "        :rtype columns: pd.Index or array-like\r\n",
    "        \"\"\"\r\n",
    "        if self._importances_map is None:\r\n",
    "            self._importances_map = pd.DataFrame(data={}, columns=columns)\r\n",
    "\r\n",
    "    def common_mlf_logging(self):\r\n",
    "        \"\"\" Log the parameters used for gridsearch and regression\r\n",
    "            in mlflow\r\n",
    "        \"\"\"\r\n",
    "        log_param('Test size', self.xcorr_params['test_size'])\r\n",
    "        log_param('Model', 'XGBRegressor')\r\n",
    "\r\n",
    "    def gridsearch_mlf_logging(self):\r\n",
    "        \"\"\" Log the parameters used for gridsearch\r\n",
    "            in mlflow\r\n",
    "        \"\"\"\r\n",
    "        log_param('Gridsearch scoring',\r\n",
    "                  self.xcorr_params['gridsearch_scoring'])\r\n",
    "        log_param('Gridsearch parameters', self.model_params)\r\n",
    "        self.common_mlf_logging()\r\n",
    "\r\n",
    "    def regression_mlf_logging(self):\r\n",
    "        \"\"\" Log the parameters used for regression\r\n",
    "            in mlflow.\r\n",
    "        \"\"\"\r\n",
    "        self.common_mlf_logging()\r\n",
    "        log_params(self.model_params)\r\n",
    "\r\n",
    "    def __build_parameters(self, X):\r\n",
    "        \"\"\" Remove features only from\r\n",
    "            being predicted.\r\n",
    "\r\n",
    "            :param X: The dataset\r\n",
    "            :type X: pd.DataFrame\r\n",
    "            :return: List of remaining features that are not removed\r\n",
    "            :rtype: list\r\n",
    "        \"\"\"\r\n",
    "        if self.xcorr_params['feature_columns'] is None:\r\n",
    "            return list(X.columns)\r\n",
    "\r\n",
    "        LOGGER.info('Removing features from the parameters : %s',\r\n",
    "                    self.xcorr_params['feature_columns'])\r\n",
    "        feature_to_remove = set(self.xcorr_params['feature_columns'])\r\n",
    "\r\n",
    "        return [x for x in list(X.columns) if x not in feature_to_remove]\r\n",
    "    \r\n",
    "    \r\n",
    "    \r\n",
    "print(\"Ok\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\r\n",
    "Module to launch different data analysis.\r\n",
    "\"\"\"\r\n",
    "import logging\r\n",
    "\r\n",
    "from fets.math import TSIntegrale\r\n",
    "from mlflow import set_experiment\r\n",
    "import polaris\r\n",
    "from polaris.data.graph import PolarisGraph\r\n",
    "from polaris.data.readers import read_polaris_data\r\n",
    "from polaris.dataset.metadata import PolarisMetadata\r\n",
    "from polaris.learn.feature.extraction import create_list_of_transformers, \\\r\n",
    "    extract_best_features\r\n",
    "from polaris.learn.predictor.cross_correlation_configurator import \\\r\n",
    "    CrossCorrelationConfigurator\r\n",
    "\r\n",
    "LOGGER = logging.getLogger(__name__)\r\n",
    "\r\n",
    "\r\n",
    "class NoFramesInInputFile(Exception):\r\n",
    "    \"\"\"Raised when frames dataframe is empty\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "def feature_extraction(input_file, param_col):\r\n",
    "    \"\"\"\r\n",
    "    Start feature extraction using the given settings.\r\n",
    "\r\n",
    "        :param input_file: Path of a CSV file that will be\r\n",
    "            converted to a dataframe\r\n",
    "        :type input_file: str\r\n",
    "        :param param_col: Target column name\r\n",
    "        :type param_col: str\r\n",
    "    \"\"\"\r\n",
    "    # Create a small list of two transformers which will generate two\r\n",
    "    # different pipelines\r\n",
    "    transformers = create_list_of_transformers([\"5min\", \"15min\"], TSIntegrale)\r\n",
    "\r\n",
    "    # Extract the best features of the two pipelines\r\n",
    "    out = extract_best_features(input_file,\r\n",
    "                                transformers,\r\n",
    "                                target_column=param_col,\r\n",
    "                                time_unit=\"ms\")\r\n",
    "\r\n",
    "    # out[0] is the FeatureImportanceOptimization object\r\n",
    "    # from polaris.learn.feature.selection\r\n",
    "    # pylint: disable=E1101\r\n",
    "    print(out[0].best_features)\r\n",
    "\r\n",
    "\r\n",
    "# pylint: disable-msg=too-many-arguments\r\n",
    "def cross_correlate(input_file,\r\n",
    "                    regressor=\"XGB\",\r\n",
    "                    output_graph_file=None,\r\n",
    "                    xcorr_configuration_file=None,\r\n",
    "                    graph_link_threshold=0.1,\r\n",
    "                    use_gridsearch=False,\r\n",
    "                    csv_sep=',',\r\n",
    "                    force_cpu=False):\r\n",
    "    \"\"\"\r\n",
    "    Catch linear and non-linear correlations between all columns of the\r\n",
    "    input data.\r\n",
    "\r\n",
    "        :param input_file: CSV or JSON file path that will be\r\n",
    "            converted to a dataframe\r\n",
    "        :type input_file: str\r\n",
    "        :param output_graph_file: Output file path for the generated graph.\r\n",
    "            It will overwrite if the file already exists. Defaults to None,\r\n",
    "            which is'/tmp/polaris_graph.json'\r\n",
    "        :type output_graph_file: str, optional\r\n",
    "        :param xcorr_configuration_file: XCorr configuration file path,\r\n",
    "            defaults to None. Refer to CrossCorrelationConfigurator for\r\n",
    "            the default parameters\r\n",
    "        :type xcorr_configuration_file: str, optional\r\n",
    "        :param graph_link_threshold: Minimum link value to be considered\r\n",
    "            as a link between two nodes\r\n",
    "        :type graph_link_threshold: float, optional\r\n",
    "        :param use_gridsearch: Use grid search for the cross correlation.\r\n",
    "            If this is set to False, then it will just use regression.\r\n",
    "            Defaults to False\r\n",
    "        :type use_gridsearch: bool, optional\r\n",
    "        :param csv_sep: The character that separates the columns inside of\r\n",
    "            the CSV file, defaults to ','\r\n",
    "        :type csv_sep: str, optional\r\n",
    "        :param force_cpu: Force CPU for cross corelation, defaults to False\r\n",
    "        :type force_cpu: bool, optional\r\n",
    "        :raises NoFramesInInputFile: If there are no frames in the converted\r\n",
    "            dataframe\r\n",
    "    \"\"\"\r\n",
    "    # Reading input file - index is considered on first column\r\n",
    "    metadata, dataframe = read_polaris_data(input_file, csv_sep)\r\n",
    "\r\n",
    "    if dataframe.empty:\r\n",
    "        LOGGER.error(\"Empty list of frames -- nothing to learn from!\")\r\n",
    "        raise NoFramesInInputFile\r\n",
    "\r\n",
    "    input_data = normalize_dataframe(dataframe)\r\n",
    "    source = metadata['satellite_name']\r\n",
    "\r\n",
    "    set_experiment(experiment_name=source)\r\n",
    "\r\n",
    "    xcorr_configurator = CrossCorrelationConfigurator(\r\n",
    "        xcorr_configuration_file=xcorr_configuration_file,\r\n",
    "        use_gridsearch=use_gridsearch,\r\n",
    "        force_cpu=force_cpu)\r\n",
    "\r\n",
    "    # Creating and fitting cross-correlator\r\n",
    "    xcorr = XCorr(metadata, xcorr_configurator.get_configuration(), regressor)\r\n",
    "    xcorr.fit(input_data)\r\n",
    "\r\n",
    "    if output_graph_file is None:\r\n",
    "        output_graph_file = \"/tmp/polaris_graph.json\"\r\n",
    "\r\n",
    "    metadata = PolarisMetadata({\"satellite_name\": source})\r\n",
    "    graph = PolarisGraph(metadata=metadata)\r\n",
    "    graph.from_heatmap(xcorr.importances_map, graph_link_threshold)\r\n",
    "    with open(output_graph_file, 'w') as graph_file:\r\n",
    "        graph_file.write(graph.to_json())\r\n",
    "\r\n",
    "\r\n",
    "def normalize_dataframe(dataframe):\r\n",
    "    \"\"\"\r\n",
    "        Apply dataframe modification so it's compatible\r\n",
    "        with the learn module. The time column is first\r\n",
    "        set as the index of the dataframe. Then, we drop\r\n",
    "        the time column.\r\n",
    "\r\n",
    "        :param dataframe: The pandas dataframe to normalize\r\n",
    "        :type dataframe: pd.DataFrame\r\n",
    "        :return: Pandas dataframe normalized\r\n",
    "        :rtype: pd.DataFrame\r\n",
    "    \"\"\"\r\n",
    "    dataframe.index = dataframe.time\r\n",
    "    dataframe.drop(['time'], axis=1, inplace=True)\r\n",
    "\r\n",
    "    return dataframe\r\n",
    "\r\n",
    "print(\"Ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-6-9e4c8a80f6fa>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-9e4c8a80f6fa>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    cross_correlate(\"lightsail_dataset.csv\", RandomForestB\")\u001b[0m\n\u001b[1;37m                                                            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "#GENERA EL GRAFO DE LIGHTSAIL2\r\n",
    "cross_correlate(\"lightsail_dataset.csv\", RandomForestB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Random Forest\")\r\n",
    "cross_correlate(\"lightsail_dataset.csv\", \"RandomForest\")\r\n",
    "print(\"AdaBoost\")\r\n",
    "cross_correlate(\"lightsail_dataset.csv\", \"AdaBoost\")\r\n",
    "print(\"Extra trees\")\r\n",
    "cross_correlate(\"lightsail_dataset.csv\", \"ExtraTrees\")\r\n",
    "print(\"Gradient boosting\")\r\n",
    "cross_correlate(\"lightsail_dataset.csv\", \"GradientBoosting\")\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TFM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "metadata": {
   "interpreter": {
    "hash": "de072921dc87486613898b1ef56959cc98c50a630fb49de1898fb32d92a683cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
